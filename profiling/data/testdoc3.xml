<latexdoc>
  <preamble>
    <documentclass>
      <config>
        <text>12pt, english, a4paper</text>
      </config>
      <block>
        <text>article</text>
      </block>
    </documentclass>
    <cmd_usepackage>
      <block>
        <text>microtype</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <config>
        <text>english, czech</text>
      </config>
      <block>
        <text>babel</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <config>
        <text>utf8x</text>
      </config>
      <block>
        <text>inputenc</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>ucs</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <config>
        <text>T1</text>
      </config>
      <block>
        <text>fontenc</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>t1enc</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>type1cm</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>times</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>setspace</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <config>
        <text>sort</text>
        <SPECIAL>&amp;</SPECIAL>
        <text>compress</text>
      </config>
      <block>
        <text>natbib</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>color</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>eurosym</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>ifpdf</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <block>
        <text>xmpincl</text>
      </block>
    </cmd_usepackage>
    <cmd_usepackage>
      <config>
        <text>pdftex</text>
      </config>
      <block>
        <text>hyperref</text>
      </block>
    </cmd_usepackage>
    <cmd_hypersetup>
      <block>
        <text>
          colorlinks,
              citecolor=black,
              filecolor=black,
              linkcolor=black,
              urlcolor=black,
              bookmarksopen=true,     
          bookmarksnumbered=true, 
          bookmarksopenlevel=1,   
          pdfstartview=FitV,       
          pdfpagemode=UseOutlines, 
        </text>
        
        
        
      </block>
    </cmd_hypersetup>
    <cmd_usepackage>
      <block>
        <text>bookmark</text>
      </block>
    </cmd_usepackage>
    <cmd_includexmp>
      <block>
        <text>Whats_wrong_with_social_simulations</text>
      </block>
    </cmd_includexmp>
    <pdfinfo>
      <block>
        <text>
          /Author (Eckhart Arnold)
            /Title (What's wrong with social simulations?)
            /Subject (A critique of the common practice to publish unvalidated simulations in the field of social simulations.)
            /Keywords (Social Simulations, Epistemology of Models, Philosophy of the Social Sciences, Economic Modeling)
          
        </text>
      </block>
    </pdfinfo>
    <cmd_usepackage>
      <block>
        <text>hyperref</text>
      </block>
    </cmd_usepackage>
    <cmd_captionsczech>
      <block>
        <cmd_renewcommand>
          <block>
            <cmd_contentsname></cmd_contentsname>
          </block>
        </cmd_renewcommand>
        <block>
          <text>Table of contents</text>
        </block>
        
        
        
      </block>
    </cmd_captionsczech>
  </preamble>
  <document>
    <frontpages>
      <paragraph></paragraph>
      <paragraph>
        <cmd_title>
          <block>
            <text>What's wrong with social simulations?</text>
          </block>
        </cmd_title>
      </paragraph>
      <paragraph>
        <cmd_author>
          <block>
            <text>Eckhart Arnold</text>
            <LINEFEED>\\</LINEFEED>
            <block>
              <cmd_small></cmd_small>
              <text>Institute for Philosophy</text>
            </block>
            <LINEFEED>\\</LINEFEED>
            <block>
              <cmd_small></cmd_small>
              <text>Heinrich Heine University of Düsseldorf</text>
            </block>
          </block>
        </cmd_author>
        
        
        
        <cmd_date>
          <block>
            <text>September 2013; last revision: March 2016</text>
          </block>
        </cmd_date>
      </paragraph>
      <paragraph></paragraph>
      <paragraph></paragraph>
      <paragraph>
        <text>
          This paper tries to answer the question why the epistemic value of so
          many social simulations is questionable. I consider the epistemic
          value of a social simulation as questionable if it contributes neither
          directly nor indirectly to the understanding of empirical reality. In
          order to justify this allegation I rely mostly but not entirely on the
          survey by 
        </text>
        <citet>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citet>
         
        <text>
          according to which 2/3 of all
          agent-based-simulations are not properly empirically validated.  In
          order to understand the reasons why so many social simulations are of
          questionable epistemic value, two classical social simulations are
          analyzed with respect to their possible epistemic justification:
          Schelling’s neighborhood segregation model 
        </text>
        <citep>
          <block>
            <text>schelling:1971</text>
          </block>
        </citep>
         
        <text>
          and
          Axelrod’s reiterated Prisoner’s Dilemma simulations of the evolution
          of cooperation 
        </text>
        <citep>
          <block>
            <text>axelrod:1984</text>
          </block>
        </citep>
        <text>
          . It is argued that Schelling’s
          simulation is useful, because it can be related to empirical reality,
          while Axelrod’s simulations and those of his followers cannot be
          related to empirical reality and therefore their scientific value
          remains doubtful. Finally, I critically discuss some of the typical
          epistemological background beliefs of modelers as expressed in Joshua
          Epsteins’s keynote address ``Why model?''
          
        </text>
        <citep>
          <block>
            <text>epstein:2008</text>
          </block>
        </citep>
        <text>
          . Underestimating the importance of empirical
          validation is identified as one major cause of failure for social
          simulations.
        </text>
      </paragraph>
      <paragraph>
        <cmd_vspace>
          <block>
            <text>0.1cm</text>
          </block>
        </cmd_vspace>
      </paragraph>
      <paragraph>
        <block>
          <text>Keywords</text>
        </block>
        <text>
          : Social Simulations, Epistemology of Models, Philosophy of the Social Sciences, Economic Modeling
          
        </text>
      </paragraph>
      <paragraph>
        <block>
          <text>Published in: The Monist 2014, Vol. 97, No.3, pp. 361-379.</text>
        </block>
      </paragraph>
      <paragraph></paragraph>
      <paragraph></paragraph>
      <paragraph></paragraph>
    </frontpages>
    <Section>
      <heading>
        <text>Introduction</text>
      </heading>
      <paragraph>
        <text>
          In this paper I will try to answer the question: Why is the epistemic
          value of so many social simulations questionable? Under social
          simulations I understand computer simulations of human interaction as
          it is studied in the social sciences. The reason why I consider the
          epistemic value of many social simulations as questionable is that
          many simulation studies cannot give an answer to the most salient
          question that any scientific study should be ready to answer: “How do
          we know it’s true?” or, if specifically directed to simulation
          studies: “How do we know that the simulation simulates the phenomenon
          correctly that it simulates?” Answering this question requires some
          kind of empirical validation of the simulation. The requirement of
          empirical validation is in line with the widely accepted notion that
          science is demarcated from non-science by its empirical testability or
          falsifiability. Many simulation studies, however, do not offer any
          suggestion how they could possibly be validated empirically.
        </text>
      </paragraph>
      <paragraph>
        <text>
          A frequent reply by simulation scientists is that no simulation of
          empirical phenomena was intended, but that the simulation only serves
          a “theoretical” purpose. Then, however, another equally salient
          question should be answered: “Why should we care about the results?”
          It is my strong impression that many social simulation studies cannot
          answer either this or the first question. This is not to say that the
          use of computer programs for answering purely theoretical questions is
          generally or necessarily devoid of value. The computer assisted proofs
          of the four color theorem 
        </text>
        <citep>
          <block>
            <text>wilson:2002</text>
          </block>
        </citep>
         
        <text>
          are an important
          counterexample. But in the social sciences it is hard to find
          similarly useful examples of the use of computers for purely
          theoretical purposes. In any case, the social sciences are empirical
          sciences. Therefore, social simulations should contribute either
          directly or indirectly to our understanding of social phenomena in the
          empirical world.
        </text>
      </paragraph>
      <paragraph>
        <text>
          There exist many different types of simulations but I will restrict
          myself to agent-based and game theoretical simulations.  I do not make
          a sharp difference between models and simulations. For the purpose of
          this paper I identify computer simulations just with programmed
          models. Most of my criticism of the practice of these simulation types
          can probably be generalized to other types of simulations or models in
          the social sciences and maybe also to some instances of the simulation
          practice in the natural sciences. It would lead too far afield to
          examine these connections here, but it should be easy to determine in
          other cases whether the particulars of bad simulation practice against
          which my criticism is directed are present or not.
        </text>
      </paragraph>
      <paragraph>
        <text>
          In order to bring my point home, I rely on the survey by
          
        </text>
        <citet>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citet>
         
        <text>
          on agent-based modeling practice for a
          general overview and on two example cases that I examine in detail. I
          start by discussing the survey which reveals that in an important
          sub-field of social simulations, namely, agent based simulations,
          empirical validation is commonly lacking. After that I first discuss
          Thomas Schelling’s well-known neighborhood segregation model. This is
          a model that I do not consider as being devoid of epistemic
          value. For, unlike most social simulations, it can be empirically
          falsified. The discussion of the particular features that make this
          model scientifically valuable will help us to understand why the
          simulation models discussed in the following fail to be so.
        </text>
      </paragraph>
      <paragraph>
        <text>
          The simulation models that I discuss in the following are simulations
          in the tradition of Robert Axelrod’s “Evolution of Cooperation”
          
        </text>
        <citep>
          <block>
            <text>axelrod:1984</text>
          </block>
        </citep>
        <text>
          . Although the modeling tradition initiated by
          Axelrod has delivered hardly any tenable and empirically applicable
          results, it still continues to thrive today. By some, Axelrod’s
          approach is still taken as a role model
          
        </text>
        <citep>
          <config>
            <text>208-209</text>
          </config>
          <block>
            <text>rendell-et-al:2010a</text>
          </block>
        </citep>
        <text>
          , although there has been severe
          criticism by others 
        </text>
        <citep>
          <block>
            <text>arnold:2008, binmore:1994, binmore:1998</text>
          </block>
        </citep>
        <text>.</text>
      </paragraph>
      <paragraph>
        <text>
          Finally, the question remains why scientists continue to produce such
          an abundance of simulation studies that fail to be empirically
          applicable. Leaving possible sociological explanations like the
          momentum of scientific traditions, the cohesion of peer groups, the
          necessity of justifying the investment in acquiring particular skills
          (e.g. math and programming) aside, I confine myself to the ideological
          background of simulation scientists. In my opinion the failure to
          produce useful results has a lot to do with the positivist attitude
          prevailing in this field of the social sciences. This attitude
          includes the dogmatic belief in the superiority of the methods of
          natural sciences like physics in any area of science. Therefore,
          despite frequent failure, many scientists continue to believe that
          formal modeling is just the right method for the social sciences. The
          attitude is well described in 
        </text>
        <citet>
          <block>
            <text>shapiro:2005</text>
          </block>
        </citet>
        <text>
          . Such attitudes are
          less often expressed explicitly in the scientific papers. Rather they
          form a background of shared convictions that, if not simply taken for
          granted as “unspoken assumptions”, find their expression in informal
          texts, conversations, blogs, keynote speeches. I discuss Joshua
          Epstein’s keynote lecture “Why Model?” 
        </text>
        <citep>
          <block>
            <text>epstein:2008</text>
          </block>
        </citep>
         
        <text>
          as an
          example.
        </text>
      </paragraph>
    </Section>
    <Section>
      <heading>
        <text>Simulation without validation in agent-based models</text>
      </heading>
      <paragraph>
        <text>
          In this section I give my interpretation of a survey by
          
        </text>
        <citet>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citet>
         
        <text>
          on agent-based-simulations. I do so with the
          intention of substantiating my claim that many social simulations are
          indeed useless. This is neither the aim nor the precise conclusion
          that 
        </text>
        <citet>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citet>
         
        <text>
          draw, but their study does reveal that
          two thirds of the surveyed simulation studies are not completely
          validated and the authors of the study consider this state of affairs
          as ``not acceptable'' 
        </text>
        <citep>
          <config>
            <text>4.11</text>
          </config>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citep>
        <text>
          . Thus my reading
          does not run counter the results of the survey. And it follows as a
          natural conclusion, if one accepts that a) an unvalidated simulation
          is - in most of the cases - a useless one and b) agent-based
          simulations make up a substantial part of social simulations.
        </text>
      </paragraph>
      <paragraph>
        <text>The survey by </text>
        <citet>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citet>
         
        <text>
          examines agent-based mode- ling
          practices between 1998 and 2008. It encompasses “279 articles from 92
          unique publication outlets in which the authors had constructed and
          analyzed an agent-based model” (Heath, Hill and Ciarallo, 2009,
          abstract). The articles stem from different fields of the social
          sciences including, business, economics, public policy, social
          science, traffic, military and also biology. The authors are not only
          interested in verification and validation practices, but the results
          concerning these are the results that I am interested in
          here. Verification and validation concern two separate aspects of
          securing the correctness of a simulation model.  Verification,
          as the term is used in the social simualtions community, roughly
          concerns the question whether the simulation software is bug-free and
          correctly implements the intended simulation model. Validation
          concerns the question whether the simulation model represents the
          simulated empirical target system adequately (for the intended
          purpose).
        </text>
      </paragraph>
      <paragraph>
        <text>
          Regarding verification, Heath, Hill and Ciarallo notice that ``Only 44
          (15.8
        </text>
        <text_command>
          <ESCAPED>%</ESCAPED>
        </text_command>
        <text>
          ) of the articles surveyed gave a reference for the reader to
          access or replicate the model. This indicates that the majority of the
          authors, publication outlets and reviewers did not deem it necessary
          to allow independent access to the models.  This trend appears
          consistently over the last 10 years''
          
        </text>
        <citep>
          <config>
            <text>3.6</text>
          </config>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citep>
        <text>
          . This astonishingly low figure can in
          part be explained by the fact that as long as the model is described
          with sufficient detail in the paper, it can also be replicated by
          re-programming it from the model description. It must not be forgotten
          that the replication of computer simulation results does not have the
          same epistemological importance as the replication of experimental
          results. While the replication of experiments adds additional
          inductive support to the experimental results, the replication of
          simulation results is merely a means for checking the simulation
          software for programming errors (“bugs”). Hence the possibility of
          precise replication is not an advantage that simulations enjoy over
          material experiments, as for example 
        </text>
        <citet>
          <config>
            <text>248</text>
          </config>
          <block>
            <text>reiss:2011</text>
          </block>
        </citet>
        
        
        
        <text>
          argues. Obviously, if the same simulation software is run in the same
          system environment the same results will be produced, no matter
          whether this is done by a different team of researchers at a different
          time and place with different computers. Even if the model is
          re-implemented the results must necessarily be the same provided that
          both the model and the system environment are fully specified and no
          programming errors have been made in the original implementation or
          the re-implementation.
        </text>
        <footnote>
          <block_of_paragraphs>
            <text>
              A possible exception concerns the
                frequent use of random numbers. As long as only pseudo random
                numbers with the same random number generator and the same “seed”
                are used, the simulation is still completely deterministic. This not
                to say that sticking to the same “seeds” is good practice other than
                for debugging.
            </text>
          </block_of_paragraphs>
        </footnote>
        <text>
          Replication or reimplementation can, however, help
          to reveal such errors.
        </text>
        <footnote>
          <block_of_paragraphs>
            <text>
              I am indebted to Paul Humphreys for
                pointing this out to me.
            </text>
          </block_of_paragraphs>
        </footnote>
        <text>
          It can therefore be considered as one of
          several possible means for the verification (but not validation) of a
          computer simulation. Error detection becomes much more laborious if no
          reference to the source code is provided. And it does happen that
          simulation models are not specified with sufficient detail to
          replicate them 
        </text>
        <citep>
          <block>
            <text>will-hegselmann:2008</text>
          </block>
        </citep>
        <text>
          . Therefore, the rather low
          proportion of articles that provide a reference to access or replicate
          the simulation is worrisome.
        </text>
      </paragraph>
      <paragraph>
        <text>
          More important than the results concerning verification is what Heath,
          Hill and Ciarallo find out about validation or, rather, the lack of
          validation:
        </text>
      </paragraph>
      <quotation>
        <text>
          Without validation a model cannot be said to be representative of
            anything real. However, 65
        </text>
        <text_command>
          <ESCAPED>%</ESCAPED>
        </text_command>
         
        <text>
          of the surveyed articles were not
            completely validated.  This is a practice that is not acceptable in
            other sciences and should no longer be acceptable in ABM practice
            and in publications associated with
            ABM. 
        </text>
        <citep>
          <config>
            <text>4.11</text>
          </config>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citep>
      </quotation>
      <paragraph>
        <text>This conclusion needs a little further commentary. The figure of 65</text>
        <text_command>
          <ESCAPED>%</ESCAPED>
        </text_command>
        
        
        
        <text>
          of not completely validated simulations is an average value over the
          whole period of study. In the earlier years that are covered by the
          survey hardly any simulation was completely validated. Later this
          figure decreases, but a ratio of less than 45
        </text>
        <text_command>
          <ESCAPED>%</ESCAPED>
        </text_command>
         
        <text>
          of completely
          validated simulation studies remains constant during the last 4 yours
          of the period covered 
        </text>
        <citep>
          <config>
            <text>3.10</text>
          </config>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citep>
        <text>.</text>
      </paragraph>
      <paragraph>
        <text>
          Furthermore it needs to be qualified what Heath, Hill and Ciarallo
          mean when they speak of complete validation.  The authors make a
          distinction between conceptual validation and operational validation.
          Conceptual validation concerns the question whether the mechanisms
          built into the model represent the mechanisms that drive the modeled
          real system. An “invalid conceptual model indicates the model may not
          be an appropriate representation of reality.” Operational validation
          then “validates results of the simulation against results from the
          real system.” 
        </text>
        <citep>
          <config>
            <text>2.13</text>
          </config>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citep>
        <text>
          . The demand for complete
          validation is well motivated: “If a model is only conceptually
          validated, then it 
        </text>
        <text_command>
          <BRACKETS>[</BRACKETS>
        </text_command>
        <text>is</text>
        <text_command>
          <BRACKETS>]</BRACKETS>
        </text_command>
         
        <text>
          unknown if that model will produce correct
          output results.” 
        </text>
        <citep>
          <config>
            <text>4.12</text>
          </config>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citep>
        <text>
          . For even if the
          driving mechanisms of the real system are represented in the model, it
          remains – without operational validation – unclear whether the
          representation is good enough to produce correct output results. On
          the other hand, a model that has been operationally validated only,
          may be based on a false or unrealistic mechanism and thus fail to
          explain the simulated phenomenon, even if the data matches. Heath,
          Hill and Ciarallo do not go into much detail concerning how exactly
          conceptual and operational validation are done in practice and under
          what conditions a validation attempt is to be considered as successful
          or as a failure.
        </text>
      </paragraph>
      <paragraph>
        <text>
          But do really all simulations need to be validated both conceptually
          and operationally as Heath, Hill and Ciarallo demand? After all, some
          simulations may – just like thought experiments – have been intended
          to merely prove conceptual possibilities.  One would usually not
          demand an empirical (i.e. operational) validation from a thought
          experiment. Heath, Hill and Ciarallo themselves make a distinction
          between the generator, mediator and predictor role of a simulation
          
        </text>
        <citep>
          <config>
            <text>2.16</text>
          </config>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citep>
        <text>
          . In the generator role simulations are
          merely meant to generate hypotheses. Simulations in the mediator role
          “capture certain behaviors of the system and 
        </text>
        <text_command>
          <BRACKETS>[</BRACKETS>
        </text_command>
        <text>..</text>
        <text_command>
          <BRACKETS>]</BRACKETS>
        </text_command>
         
        <text>
          characterize how the
          system may behave under certain scenarios” (3.4) and only simulations
          in the predictor role are actually calculating a real system. All of
          the surveyed studies fall into the first two categories. Obviously,
          the authors require complete validation even from these types of
          simulations.
        </text>
      </paragraph>
      <paragraph>
        <text>
          This can be disputed. As stated in the introduction, in order to be
          useful, a simulation study should make a contribution to answering
          some relevant question of empirical science. This contribution can be
          direct or indirect. The contribution is direct if the model can be
          applied to some empirical process and if it can be tested empirically
          whether the model is correct. The model’s contribution is indirect, if
          the model cannot be applied empirically, but if we can learn something
          from the model which helps us to answer an empirical question, the
          answer to which we would not have known otherwise. The latter kind of
          simulations can be said to function as thought experiments. It would
          be asking too much to demand complete empirical validation from a
          thought experiment.
        </text>
      </paragraph>
      <paragraph>
        <text>
          But does this mean that the figures from Heath, Hill and Ciarallo
          concerning the validation of simulations need to be interpreted
          differently by taking into account that some simulations may not
          require complete validation in the first place? This objection would
          miss the point, because the scenario just discussed is the exception
          rather than the rule. Classical thought experiments like Schrödinger’s
          cat usually touch upon important theoretical disputes. However, as
          will become apparent from the discussion of simulations of the
          evolution of cooperation, below, computer simulation studies all too
          easily lose the contact to relevant scientific questions. We just do
          not need all those digital thought experiments on conceivable variants
          of one and the same game theoretical model of cooperation. And the
          same surely applies to many other traditions of social modeling as
          well. But if this is true, then the figure of 65
        </text>
        <text_command>
          <ESCAPED>%</ESCAPED>
        </text_command>
         
        <text>
          of not completely
          validated simulation studies in the field of agent-based simulations
          is alarming indeed.
        </text>
        <footnote>
          <block_of_paragraphs>
            <text>
              For a detailed discussion of the cases in
                which even unvalidated simulations can be considered as useful, see
                
            </text>
            <citet>
              <block>
                <text>arnold:2013</text>
              </block>
            </citet>
            <text>
              . There are such cases, but the conditions under
                which this is possible appear to be quite restrictive.
            </text>
          </block_of_paragraphs>
        </footnote>
      </paragraph>
      <paragraph>
        <text>
          Given how important empirical validation is, “because it is the only
          means that provides some evidence that a model can be used for a
          particular purpose.” 
        </text>
        <citep>
          <config>
            <text>4.11</text>
          </config>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citep>
        <text>
          , it is surprising
          how little discussion this important topic finds in the textbook
          literature on social simulations. 
        </text>
        <citet>
          <block>
            <text>gilbert-troitzsch:2005</text>
          </block>
        </citet>
        
        
        
        <text>
          mention validation as an important part of the activity of conducting
          computer simulations in the social sciences, but then they dedicate
          only a few pages to it (22-25). 
        </text>
        <citet>
          <config>
            <text>98</text>
          </config>
          <block>
            <text>salamon:2011</text>
          </block>
        </citet>
         
        <text>
          also mentions
          it as an important question without giving any satisfactory answer to
          this question and without providing readers with so much as a hint
          concerning how simulations must be constructed so that their validity
          can be empirically tested. 
        </text>
        <citet>
          <block>
            <text>railsback-grimm:2011</text>
          </block>
        </citet>
         
        <text>
          dedicate many
          pages to describing the ODD-protocol, a protocol that is meant to
          standardize agent-based simulations and thus to facilitate the
          construction, comparison and evaluation of agent-based simulations.
          Arguably the most important topic, empirical validation of agent-based
          simulations, is not an explicit part of this protocol.  One could
          argue that this is simply a different matter, but then, given the
          importance of this topic it is slightly disappointing that Railsback
          and Grimm do not treat it more explicitly in their book.
        </text>
      </paragraph>
      <paragraph>
        <text>
          Summing it up, the survey by Heath, Hill and Ciarallo shows that an
          increasingly important sub-discipline of social simulations, namely
          the field of agent-based simulations faces the serious problem that a
          large part of its scientific literature consists of unvalidated and
          therefore most probably useless computer simulations. Moreover,
          considering the textbook literature on agent-based simulations one can
          get the impression that the scientific community is not at all
          sufficiently aware of this problem.
        </text>
      </paragraph>
    </Section>
    <Section>
      <heading>
        <text>How a model works that works: Schelling’s neighborhood segregation model</text>
      </heading>
      <paragraph>
        <text>
          Moving from the general finding to particular examples, I now turn to
          the discussion of Thomas Schelling’s neighborhood segregation
          model. Schelling’s neighborhood segregation model
          
        </text>
        <citep>
          <block>
            <text>schelling:1971</text>
          </block>
        </citep>
         
        <text>
          is widely known and has been amply discussed
          not only among economists but also among philosophers of science as a
          role model for linking micro-motifs with macro-outcomes. I will
          therefore say little about the model itself, but concentrate on the
          questions if and, if so, how it fulfills my criteria for epistemically
          valuable simulations.
        </text>
      </paragraph>
      <paragraph>
        <text>
          Schelling’s model was meant to investigate the role of individual
          choice in bringing about the segregation of neighborhoods that are
          either predominantly inhabited by blacks or by whites. Schelling
          considered the role of preference based individual choice as one of
          many possible causes of this phenomenon – and probably not even the
          most important, at least not in comparison to organized action and
          economic factors as two other possible causes
          
        </text>
        <citep>
          <config>
            <text>144</text>
          </config>
          <block>
            <text>schelling:1971</text>
          </block>
        </citep>
        <text>.</text>
      </paragraph>
      <paragraph>
        <text>
          In order to investigate the phenomenon, Schelling used a checkerboard
          model where the fields of the checkerboard would represent houses. The
          skin color of the inhabitants can be represented for example by
          pennies that are turned either heads or tails.
        </text>
        <footnote>
          <block_of_paragraphs>
            <text>
              Schelling’s
                article was published before personal computers existed. Today one
                would of course use a computer.  A simple version of Schelling’s
                model can be found in the netlogo models library
                
            </text>
            <citep>
              <block>
                <text>Wilensky1999</text>
              </block>
            </citep>
            <text>.</text>
          </block_of_paragraphs>
        </footnote>
        <text>
          Schelling assumed a certain tolerance
          threshold concerning the number of differently colored inhabitants in
          the neighborhood, before a household would move to another place. A
          result that was relatively stable among the different variants of the
          model he examined was that segregated neighborhoods would emerge –
          even if the threshold preference for equally colored neighbors was far
          below 50
        </text>
        <text_command>
          <ESCAPED>%</ESCAPED>
        </text_command>
        <text>
          , which means that segregation emerged even if the
          inhabitants would have been perfectly happy to live in an integrated
          environment with a mixed population. As 
        </text>
        <citet>
          <block>
            <text>aydinonat:2007</text>
          </block>
        </citet>
        
        
        
        <text>
          reports, the robustness of this result has been confirmed by many
          subsequent studies that employed variants of Schelling’s model. At the
          end of his paper Schelling discusses “tipping” that occurs when the
          entrance of a new minority starts to cause the evacuation of an area
          by its former inhabitants. In this connection Schelling also mentions
          an alternative hypothesis according to which inhabitants do not react
          to the frequency of similar or differently colored neighbors but on
          their on expectation about the future ratio of differently colored
          inhabitants. He assumes that this would aggravate the segregation
          process, but he does not investigate this hypothesis further
          
        </text>
        <citep>
          <config>
            <text>185-186</text>
          </config>
          <block>
            <text>schelling:1971</text>
          </block>
        </citep>
         
        <text>
          and his model is built on the
          assumption that individuals react to the actual and not the future
          ratio of skin colors.
        </text>
      </paragraph>
      <paragraph>
        <text>
          Is this model scientifically valuable? Can we draw conclusions from
          this model with respect to empirical reality and can we check whether
          these conclusions are true? Concerning these questions the following
          features of this model are important:
        </text>
      </paragraph>
      <enumerate>
        <item>
          <paragraph>
            <text>
              The assumptions on which the model rests can be tested
                empirically. The most important assumption is that individuals have
                a threshold for how many neighbors of a different color they
                tolerate and that they move to another neighborhood if this
                threshold is passed. This assumption can be tested empirically with
                the usual methods of empirical social research (and, of course,
                within the confinements of these methods). Also, the question
                whether people base their decision to move on the frequency of
                differently colored neighbors or on their on expectation concerning
                future changes of the neighborhood can be tested empirically.
            </text>
          </paragraph>
        </item>
        <item>
          <paragraph>
            <text>
              The model is highly robust. Changes of the basic setting and
                even fairly large variations of its input parameters, e.g. tolerance
                threshold, population size, do not lead to a significantly different
                outcome. Therefore even if the empirical measurement of, say, the
                tolerance threshold, is inaccurate, the model can still be applied.
                Robustness in this sense is directly linked to empirical
                testability. It should best be understood as a relational property
                between the measurement (in-)accuracy of the input parameters and
                the stability of the output values of a simulation.
            </text>
            <footnote>
              <block_of_paragraphs>
                <text>
                  There
                      are of course different concepts of robustness. I consider this
                      relational concept of robustness as the most important concept. An
                      important non-relational concept of robustness is that of
                      derivational robustness analysis
                      
                </text>
                <citep>
                  <block>
                    <text>kuorikoski-lehtinen:2009</text>
                  </block>
                </citep>
                <text>. See below.</text>
              </block_of_paragraphs>
            </footnote>
          </paragraph>
        </item>
        <item>
          <paragraph>
            <text>
              The model captures only one of many possible causes of
                neighborhood segregation. Before one can claim that the model
                explains or, rather, contributes to an explanation of neighborhood
                segregation, it is necessary to identify the modeled mechanism
                empirically and to estimate its relative weight in comparison with
                other actual causes. While the model shows that even a preference
                for integrated neighborhoods (if still combined with a tolerance
                limit) can lead to segregation, it may in reality still be the case
                that latent or manifest racism causes segregation. The model alone
                is not an explanation. (Schelling was aware of this.)
            </text>
          </paragraph>
        </item>
        <item>
          <paragraph>
            <text>
              Besides empirical explanation another possible use of the model
                would be policy advice. In this respect the model could be useful
                even if it does not capture an actual cause. For public policy must
                also be concerned about possible future causes.
            </text>
          </paragraph>
          <paragraph>
            <text>
              Assume for example, that manifest racism was a cause of neighborhood
                segregation, but that due to increasing public awareness racism is
                on the decline. Then the model can demonstrate that even if all
                further possible causes, e.g. economic causes, be removed as well,
                this might still not result in desegregated
                neighborhoods
            </text>
            <footnote>
              <block_of_paragraphs>
                <text>
                  But then, would we really worry about
                      segregated neighborhoods, if the issue wasn't tied to racial
                      discrimination and social injustice? After all, ethnic or
                      religious groups in Canada also often live in segregated areas
                      (``Canadian mosaic''). But other than in the U.S. this is hardly
                      an issue. Therefore, Schelling's model -- for all its
                      epistemological merits that are discussed here -- really seems to
                      miss the point in terms of scientific relevance. Discrimination is
                      the important point here, not segregation. But Schelling's model
                      induces us to frame the question in a way that makes us miss the
                      point. (
                </text>
                <block>
                  <text>
                    This comment has been added later as the result of
                          some discussions I had on this point. E.A., March 25th 2016.
                  </text>
                </block>
                <text>)</text>
              </block_of_paragraphs>
            </footnote>
            <text>
              -
                  provided, of course, that the basic assumption about a tolerance
                  threshold is true.
            </text>
          </paragraph>
          <paragraph>
            <text>
              Thus, for the purpose of policy advice a model does not need to
                capture actual causes. It can be counter-factual, but it must still
                be realistic in the sense that its basic assumptions can be
                empirically validated. Therefore, while the purpose of policy advice
                justifies certain counter-factual assumptions in a model, it cannot
                justify unrealistic and unvalidated models. This generally holds for
                models that are meant to describe possible instead of actual
                scenarios.
            </text>
          </paragraph>
        </item>
      </enumerate>
      <paragraph>
        <text>
          Schelling did not validate his model empirically. But for classifying
          the model as useful it is sufficient that it can be validated.  Now,
          the interesting question is: Can the model be validated and is it
          valid? Recent empirical research on the topic of neighborhood
          segregation suggests that inhabitants react to anticipated future
          changes in the frequency of differently colored neighbors rather than
          the frequency itself 
        </text>
        <citep>
          <config>
            <text>124-125</text>
          </config>
          <block>
            <text>ellen:2000</text>
          </block>
        </citep>
        <text>
          . An important role is
          played by the fear of whites that they might end up in an all-black
          neighborhood. Thus, the basic assumption of the model that individuals
          react upon the ratio of differently colored inhabitants in their
          neighborhood is wrong and one can say that the model is in this sense
          falsified.
        </text>
        <footnote>
          <block_of_paragraphs>
            <text>
              There are two senses in which a model (or more
                precisely: a model-based explanation) can be falsified: a) if the
                model’s assumptions are empirically not valid as in this case and b)
                if the causes the model captures are (i) either blocked by factors
                not taken into account in the model or (ii) cannot be disentangled
                from other possible causes or (iii) turn out to be irrelevant in
                comparison with other, stronger or otherwise more important causes
                for the same phenomenon. The connection between the model’s
                assumptions and its output, being a logical one, can, of course, not
                be empirically falsified.
            </text>
          </block_of_paragraphs>
        </footnote>
      </paragraph>
      <paragraph>
        <text>
          The strong emphasis that is placed on empirical validation here stands
          in contrast to some of the epistemological literature on simulations
          and models. Robert Sugden, noticing that “authors typically say very
          little about how their models relate to the real world”, treats models
          like that of Schelling (which is one of his examples
          
        </text>
        <citep>
          <config>
            <text>6-8</text>
          </config>
          <block>
            <text>sugden:2000</text>
          </block>
        </citep>
        <text>
          ) as “credible counterfactual worlds”
          
        </text>
        <citep>
          <config>
            <text>3</text>
          </config>
          <block>
            <text>sugden:2009</text>
          </block>
        </citep>
         
        <text>
          which are not intended to raise any particular
          empirical claims. Even though the particular relation to the real
          world is not clear, Sugden believes that such models can inform us
          about the real world. His account suffers from the fact that he
          remains unclear about how we can tell a counter-factual world that is
          credible from one that is incredible, if there is no empirical
          validation.
        </text>
      </paragraph>
      <paragraph>
        <text>
          A possible candidate for stepping in this gap of Sugden’s account is
          Kuorikoski’s and Lehtinen’s concept of “derivational robustness
          analysis” 
        </text>
        <citep>
          <block>
            <text>kuorikoski-lehtinen:2009</text>
          </block>
        </citep>
        <text>
          . According to this concept
          conclusions from unrealistic models to reality might be vindicated if
          the model remains robust under variations of its unrealistic
          assumptions. For example, in Schelling’s model the checkerboard
          topography could be replaced by other different topographies
          
        </text>
        <citep>
          <config>
            <text>441</text>
          </config>
          <block>
            <text>aydinonat:2007</text>
          </block>
        </citep>
        <text>
          . If the model still yields the same
          results about segregation, we are – if we follow the idea of
          “derivational robustness analysis” – entitled to draw the inductive
          conclusion that the model’s results would still be the same if the
          unrealistic topographies were exchanged by the topography of some real
          city, even though we have not tested it with a real topography.  A
          problem with this account is that it requires an inductive leap of a
          potentially dangerous kind: How can we be sure that the inductive
          conclusion derived from varying unrealistic assumptions holds for the
          conditions in reality which differ from any of these assumptions?
        </text>
      </paragraph>
      <paragraph>
        <text>
          Some philosophers also dwell on the analogy between simulations and
          experiments and consider simulations as “isolating devices” similar to
          experiments 
        </text>
        <citep>
          <block>
            <text>maeki:2009</text>
          </block>
        </citep>
        <text>
          . But the analogy between simulations
          and experiments is rather fragile, because other than experiments
          simulations are not empirical and do not allow us to learn anything
          about the world apart from what is implied in the premises of the
          simulation. In particular, we can – without some kind of empirical
          validation – never be sure whether the causal mechanism modeled in the
          simulation represents a real cause isolated in the model or does not
          exist in reality at all.
        </text>
      </paragraph>
      <paragraph>
        <text>
          Summing it up, it is difficult, if not impossible, to claim that
          models can inform us about reality without any kind of empirical
          validation. Schelling’s model, however, appears to be a scientifically
          useful model, at least in the sense that it can be validated (or
          falsified for that matter). The most decisive features of the model in
          this respect are its robustness and the practical feasibility of
          identifying the modeled cause in empirical reality. Next we will see
          how models fare when these features are not present.
        </text>
      </paragraph>
    </Section>
    <Section>
      <heading>
        <text>How models fail: The Reiterated Prisoner’s  Dilemma model</text>
      </heading>
      <paragraph>
        <text>
          Robert Axelrod’s computer simulations of the Reiterated Prisoner’s
          Dilemma (RPD) 
        </text>
        <citep>
          <block>
            <text>axelrod:1984</text>
          </block>
        </citep>
         
        <text>
          are well known and still considered
          by some as a role model for successful simulation research
          
        </text>
        <citep>
          <config>
            <text>408-409</text>
          </config>
          <block>
            <text>rendell-et-al:2010a</text>
          </block>
        </citep>
        <text>
          . What is not so widely known is
          that the simulation research tradition initiated by Axelrod has
          remained entirely unsuccessful in terms of generating explanations for
          empirical instances of cooperation. What are the reasons for this lack
          of explanatory success? And how come that Axelrod’s research design is
          none the less considered as a role model today?
        </text>
      </paragraph>
      <paragraph>
        <text>
          Axelrod had the ingenious idea to advertise a public computer
          tournament where participation was open to everybody. Participants
          were asked to hand in their guess at a best strategy in the reiterated
          two person Prisoner’s Dilemma in the form of an algorithmic
          description or computer program. This provided Axelrod with a rich,
          though naturally very contingent set of diverse strategies and it had
          the, surely welcome, side-effect of generating attention for Axelrod’s
          research project. Axelrod ran a sequence of two tournaments. As is
          well known the rather simplistic strategy 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
         
        <text>
          won both
          tournaments.
        </text>
      </paragraph>
      <paragraph>
        <text>
          In the Prisoner’s Dilemma Game the players can decide whether to
          cooperate or not to cooperate. Mutual cooperation yields a higher
          payoff than mutual non-cooperation, but it is best to cheat by letting
          the other player cooperate while not cooperating oneself. And it is
          worst to be cheated, i.e. to cooperate while the other player does
          not. 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
         
        <text>
          cooperates in the first round of the Repeated
          Prisoner’s Dilemma, but if the other player cheats, then 
        </text>
        <block>
          <text>
            Tit For
              Tat
          </text>
        </block>
         
        <text>
          will punish the other player by not cooperating in the
          following round.
        </text>
        <footnote>
          <block_of_paragraphs>
            <text>
              For a detailed description RPD-model and the
                tournament see Axelrod (1984). An open-source implementation is
                available from: 
            </text>
            <cmd_url>
              <block>
                <text>www.eckhartarnold.de/apppages/coopsim</text>
              </block>
            </cmd_url>
            <text>.</text>
          </block_of_paragraphs>
        </footnote>
        <text>
          Axelrod analyzed the course of the tournament in order to understand
          just why 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
         
        <text>
          was such a successful strategy. He
          concluded that it is a number of characteristics that determine the
          success of a strategy in the Reiterated Prisoner’s Dilemma
          
        </text>
        <citep>
          <config>
            <text>chapter 6</text>
          </config>
          <block>
            <text>axelrod:1984</text>
          </block>
        </citep>
        <text>
          : Successful strategies are (1)
          “friendly”, i.e. they start with cooperative moves, (2) envy-free, (3)
          punishing, but also (4) forgiving. Axelrod furthermore believed that
          repeated interaction is a necessary requirement for cooperation to
          evolve and that, of course, 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
         
        <text>
          is generally quite a
          good strategy in Reiterated Prisoner’s Dilemma situations.
        </text>
      </paragraph>
      <paragraph>
        <text>
          Unfortunately for Axelrod, the Reiterated Prisoner’s Dilemma model is
          anything but robust. For each of his conclusions, variations of the
          RPD-model can be constructed where the conclusion becomes invalid
          
        </text>
        <citep>
          <config>
            <text>107</text>
          </config>
          <block>
            <text>arnold:2013</text>
          </block>
        </citep>
        <text>
          . It is even possible to construct a variant
          that allows strategies to break off the repeated interaction at will
          and that does not lead to the breakdown of cooperation
          
        </text>
        <citep>
          <block>
            <text>schuessler:1990</text>
          </block>
        </citep>
        <text>
          . The failure to derive any robust results
          highlights the danger of drawing generalizing conclusions from models
          and of relying on models as a tool of theoretical investigation. This
          point has most strongly been emphasized by Ken Binmore, who describes
          the popularity that Axelrod’s model enjoyed derogatorily as the “The
          Tit-For-Tat Bubble” 
        </text>
        <citep>
          <config>
            <text>194</text>
          </config>
          <block>
            <text>binmore:1994</text>
          </block>
        </citep>
        <text>
          . Because the folk
          theorem from game theory implies that there are infinitely many
          equilibria in the Reiterated Prisoner’s Dilemma, there is not much
          reason to assign of all things the 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
        <text>
          -equilibrium a
          special place 
        </text>
        <citep>
          <config>
            <text>313-317</text>
          </config>
          <block>
            <text>binmore:1994</text>
          </block>
        </citep>
        <text>
          . If one follows Binmore’s
          criticism then it is not the reiterated Prisoner’s Dilemma that
          explains why 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
         
        <text>
          is such a good strategy, but rather the
          fact that 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
         
        <text>
          is a very salient and easily understood
          mode of behavior in many areas of life that explains why people so
          easily believed in the superiority of the 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
         
        <text>
          strategy
          in the RPD
          game. 
        </text>
      </paragraph>
      <paragraph>
        <text>
          It is not only its lack of robustness that troubles Axelrod’s
          model. It is also the difficulty of relating it to any concrete
          empirical subject matter – a problem that Axelrod shares with many
          game theoretical explanations.
        </text>
        <footnote>
          <block_of_paragraphs>
            <text>
              This is very frankly admitted
                by the leading game theorist 
            </text>
            <citet>
              <block>
                <text>rubinstein:2013</text>
              </block>
            </citet>
             
            <text>
              in a newspaper
                article. Rubinstein resorts to an aesthetic vindication of game
                theory (“flowers in the garden of God”).
            </text>
          </block_of_paragraphs>
        </footnote>
        <text>
          Axelrod himself had
          offered a very impressive example of empirical application by relating
          the RPD model to the silent “Live and Let Live” agreement that emerged
          between enemy soldiers on some of the quieter stretches of the western
          front in the First World War. However, as critics were quick to point
          out 
        </text>
        <citep>
          <block>
            <text>battermann-et-al:1998, schuessler:1990</text>
          </block>
        </citep>
        <text>
          , it is not at all
          clear whether this situation really is a Prisoner’s Dilemma situation,
          let alone how the numerical values of the payoff parameters could be
          assessed. But precise numerical payoff values would be necessary since
          Axelrod’s model is not robust against changes of the numerical values
          of the payoff parameters within the boundaries that the Prisoner’s
          Dilemma game allows 
        </text>
        <citep>
          <config>
            <text>80</text>
          </config>
          <block>
            <text>arnold:2008</text>
          </block>
        </citep>
        <text>
          . Also, Axelrod’s model
          could not explain why “Live and Let Live” occurred only on some
          stretches of the front line 
        </text>
        <citep>
          <config>
            <text>180</text>
          </config>
          <block>
            <text>arnold:2008</text>
          </block>
        </citep>
        <text>
          . Therefore,
          Axelrod’s theory of the evolution of cooperation could not really add
          anything substantial to the historical explanation of the “Live and
          Let Live” by Tony 
        </text>
        <citet>
          <block>
            <text>ashworth:1980</text>
          </block>
        </citet>
        <text>.</text>
      </paragraph>
      <paragraph>
        <text>
          The chapter from Axelrod’s book on the “Live and Let Live”-system
          shows that he did not understand his model only as a normative model,
          but at least also as an explanatory model. And the model was certainly
          understood as potentially explanatory by the biologists who were
          trying to apply it to cooperative behavior among animals (see
          below). The distinction is important, because the validation
          requirements for normative models are somewhat relaxed in comparison
          to explanatory models. After all, we would not expect from a model
          that is meant to generate advice for rationally adequate behavior to
          correctly predict the behavior of unadvised and potentially irrational
          agents. Still, even normative models must capture the essentials of
          the empirical situations to which they are meant to be applied well
          enough to generate credible advice. Here, too, robustness is an
          important issue. For similar reasons as in the descriptive case it
          would be dangerous to trust the advice given on the basis of a
          non-robust model.
        </text>
      </paragraph>
      <paragraph>
        <text>
          Thus, in contrast to Schelling’s model Axelrod’s model is neither
          robust nor can the postulated driving factors of the emergent
          phenomenon (stable cooperation) easily be identified empirically. In
          Schelling’s case the driving factor was the assumed tolerance
          threshold, in Axelrod’s case it is the payoff parameters of the
          Prisoner’s Dilemma.  Therefore, two important prerequisites
          (robustness and empirical identifiability) for the application of a
          formal model to a social process appear to be absent in Axelrod’s
          case.
        </text>
      </paragraph>
      <paragraph>
        <text>
          The popularity of Axelrod’s computer tournaments had the consequence
          that it became a role model for much of the subsequent simulation
          research on the evolution of cooperation. It spawned myriads of
          similar simulation studies on the evolution of cooperation
          
        </text>
        <citep>
          <block>
            <text>dugatkin:1997, hoffmann:2000</text>
          </block>
        </citep>
        <text>
          . Unfortunately, most of these
          simulation studies remained unconnected to empirical research. Axelrod
          had – most probably without intending it – initiated a self-sustaining
          modeling tradition where modelers would orientate their next research
          project on the models that they or others had published before without
          paying much attention to what kind of models might be useful from an
          empirical perspective. Instead it was more or less silently assumed
          that because of the generality of the model investigations of the
          reiterated Prisoner’s Dilemma model would surely be useful.
        </text>
      </paragraph>
      <paragraph>
        <text>
          How little contact the modeling tradition initiated by Axelrod had to
          empirical research becomes very obvious in a survey of empirical
          research on the evolution of cooperation in biology by
          
        </text>
        <citet>
          <block>
            <text>dugatkin:1997</text>
          </block>
        </citet>
        <text>
          . In the beginning, Dugatkin lists several dozens
          of game theoretical simulation models of the evolution of cooperation,
          an approach to which Dugatkin himself is very favorable. However, none
          of the models can be related to particular instances of cooperation in
          animal wildlife. A seemingly insurmountable obstacle in this respect
          is that payoff parameters usually cannot be measured. It is just very
          difficult to measure precisely the increased reproductive success,
          say, that apes that reciprocate grooming enjoy over apes that don’t.
        </text>
      </paragraph>
      <paragraph>
        <text>
          The most serious attempt to apply Axelrod’s model was undertaken by
          
        </text>
        <citet>
          <block>
            <text>milinski:1987</text>
          </block>
        </citet>
         
        <text>
          in a study on predator inspection behavior in
          shoal fishes like sticklebacks. When a predator approaches, it happens
          that one or two sticklebacks leave the shoal and carefully swim closer
          to the predator. The hypothesis was that if two sticklebacks approach
          the predator they play a Reiterated Prisoner’s Dilemma and make the
          decision to turn back based on a 
        </text>
        <block>
          <text>Tit For Tat</text>
        </block>
         
        <text>
          strategy taking
          into account whether the partner fish stays back or not. This was
          tested experimentally by 
        </text>
        <citet>
          <block>
            <text>milinski:1987</text>
          </block>
        </citet>
         
        <text>
          as well as others
          
        </text>
        <citep>
          <config>
            <text>59-69</text>
          </config>
          <block>
            <text>dugatkin:1998</text>
          </block>
        </citep>
        <text>
          . While in his 1987-paper Milinski himself
          believed that the hypothesis could be confirmed, it was after a long
          controversy ultimately abandoned. In a joint paper on the same topic
          that appeared ten years later 
        </text>
        <citet>
          <block>
            <text>milinski-parker:1997</text>
          </block>
        </citet>
         
        <text>
          do not draw
          on the RPD model any more. In fact they treat it as an unresolved
          question whether the observed behavior is cooperative at all.
        </text>
      </paragraph>
      <paragraph>
        <text>
          In a later discussion, Dugatkin explained the problem when linking the
          model research about cooperation to the empirical research in biology
          by the difficulty of establishing a feedback-loop between model
          research and empirical research 
        </text>
        <citep>
          <config>
            <text>57-58</text>
          </config>
          <block>
            <text>dugatkin:1998a</text>
          </block>
        </citep>
        <text>
          . The
          empirical results were never fed back into the model building process
          and the obstacles when trying to apply the models were never
          considered by the modelers. Without a feedback-loop between
          theoretical and empirical research, however, the model-building
          process soon reaches a stalemate where models remain detached from
          reality.
        </text>
      </paragraph>
      <paragraph>
        <text>
          The frustration about this kind of pure model research is well
          expressed in a polemical article by Peter
          
        </text>
        <citet>
          <block>
            <text>hammerstein:2003</text>
          </block>
        </citet>
        <text>
          . “Why is there such a discrepancy between
          theory and facts?” asks 
        </text>
        <citet>
          <config>
            <text>83</text>
          </config>
          <block>
            <text>hammerstein:2003</text>
          </block>
        </citet>
         
        <text>
          and continues: “A
          look at the best known examples of reciprocity shows that simple
          models of repeated games do not properly reflect the natural
          circumstances under which evolution takes place. Most repeated animal
          interactions do not even correspond to repeated games.” In saying so,
          Hammerstein is by no means opposed to employing game theory in
          biology. It’s just that in the aftermath of Axelrod most simulation
          studies on the evolution of cooperation focused on the Reiterated
          Prisoner’s Dilemma or similar repeated games. This shows that the
          demand for empirical validation has an important side effect besides
          allowing to judge the truth and falsehood of the models themselves: It
          forces the modelers to concern themselves seriously with the empirical
          literature and the empirical phenomena that their models address. If
          they do so, there is hope that this will lead quite naturally to the
          choice of simulation models that address relevant questions of
          empirical research. Or, as 
        </text>
        <citet>
          <config>
            <text>92</text>
          </config>
          <block>
            <text>hammerstein:2003</text>
          </block>
        </citet>
         
        <text>
          nicely puts
          it: “Most certainly, if we invested the same amount of energy in the
          resolution of all problems raised in this discourse, as we do in the
          publishing of toy models with limited applicability, we would be
          further along in our understanding of cooperation.”
        </text>
      </paragraph>
      <paragraph>
        <text>
          Just how little model researchers care for the empirical content of
          their research is inadvertently demonstrated by a research report on
          the evolution of cooperation that appeared roughly 20 years after
          the publication of Axelrod’s first paper about his computer tournament
          
        </text>
        <citep>
          <block>
            <text>hoffmann:2000</text>
          </block>
        </citep>
        <text>
          . There is only one brief passage where the
          author of this research report talks about empirical applications of
          the theory of the evolution of cooperation. And in this passage there
          is but one piece of empirical literature that the author quotes, the
          study on predator inspection in sticklebacks by 
        </text>
        <citet>
          <block>
            <text>milinski:1987</text>
          </block>
        </citet>
        <text>
          !
          Nevertheless, Hoffmann believes that the “general framework is
          applicable to a host of realistic scenarios both in the social and
          natural worlds” 
        </text>
        <citep>
          <config>
            <text>4.3</text>
          </config>
          <block>
            <text>hoffmann:2000</text>
          </block>
        </citep>
        <text>
          . Much more believable is
          Dugatkin’s summary of the situation: “Despite the fact that game
          theory has a long standing tradition in the social sciences, and was
          incorporated in behavioral ecology 20 years ago, controlled tests of
          game theory models of cooperation are still relatively rare. It might
          be argued that this is not the fault of the empiricists, but rather
          due to the fact that much of the theory developed is unconnected to
          natural systems and thus may be mathematically intriguing but
          biologically meaningless” 
        </text>
        <citep>
          <config>
            <text>57</text>
          </config>
          <block>
            <text>dugatkin:1998a</text>
          </block>
        </citep>
        <text>
          . That this fact
          could escape the attention of the modelers tells a lot about the
          prevailing attitude of modelers towards empirical research.
        </text>
      </paragraph>
    </Section>
    <Section>
      <heading>
        <text>An ideology of modeling</text>
      </heading>
      <paragraph>
        <text>
          The examples discussed previously indicate that simulation models can
          be a valuable tool to study some of the possible causes of some social
          phenomena. However, the examples also content that a) modeling approaches
          in the social sciences can easily fail to deliver resilient results,
          that b) social simulations are not yet generally embedded in a
          research culture where the critical assessment of the (empirical)
          validity of the simulation models is a salient part of the research
          process and that c) the significance of pure simulation results is
          likely to be overrated.
        </text>
      </paragraph>
      <paragraph>
        <text>
          Unsurprisingly, simulation models in the social sciences excel when
          studying those causes that can be represented by a mathematical model
          as in the case of Schelling’s neighborhood segregation model. Part of
          the secret of Schelling’s success is surely that he had a good
          intuition for picking those example cases where mathematical models
          really work. But many of the causal connections that are of interest
          in the social science cannot be described mathematically.  For
          example, the question how the proliferation and easy accessibility of
          adult content in the internet shapes the attitude of youngsters
          towards love, sex and relationships, is hardly a question that could
          be answered with mathematical models. Or, if we want to understand
          what makes people follow orders to slaughter other people even in
          contradiction to their acquired moral codes
          
        </text>
        <citep>
          <block>
            <text>Browning:1992</text>
          </block>
        </citep>
        <text>
          , then any reasonable answer to this
          question will hardly have the form of a mathematical model.
        </text>
        <footnote>
          <block_of_paragraphs>
            <text>
              A
                good discussion of the respective merits and limitations of
                different research paradigms in the social sciences can be found in
                
            </text>
            <citet>
              <block>
                <text>moses-knutsen:2012</text>
              </block>
            </citet>
            <text>.</text>
          </block_of_paragraphs>
        </footnote>
      </paragraph>
      <paragraph>
        <text>
          Unfortunately, the field of social simulations has by now become so
          much of a specialized field that modelers are hardly aware of the
          strong limitations of their approach in comparison with conventional,
          model-free methods in the social sciences. There is a widespread,
          though not necessarily always outspoken belief that more or less
          everything can -- somehow -- be cast into a simulation model. Part of
          the reason for this belief may be the fact that with computers the
          power of modeling techniques has indeed greatly increased. This belief
          has found explicit expression in Joshua Epstein’s keynote address to
          the Second World Congress of Social Simulation under the title “Why
          model?” 
        </text>
        <citep>
          <block>
            <text>epstein:2008</text>
          </block>
        </citep>
        <text>.</text>
      </paragraph>
      <paragraph>
        <text>
          In the following I am going to discuss Epstein’s arguments and point
          out the misconceptions underlying this belief. In my opinion these
          misconceptions are to no small degree responsible for the misguided
          practices in the field of social simulations.  Epstein sets out by
          arguing that it is never wrong to model, because – as he believes –
          there exists only the choice between explicit and implicit models,
          anyway:
        </text>
      </paragraph>
      <quotation>
        <paragraph>
          <text>
            The first question that arises frequently -- sometimes innocently
              and sometimes not -- is simply, "Why model?" Imagining a rhetorical
              (non-innocent) inquisitor, my favorite retort is, "You are a
              modeler."Anyone who ventures a projection, or imagines how a social
              dynamic -- an epidemic, war, or migration -- would unfold is running
              some model. But typically, it is an implicit model in which the
              assumptions are hidden, their internal consistency is untested,
              their logical con- sequences are unknown, and their relation to data
              is unknown. But, when you close your eyes and imagine an epidemic
              spreading, or any other social dynamic, you are running some model
              or other. It is just an implicit model that you haven’t written down
              (see Epstein 2007).
          </text>
        </paragraph>
        <paragraph>
          <text>...</text>
        </paragraph>
        <paragraph>
          <text>
            The choice, then, is not whether to build models; it’s whether to
              build explicit ones. In explicit models, assumptions are laid out in
              detail, so we can study exactly what they entail. On these
              assumptions, this sort of thing happens. When you alter the
              assumptions that is what happens. By writing explicit models, you let
              others replicate your results. 
          </text>
          <citep>
            <config>
              <text>1.2-1.5</text>
            </config>
            <block>
              <text>epstein:2008</text>
            </block>
          </citep>
        </paragraph>
      </quotation>
      <paragraph>
        <text>
          It is not entirely clear whether Epstein restricts his arguments to
          projections, but even in this case it is most likely false. It is
          simply not possible to cast anything that can be described in natural
          language into the form of a mathematical or computer model. But then
          we also cannot assume that this must be possible, if projections to
          the future are concerned. It is of course always commendable to make
          one’s own assumptions explicit. But this does not require modeling.
        </text>
      </paragraph>
      <paragraph>
        <text>
          In addition, there are certain dangers associated with mathematical
          and computational modeling:
        </text>
      </paragraph>
      <enumerate>
        <item>
          <paragraph>
            <text>
              the danger of underrating or ignoring those causal connections
                that do not lend themselves to formal descriptions.
            </text>
          </paragraph>
        </item>
        <item>
          <paragraph>
            <text>
              the danger of arbitrary ad hoc decisions when modeling causes of
                which we only have a vague empirical understanding.  The necessity
                to specify everything precisely easily leads to the sin of false
                precision, which consists in assuming detailed knowledge where in
                fact there is none.
            </text>
          </paragraph>
        </item>
        <item>
          <paragraph>
            <text>
              the danger of conferring a deceptive impression of understanding
                even if the model is not validated.
            </text>
          </paragraph>
        </item>
        <item>
          <paragraph>
            <text>
              the shaping and selection of scientific questions by the
                requirements of modeling, rather than by other, arguably more
                important, criteria of relevance as, for example, the social impact
                or relevance for public policy.
            </text>
          </paragraph>
        </item>
      </enumerate>
      <paragraph>
        <text>
          That Epstein mentions replicability as another advantage of explicit
          modeling is ironic given that it is still quite uncommon in published
          simulation studies to give a reference for the reader to access and
          replicate the model (as described further above). More worrisome,
          however, is Epstein’s attitude towards validation:
        </text>
      </paragraph>
      <quotation>
        <text>
          ... I am always amused when these same people challenge me with the
            question, "Can you validate your model?" The appropriate retort, of
            course, is, "Can you validate yours?"At least I can write mine down
            so that it can, in principle, be calibrated to data, if that is what
            you mean by "validate,"a term I assiduously avoid (good Popperian
            that I am). 
        </text>
        <citep>
          <config>
            <text>1.4</text>
          </config>
          <block>
            <text>epstein:2008</text>
          </block>
        </citep>
      </quotation>
      <paragraph>
        <text>
          Calibration (i.e. fitting a model to data) is of course neither the
          same nor a proper substitute for validation (testing a model against
          data), as Epstein knows. Validation in the sense of empirical testing
          of a model, hypothesis or theory is a common standard in almost all
          sciences, including those sciences mentioned earlier that usually do
          not rely on formal models like history, ethnology, sociology,
          political science. It is obviously not the case that validation
          presupposes explicit modeling, for otherwise history as an empirical
          science would be impossible.
        </text>
      </paragraph>
      <paragraph>
        <text>
          Epstein furthermore advances 16 reasons for building models other than
          prediction 
        </text>
        <citep>
          <config>
            <text>1.9-1.17</text>
          </config>
          <block>
            <text>epstein:2008</text>
          </block>
        </citep>
        <text>
          . None of these reasons is
          exclusively a reason for employing models, though. The functions, for
          example, of guiding data collection or discovering new questions can
          be fulfilled by models and also by any other kind of theoretical
          reasoning.  Nor is it an exclusive virtue of the modeling approach
          “that it enforces a scientific habit of mind”
          
        </text>
        <citep>
          <config>
            <text>1.6</text>
          </config>
          <block>
            <text>epstein:2008</text>
          </block>
        </citep>
        <text>
          . Here Epstein is merely articulating the
          positivistic stock prejudice of the superiority, if only of a didactic
          kind, of formal methods. Given what 
        </text>
        <citet>
          <block>
            <text>heath-et-al:2009</text>
          </block>
        </citet>
         
        <text>
          have
          found out about the lack of proper validation of many agent-based
          simulations one might even be inclined to believe the opposite about
          the simulation method’s aptitude to encourage a scientific habit of
          mind.
        </text>
      </paragraph>
      <paragraph>
        <text>
          It fits into the picture of a somewhat dogmatic belief in the power of
          modeling approaches that modelers consider the lack of acceptance of
          their method often as more of a psychological problem on the side of
          the recipients to be addressed by better propaganda 
        </text>
        <citep>
          <config>
            <text>
              2.11-2.12,
              3.22-3.26
            </text>
          </config>
          <block>
            <text>barth-et-al:2012</text>
          </block>
        </citep>
        <text>
          , rather than a consequence of the still
          immature methodological basis of many agent-based simulation
          studies. This attitude runs the risk of self-deception, because one of
          the major reasons why non-modelers tend to be skeptical of agent-based
          simulations is that they perceive such simulations as highly
          speculative. As we have seen, the skeptics have good reason to do so.
        </text>
      </paragraph>
    </Section>
    <Section>
      <heading>
        <text>Conclusions</text>
      </heading>
      <paragraph>
        <text>
          It is in my opinion not least because of the abundance of simulations
          with low empirical impact that “social simulation is not yet
          recognized in the social science mainstream”
          
        </text>
        <citep>
          <config>
            <text>abstract</text>
          </config>
          <block>
            <text>squazzoni-casnici:2013</text>
          </block>
        </citep>
        <text>
          . Why should a mainstream
          social scientist take simulation studies seriously, if he or she
          cannot be sure about the reliability of the results, because the
          simulations have never been validated? If modelers started to take the
          requirement of empirical validation more seriously, I expect two
          changes to occur – both of them beneficial: 1) Social simulations will
          become more focused in scope. Scientists will not attempt to cast
          anything into the form of a computer simulation from classical social
          contract philosophy 
        </text>
        <citep>
          <block>
            <text>skyrms:1996, skyrms:2004</text>
          </block>
        </citep>
         
        <text>
          to, well, the
          whole world 
        </text>
        <citep>
          <block>
            <text>futureict:2013, livingearth:2013</text>
          </block>
        </citep>
        <text>
          , but they will
          develop a better feeling for when simulations can be empirically
          validated and when not, and they will mostly leave out those problems
          where computer simulations cannot be applied with some hope of
          producing empirically applicable results. 2) Yet, while the simulation
          method will become more focused in scope, it will at the same time
          become much more useful in practice, because simulations will more
          frequently yield results that other scientists can rely on without
          needing to worry about their speculative character and potential lack
          of reliability.
        </text>
      </paragraph>
      <paragraph>
        <cmd_bibliographystyle>
          <block>
            <text>apsr</text>
          </block>
        </cmd_bibliographystyle>
        
        
        
        <cmd_bibliography>
          <block>
            <text>bibliography</text>
          </block>
        </cmd_bibliography>
      </paragraph>
    </Section>
  </document>
</latexdoc>