.. _processing_pipelines:

Processing Pipelines
====================

Processing pipelines are software design pattern to organize the
passing of data through several stages of refinement before a
final output is reached. The "standard"-pipeline looks like this::

  data stage        text ---> CST --------> AST ---> output data
                          |           |          |
                          |           |          |
  transition           parsing  transforming  compiling
  (stage)

However, processing pipelines can become much longer, depending
into how many steps it is suitable to organize the data-refinement.
And, they can be bifurcated, if different outputs are derived
from the same input, say an HTML- and a PDF-version of the
input-document.

The standard-pipeline
---------------------

When compiling a document in a domain specific notation or language,
DHParser assumes the same standard-pipeline of four steps:

1. *preprocessing*, which is a str -> str transformation. More
   precisely, it takes a text document as input and yields a text
   document as well as a source-mapping-table as output. The output
   document of the preprocessor is usually a modified version of the
   input document.

2. *parsing*, which is a str -> node-tree transformation. More
   precisely, it yields the (potentially already somewhat simplified)
   concrete syntax-tree of the input-text. A list of parsing-errors may
   have been attached to the root-node of that syntax-tree.

3. *AST-transformation*, which is a node-tree -> node-tree
   transformation that converts the concrete syntax-tree (in-place) into
   the abstract-syntax-tree. Again, errors may have been added to the
   error-list of the root-node.

4. *compiling*: which is a node-tree -> anything transformation. More
   precisely, it takes the abstract-syntax-tree as input and yields the
   compiled data as output. What format the compiled data is, depends
   entirely on the compiler. It can be a another node-tree, but also
   anything else. The abstract-syntax-tree may be changed or even
   destroyed during the compilation. In any case, errors that occur
   during compilation will again be reported to the root-node of the
   tree and can later be collected by accessing ``root.errors``

The xxxParser.py-script that is autogenerated by DHParser when compiling an
EBNF-grammar provides transformation-functions for each of these steps and
generators that yield a thread-local-version of each of these
transformation-functions or callable transformation-classes.

Module :py:mod:`DHParser.compile` provides the helper function
:py:func:`DHParser.compile.compile_source` that calls these four stages
in sequence and collects the result, i.e. the output of the last stage,
the error messages, if any, and, optionally, the AST-tree. Example::

   >>> from DHParser.compile import compile_source
   >>> json_str = '{"test": ["This", "is", 1, "JSON", "test"]}'
   >>> json_objs, errors, ast = compile_source(json_str, None,
   ...                              json_parser,
   ...                              lambda tree: traverse(tree, json_AST_trans),
   ...                              simplifiedJSONCompiler(),
   ...                              preserve_AST=True)
   >>> json_objs
   {'test': ['This', 'is', 1.0, 'JSON', 'test']}
   >>> errors
   []
   >>> print(ast.as_sxpr())
   (json
     (object
       (member
         (string "test")
         (array
           (string "This")
           (string "is")
           (other_literal "1")
           (string "JSON")
           (string "test")))))

Subsequent stages of the processing pipeline will only be called if no
fatal errors have occurred in any of the earlier stages. This means that
when designing the AST-transformation, the compiler and, if the extended
pipeline (see below) is used, any further processing stages, it should
be provided for the case that the input is faulty stemming from earlier
stages can to some degree (determined by your assignment or seriousness
to different possible errors) be faulty.

Function :py:func:`DHParser.compile.process_tree` is a convenient helper
function that calls a given processing stage only, if the tree handed
over from the last stage does not contain any fatal errors. Thus, a
sequence of processing stages can be written as a sequence of calls of
:py:func:`DHParser.compile.process_tree` without the need of any if
clauses to check the results for fatal errors after each call.

The extended pipeline
---------------------

There are many contexts where the four above-mentioned stages are not
sufficient. In the digital humanities, for example, it is typical that
the data is passed through many different tree-processing stages, before
it is transformed into a form that is not a tree, any more. And it is
not at all uncommon that this processing pipeline is bifurcated as the
following schema, taken from the `Medieval Latin dictionary
<https://mlw.badw.de>`_ or the `Bavarian Academy of Sciences and
Humanities <https://www.badw.de>`_, shows::

    ----------------
    | source (DSL) |
    ----------------
           |
           |--- Parsing
           |
        -------
        | CST |
        -------
           |
           |--- AST-Transformation
           |
        -------
        | AST |
        -------
           |
           |--- data-consolidation
           |
      ------------
      | data-XML |
      ------------
           |
           |--- output-transformation
           |
     -------------- print-transform. ------------- TeX-compilation -----------
     | output-XML |----------------->| print-XML |---------------->| ConTeXt |
     --------------                  -------------                 -----------
           |
           |--- HTML-Transformation
           |
        --------
        | HTML |
        --------

In this particular example, there is no preprocessing stage. The first
three remaining stages are covered by the "standard pipeline" (i.e.
parsing, AST-transformation, compilation). The following stages,
starting from data-XML, form the extended pipeline.

.. _junctions: 

In order to support extended processing pipeline
:py:mod:`DHParser.compile` uses the very simple concept of junctions,
where a junctions is the connection of an earlier stage (origin) in the
the pipeline to a following stage (target) via a transformation or
compilation function. Pipelines are created by providing junctions from
for each intermediate stage from the starting stage (usually the last
stage of the standard pipeline) to one or more ending stages.
Bifurcations a created simply by providing to different junctions
starting from the same origin stage. (It is not allowed to have more
than one junction for one and the same target stages.)

The stages are identified by the names which may be chosen arbitrarily as long
as each name is used for one and the same stage, only. Technically, a junction
is a triple of the name of the origin stage, a factory function that returns a
transformation-callable and the name of the target stage. A (potentially
bifurcated) pipeline is then simply a set of junctions that covers all routes
from the starting stage(s) of the pipeline to its ending stage(s).

We will illustrate this by extending our example of simplified
json-compiler to a processing pipeline. So far the standard pipeline of
our json-compiler (although we did not bother to call it thus) yields
the json-data in form of Python-objects. Now let's assume, we'd like to
add two further processing stages, one which yields the json-data as a
human-readable pretty-printed json-string, the other which yields it as
a compact byte-array, ready for transmission over some kind of connection.
This is how our extension of the standard-pipeline looks like::

            |
      -------------  pretty-print  -----------------------
      | json-data |--------------->| human readable json |
      -------------                -----------------------
            |
            |--- compact-print
            |
     -----------------
     | one-line json |
     -----------------
            |
            |--- bytearry-convert
            |
    --------------------
    | transmission obj |
    --------------------

Let's define the necessary junctions "pretty-print", "compact-print" and
"bytearray-convert". Each junction is a 3-tuple of 1) the name of input
stage, 2) a compilation functions that either transforms the input-tree
produces some other kind of output and 3) the name of the output stage.

A restriction of junctions in DHParser consists in the fact that the input data
for the compilation functions must always be the root-node of a tree. This
restriction is due to the fact that the standard case for
transformation-pipelines in the Digital Humanities is that of chains of
tree-transformations. However, in some cases, as in this example, the data
already has a different form than a tree at earlier stages of the pipeline. In
order to cover those cases, DHParser uses the trick to attach the data to the
root-node of the last tree stage and then passing the root-node with the
attached data to the next junction. The RootNode thus serves as a pod for
passing the non-tree data further on through the data. This trick has the
benefit that the methods for error reporting that the
:py:class:`DHParser.nodetree.RootNode`-class provides can also be used for the
non-tree-stages of the pipeline. In our example already the first stage of the
extended data is not a node-tree, any more. So we need to attach it to the
root-node of the last tree-stage, which in this case is the AST::

   >>> ast.data = json_objs
   >>> ast.stage = 'json'
   >>> source_stages = {'json': ast}

It may appear odd that the stage of the ast-tree is named "json". However,
once the ``data``-field is set in the root-node, the ``stage``-field
indicates the stage of the data and not the tree, any more.

It is not obligatory to set the `stage`-field to any value. It can also
be left empty. But to do so helps when debugging
processing pipelines and also allows :py:func:`~compile.run_pipeline`
to check for errors in the setup of a pipeline. The following
examples reflect this practice.

Now let's define the "pretty-print"-compilation function and the
respective junction::

   >>> import json
   >>> from DHParser.nodetree import RootNode
   >>> def pretty_print(input: RootNode) -> str:
   ...     input.stage = 'pretty-json'
   ...     try:
   ...         return json.dumps(input.data, indent=2)
   ...     except TypeError as e:
   ...         input.new_error(input, "JSON-Error: " + str(e))
   ...         return f'"{str(e)}"'
   >>> pretty_print_junction = ('json', lambda : pretty_print, 'pretty-json')

Any errors can simply be attached to the RootNode-object that is passed to the
compilation-function!

Since "pretty_print" yields a final state, it does not need to return a
tree, but it may yield any data-type. This is different for the
intermediary junction "compact-print". Here, the transformed data must
be attached to the RootNode, again::

   >>> def compact_print(input: RootNode) -> RootNode:
   ...     try:
   ...         input.data = json.dumps(input.data)
   ...     except TypeError as e:
   ...         input.new_error(input, "JSON-Error: " + str(e))
   ...     input.stage = 'compact-json'
   ...     return input
   >>> compact_print_junction = ('json', lambda : compact_print, 'compact-json')

The "byte-array"-convert-junction that takes the output from the last step, the
compact-json, as input can be defined as follows::

   >>> def bytearray_convert(input: RootNode) -> bytes:
   ...     input.stage = 'byte-stream'
   ...     return input.data.encode('utf-8')
   >>> bytearray_convert_junction = ('compact-json', lambda : bytearray_convert, 'byte-stream')

Finally, all junctions must be passed to the
:py:func:`~compile.run_pipeline`-function which automatically constructs
the bifurcated pipeline from the given junctions and passes the
input-data through all bifurcations of the pipeline::

   >>> from DHParser.compile import run_pipeline
   >>> target_stages={"pretty-json", "byte-stream"}
   >>> results = run_pipeline({pretty_print_junction, compact_print_junction,
   ...                         bytearray_convert_junction},
   ...                         source_stages, target_stages)

Note, that ``source_stages`` is a mapping the of source-stage-names to the
source-stage's data, while ``target-stages`` is merely a set of names of all
final stages.

The results are a mapping of all target AND intermediary stages to 2-tuples of
the output-data of the respective stage (or None, if any fatal error has
occurred) and a potentially empty error list::

   >>> for target in sorted(list(target_stages)):
   ...    print(target, results[target][0])
   byte-stream b'{"test": ["This", "is", 1.0, "JSON", "test"]}'
   pretty-json {
     "test": [
       "This",
       "is",
       1.0,
       "JSON",
       "test"
     ]
   }

A nice feature of extended pipelines is their integration with the
testing-framework (see :py:mod:`~testing`): All stages of an extended pipeline
can be unit-tested with DHParser's unit-testing framework for grammars as long
as the results of these stages can be serialized with ``str()``.


*Classes and Functions-Reference*
---------------------------------

Types and Functions
^^^^^^^^^^^^^^^^^^^

   * :py:data:`~compile.Junction`: A type-alias for a tuple: (name of relative
        source stage, factory for a compiler, name of the relative destination
        stage). "relative" here means from the point of view of the compilation
        function returned by the factory.

   * :py:data:`~dsl.PseudoJunction`: A surrogate for :py:data:`~compile.Junction`:
        for the preprocessing-stage in particular where the root-node-object as
        a handle to passe the data through the pipeline does not yet exist.

   * :py:func:`~dsl.create_preprocess_junction`: Creates a pseudo junction for
        the preprocessing stage.

   * :py:func:`~dsl.create_junction`: Creates a junction-tuple to describe
        a particular transition in the processing pipeline.

   * :py:func:`~compile.run_pipeline`: Runs an extended pipeline of
        compilation or transformation functions (or, more precisely,
        callables) that is defined by a set of junctions and returns the
        results for selected target stages.  
