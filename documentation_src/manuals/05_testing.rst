Testing
=======

DHParser provides a powerful unit-testing framework that allows testing
individual components of a grammar separately through all stages of the
:ref:`processing pipeline <processing_pipelines>`. DHParser's
unit-testing framework allows to::

    - break down the complicated process of writing a grammar into
      relatively simpler tasks of designing grammar-components each
      of which can be tested individually

    - turn writing and refactoring grammars into controlled and
      manageable process. In fact, without unit-testing, refactoring
      of grammars is hardly feasible at all.

    - extend a grammar incrementally without breaking existing code.

    - check the syntax-tree-construction as well as all subsequent
      transformations (as long as their result is serializable).

Because writing grammars can be difficult, refactoring and testing of
grammars is crucial for the success of a project. Also, one and the
same formal language can be described by different grammars and the way
the grammar is written influences the shape of the syntax-tree that the
parser yields. Therefore, it is quite common to rewrite a grammar or
parts of it more than once during the course of developing a formal
notation. For example, in the first iteration one tries to find a
grammar that matches the given or intended notation. In the second
iteration, the grammar is refined to yield a well-shaped syntax-tree to
make further processing easier. Unit-tests help to safeguard this
process against breaking earlier changes by later changes.

Writing grammar-tests
---------------------

The canical way to write grammar-tests in a DHParser-projects is by
writing them into a file with a name that matches the the glob-pattern
``*_test_*.ini``. Tests can then simply be executed by running the
``tst_..._grammar.py``-script that has been generated by the
dhparser-command (see :ref:`full_scale_DSLs`). The format of these files
strongly resembles that of common config-files. Here is a test for the
outline-parser example from the overview (:ref:`macros`)::

    [match:document]
    M1: """# Main Heading
        ## Section 1
        ### SubSection 1.1
        ### SubSection 1.2
        ## Section 2"""

    [fail:document]
    F1: """# Main Heading
        ## Section 1
        #### SubSubSection 1.1.1  BADLY NESTED!!!
        ### SubSection 1.2
        ## Section 2"""

Test-files (".ini"-Files) are separated into sections by headings that
are enclosed in square-brackets, i.e. ``[`` and ``]``. The heading's
name consists of two parts, separeted by a colon ``:``. The first part
indicates the kind of the tests that are specified under the heading.
The second part is the name of the parser that will be called with these
tests as input. Thus, the heading "[match:document]" means that the
following tests will be feeded to the parser "document" and the parser
is expected to match. The following heading "[fail:document]" introduces
tests, where the parser "document" is expected to fail, i.e. the test is
successful if the parser does not match the snipped fed to the parser.

The tests themselves are specified by a name followed by a colon ``:``
followd by a single-line or multiline-string which line Python-strings
can be enclosed in single or tripple single-quotation-marks ``'`` or
double quotation-marks ``"``. *Multi-line-strings must always be
indented by four spaces for all lines except the first line!* (The
intentation will automatically be removed before running the test.)

.. attention:: The names for fail-test must differ from the names of
    match tests! One way to do so is the mark the test with a special
    letter like "M" for match-test and "F" for fail-tests, respectively.
    E.G.: "M1", "M2", "M3", ... and "F1", "F2", "F3", ...


Running grammar-tests
---------------------

Calling the test-script
^^^^^^^^^^^^^^^^^^^^^^^

Grammar tests can be run either by calling the (auto-generated)
test-grammar script with the filename of the test-file as argument.
Alternatively, the script can be run without any arguments, in which
case it will look for a "test_grammar" folder inside the
current-directory and then run all test-files in this directory, where
any file is considered a test-file the name of which matches the
glob-pattern ``*_test_*.ini``.

.. tip:: It is a good idea to add the DHParser-projects's
    ``tst_..._grammar.py``-script to the executable tools
    of your Python-IDE. Then it suffices to simply point to
    the test in the IDE's file-manager and pick the tool
    from the menu to run a particular test.

    This works pretty well with PyCharm, but is also possible with most
    other integrated development environments or code-editors.

From the command-line grammar-tests can be run with a call like this one::

    $ tst_outline_grammar.py

to run all tests from the "tests_grammar"-subdirectory that are
contained in test-files the filename of which matches the
test-filename-pattern ``*_test_*.ini``, or, in order to run just the
tests from a single test-file::

    $ tst_outline_grammar.py tests_grammar/03_test_Outline.ini

In the above examples the project name is "outline", thus the middle
part of the test-script name "_outline_". In other project the name
of the autogenerated test-script might be different.

When calling the script with a single file-name as argument, it is
not necessary that the file-name matches the test-filename-pattern.
For example::

    $ tst_outline_grammar.py tests_grammar/Playground.ini

works just as well as long as the file "tests_grammar/Playground.ini"
exsits, even though its name does not match the test-file-name-pattern
and will, therefore, be overlooked, if the script is called without any
arguments. This can be quite useful, if you want to experiment with
tests that you might not (yet) want to add to your regular test-suite.

Reading the test-report
^^^^^^^^^^^^^^^^^^^^^^^

After the test has been run, the results can be found in the
"REPORT"-subdirectory of the tests-directory. For each test-file that
has been executed the REPORT-subdirectory contains a Markdown-file with
the detailed results.

Failures and successes as such will also directly be reported in the
terminal-output of the command. If all tests have been successful, the
last line of the terminal-output reads: "SUCCESS! All tests passed :-)".
If one or more failures occured, the number of failed tests will be
reported.

The test-code for each test will be repeated in the report-file,
followed by the abstract-syntax-tree (AST) that the code generated in the case
of (successful) match-tests or the error-messages in case of successful
fail-tests. This information is not only helpful for testing purposes,
but also for the implementation of further processing stages which rely
on the shape of the abstract syntax-tree.

In our example of the outline-parser tests,
an excerpt from the report file might look like this::

  Match-test "M3"
  ----------------

  ### Test-code:

      # Main Heading
      ## Section 1
      ### SubSection 1.1
      ### SubSection 1.2
      ## Section 2

  ### AST

      (document
        (main
          (heading "Main Heading")
          (section
            (heading "Section 1")
            (subsection
              (heading "SubSection 1.1"))
            (subsection
              (heading "SubSection 1.2")))
          (section
            (heading "Section 2"))))

    ...

    Fail-test "F2"
    ---------------

    ### Test-code:
        # Main Heading
        ## Section 1
        #### BADLY NESTED SubSubSection 1.1.1
        ### SubSection 1.2
        ## Section 2

    ### Messages:

    3:1: Error (1010): 'EOF' expected by parser 'document', but »#### BADLY...« found instead!
    3:4: Error (1040): Parser "document" stopped before end, at: »# BADLY NE...« Terminating parser.

You might expect that a test-report of the parser would show the 
concrete-syntax-tree (CST) rather than the AST. However, the CST can be
quite verbose dependning on how far it is curbed or not curbed in the
grammar definition, already (see :ref:`simplifying_syntax_trees`) and
is usually less informative than the AST. Typically, you'll want to
see it only in very particular cases and only when debugging the 
AST-generation. For this purpose, DHParser's testing-framework allows 
to quickly turn the additional output of the CST in the test-report
on and off by simply placing an asterix ``*`` after the test name
of any match test or removing it after the debugging has been done.
If for example, your test's name is "M1" you'd simply write ``M!*:
...``` in the test-ini-file.

In case a test fails, the error-messages will appear in the report-file.
DHParser will still attempt to produce an abstract-syntax-tree (AST)
and, potentially, the results of further processing stages. But these
will not necessarily represent any reasonable structures. Typically, for
example, the AST will contain nodes named "ZOMBIE\_\_" which either
capture passages of the source could which could not be parsed properly,
due to the failure or, if empty, have been added as an anchor for
error-messages.

Debugging failed tests
^^^^^^^^^^^^^^^^^^^^^^

More important is the fact that for each failed test an HTML-log will be
produced in the "LOGS"-subdirectory which resides on the same level as the
"REPORT"-subdirectory. (If this directory does not exist it will be
created the nest time a test fails. Like the REPORT-directory it can
safely be deleted, because it will always be recreated and populated
anew during the next test-run.) The HTML-log contains a detailed log of
the parsing process. This can be seen as a post-mortem debugger for
parsing that helps to find the cause of the failure of the test. The
most frequent causes for test-failures are 1) EBNF-coding-errors, i.e.
some part of the EBNF-encoded grammar does not capture or reject a piece
of the source text that it was expected to capture or reject, or 2) the
grammar does not yet encode certain constructs of the formal
target-language and needs to be extended. Here is an excerpt of the
test-log of a failed test from a converter for
Typescript-type-definitions which does not yet know the
"extends"-keyword and therefore fails a particular unit-test:

= == =================================== ======= ===========================================
L C  parser call sequence                success text matched or failed
= == =================================== ======= ===========================================
1 1  type_alias->\`export\`              DROP    export type Exact<T extends { [key: stri...
1 8  type_alias->\`type\`                DROP    type Exact<T extends { [key: string]: un...
1 13 type_alias->identifier->!\`true\`   !FAIL   Exact<T extends { [key: string]: unk ...
1 13 type_alias->identifier->!\`false\`  !FAIL   Exact<T extends { [key: string]: unk ...
1 13 type_alias->identifier->_part       MATCH   Exact<T extends { [key: string]: unknown...
1 18 type_alias->identifier->\`.\`       FAIL    <T extends { [key: string]: unknown ...
1 13 type_alias->identifier              MATCH   Exact<T extends { [key: string]: unknown...
1 18 type_alias->type_parameters->\`<\`  DROP    <T extends { [key: string]: unknown }...
. .  ...                                 ...     ...
1 19 ... ->parameter_types               MATCH   T extends { [key: string]: unknown }> = ...
1 21 type_alias->type_parameters->\`,\`  FAIL    extends { [key: string]: unknown }> ...
1 21 type_alias->type_parameters->\`>\`  FAIL    extends { [key: string]: unknown }> ...
1 21 type_alias->type_parameters         ERROR   ERROR 1010, 50 extends { [key: string]: ...
= == =================================== ======= ===========================================

Typically, the parsing-log is a quite long and the error becomes
apparaent only at the very end. So it is advisable to scroll right to
the bottom of the page to see what has caused the test to fail by
looking at the error message (which for the sake of brevity has been
ommited from the above excerpt, though the error number 1010 for
mandatory continuation errors still indicates that another item than the
following "extends" was expected).

The parsing log log's the match or non-match of every leaf-parser (i.e.
parsers that do not call other parsers but try to match the next part of
the text directly) that is applied during the parsing process. The steps
leading up to the call a leaf-parser are not recorded individually but
can be seen from the call-stack which follows the line and column-number
of the place in the document where the parser tried to match.

The match or non-match of the leaf-parser is indicated by the
success-state. There are six different success-states:

======= ==================================================================
success meaning
======= ==================================================================
MATCH   the parser matched a part of the following text
DROP    the parser matched but the matched text was dropped from the CST
FAIL    the parser failed to match the following text
!MATCH  the parser matched but as part of a negative lookahead it's a fail
!FAIL   the parser failed but as part of a negativ lookahead it's a match
ERROR   a syntax error was detected during parsing
======= ==================================================================

Finally, the last part of each entry (i.e. line) in the log is an
exceprt from the document at the location where the parser stood. In the
HTML-log, colors indicate the which part of the excerpt was matched. (In
the pure text-output as shown above this can only be inferred from the
next line.)

With these informations in mind you should be able to "read" the above
log-excerpt. It takes a while to get used to reading oarsing-logs,
though. Reading logs can become confusing when lookahead or, in
particular, when look-behind parsers are involved. Also, keep in mind
that DHParser uses memoizing to avoid parsing the same part of a
document over and over again with the same parser. Thus, if you
encounter a line in the log where the call stack appears to be clipped,
this is usually dure to memoizing an the same parser having been called
at the same location earlier in the parsing process. (You might find the
first insantance by looking for the same line and column in the earlier
part of the log.) Still, looking at the parsing-log helps to find and 
understand the causes for an unexpected parser-behavior.




Test and Development-Workflows
------------------------------

- Test Driven Grammar-Development
- Particularly useful for the restructuring of human written
  semi-formal noations with formal grammars!


Monitoring AST-creation
-----------------------

- ASTs can and should be tested, too
- No structural validation supported as of now. (Use XML-serialization
  and Relax NG for this)


Testing the processing-pipeline
-------------------------------

- Also, later stages of the processing pipeline can be tested with
  the same apparaturs as long as their results are serializable


Conventional Unit-Testing
-------------------------

- Sometimes it becomes necessary to fallback to conventional
  unit-testing.

- How this is done

