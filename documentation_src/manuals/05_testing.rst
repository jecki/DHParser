Testing
=======

DHParser provides a powerful unit-testing framework that allows testing
individual components of a grammar separately through all stages of the
:ref:`processing pipeline <processing_pipelines>`. DHParser's
unit-testing framework allows to::

    - break down the complicated process of writing a grammar into
      relatively simpler tasks of designing grammar-components each
      of which can be tested individually

    - turn writing and refactoring grammars into controlled and
      manageable process. In fact, without unit-testing, refactoring
      of grammars is hardly feasible at all.

    - extend a grammar incrementally without breaking existing code.

    - check the syntax-tree-construction as well as all subsequent
      transformations (as long as their result is serializable).

Because writing grammars can be difficult, refactoring and testing of
grammars is crucial for the success of a project. Also, one and the
same formal language can be described by different grammars and the way
the grammar is written influences the shape of the syntax-tree that the
parser yields. Therefore, it is quite common to rewrite a grammar or
parts of it more than once during the course of developing a formal
notation. For example, in the first iteration one tries to find a
grammar that matches the given or intended notation. In the second
iteration, the grammar is refined to yield a well-shaped syntax-tree to
make further processing easier. Unit-tests help to safeguard this
process against breaking earlier changes by later changes.

Writing grammar-tests
---------------------

The canonical way to write grammar-tests in a DHParser-projects is by
writing them into a file with a name that matches the the glob-pattern
``*_test_*.ini``. Tests can then simply be executed by running the
``tst_..._grammar.py``-script that has been generated by the
dhparser-command (see :ref:`full_scale_DSLs`). The format of these files
strongly resembles that of common config-files. Here is a test for the
outline-parser example from the overview (:ref:`macros`)::

    [match:document]
    M1: """# Main Heading
        ## Section 1
        ### SubSection 1.1
        ### SubSection 1.2
        ## Section 2"""

    [fail:document]
    F1: """# Main Heading
        ## Section 1
        #### SubSubSection 1.1.1  BADLY NESTED!!!
        ### SubSection 1.2
        ## Section 2"""

Test-files (".ini"-Files) are separated into sections by headings that
are enclosed in square-brackets, i.e. ``[`` and ``]``. The heading's
name consists of two parts, separated by a colon ``:``. The first part
indicates the kind of the tests that are specified under the heading.
The second part is the name of the parser that will be called with these
tests as input. Thus, the heading "[match:document]" means that the
following tests will be fed to the parser "document" and the parser
is expected to match. The following heading "[fail:document]" introduces
tests, where the parser "document" is expected to fail, i.e. the test is
successful if the parser does not match the snipped fed to the parser.

The tests themselves are specified by a name followed by a colon ``:``
followed by a single-line or multiline-string which line Python-strings
can be enclosed in single or triple single-quotation-marks ``'`` or
double quotation-marks ``"``. *Multi-line-strings must always be
indented by four spaces for all lines except the first line!* (The
indentation will automatically be removed before running the test.)

.. ATTENTION:: The names for fail-test must differ from the names of
    match tests! One way to do so is the mark the test with a special
    letter like "M" for match-test and "F" for fail-tests, respectively.
    E.G.: "M1", "M2", "M3", ... and "F1", "F2", "F3", ...


Running grammar-tests
---------------------

The test-runner-script
^^^^^^^^^^^^^^^^^^^^^^

Grammar tests can be run either by calling the (auto-generated)
test-grammar script with the filename of the test-file as argument.
Alternatively, the script can be run without any arguments, in which
case it will look for a "test_grammar" folder inside the
current-directory and then run all test-files in this directory, where
any file is considered a test-file the name of which matches the
glob-pattern ``*_test_*.ini``.

.. tip:: It is a good idea to add the DHParser-projects's
    ``tst_..._grammar.py``-script to the executable tools
    of your Python-IDE. Then it suffices to simply point to
    the test in the IDE's file-manager and pick the tool
    from the menu to run a particular test.

    This works pretty well with PyCharm, but is also possible with most
    other integrated development environments or code-editors.

From the command-line grammar-tests can be run with a call like this one::

    $ tst_outline_grammar.py

to run all tests from the "tests_grammar"-subdirectory that are
contained in test-files the filename of which matches the
test-filename-pattern ``*_test_*.ini``, or, in order to run just the
tests from a single test-file::

    $ tst_outline_grammar.py tests_grammar/03_test_Outline.ini

In the above examples the project name is "outline", thus the middle
part of the test-script name "_outline_". In other project the name
of the autogenerated test-script might be different.

When calling the script with a single file-name as argument, it is
not necessary that the file-name matches the test-filename-pattern.
For example::

    $ tst_outline_grammar.py tests_grammar/Playground.ini

works just as well as long as the file "tests_grammar/Playground.ini"
exists, even though its name does not match the test-file-name-pattern
and will, therefore, be overlooked, if the script is called without any
arguments. This can be quite useful, if you want to experiment with
tests that you might not (yet) want to add to your regular test-suite.

.. TIP:: It is a good idea to add the DHParser-projects's
    ``tst_..._grammar.py``-script to the executable tools
    of your Python-IDE. Then it suffices to simply point to
    the test in the IDE's file-manager and pick the tool
    from the menu to run a particular test.

    This works pretty well with PyCharm, but is also possible with most
    other integrated development environments or code-editors.

Test-reports
^^^^^^^^^^^^

After the test has been run, the results can be found in the
"REPORT"-subdirectory of the tests-directory. For each test-file that
has been executed the REPORT-subdirectory contains a Markdown-file with
the detailed results.

Failures and successes as such will also directly be reported in the
terminal-output of the command. If all tests have been successful, the
last line of the terminal-output reads: "SUCCESS! All tests passed :-)".
If one or more failures occurred, the number of failed tests will be
reported.

The test-code for each test will be repeated in the report-file,
followed by the abstract-syntax-tree (AST) that the code generated in the case
of (successful) match-tests or the error-messages in case of successful
fail-tests. This information is not only helpful for testing purposes,
but also for the implementation of further processing stages which rely
on the shape of the abstract syntax-tree.

In our example of the outline-parser tests,
an excerpt from the report file might look like this::

  Match-test "M3"
  ----------------

  ### Test-code:

      # Main Heading
      ## Section 1
      ### SubSection 1.1
      ### SubSection 1.2
      ## Section 2

  ### AST

      (document
        (main
          (heading "Main Heading")
          (section
            (heading "Section 1")
            (subsection
              (heading "SubSection 1.1"))
            (subsection
              (heading "SubSection 1.2")))
          (section
            (heading "Section 2"))))

    ...

    Fail-test "F2"
    ---------------

    ### Test-code:
        # Main Heading
        ## Section 1
        #### BADLY NESTED SubSubSection 1.1.1
        ### SubSection 1.2
        ## Section 2

    ### Messages:

    3:1: Error (1010): 'EOF' expected by parser 'document', but »#### BADLY...« found instead!
    3:4: Error (1040): Parser "document" stopped before end, at: »# BADLY NE...« Terminating parser.

You might expect that a test-report of the parser would show the
concrete-syntax-tree (CST) rather than the AST. However, the CST can be
quite verbose depending on how far it is curbed or not curbed in the
grammar definition, already (see :ref:`simplifying_syntax_trees`) and
is usually less informative than the AST. Typically, you'll want to
see it only in very particular cases and only when debugging the
AST-generation. For this purpose, DHParser's testing-framework allows
to quickly turn the additional output of the CST in the test-report
on and off by simply placing an asterix ``*`` after the test name
of any match test or removing it after the debugging has been done.
If for example, your test's name is "M1" you'd simply write ``M!*:
...``` in the test-ini-file.

In case a test fails, the error-messages will appear in the report-file.
DHParser will still attempt to produce an abstract-syntax-tree (AST)
and, potentially, the results of further processing stages. But these
will not necessarily represent any reasonable structures. Typically, for
example, the AST will contain nodes named "ZOMBIE\_\_" which either
capture passages of the source could which could not be parsed properly,
due to the failure or, if empty, have been added as an anchor for
error-messages.

Debugging failed tests
^^^^^^^^^^^^^^^^^^^^^^

More important is the fact that for each failed test an HTML-log will be
produced in the "LOGS"-subdirectory which resides on the same level as the
"REPORT"-subdirectory. (If this directory does not exist it will be
created the nest time a test fails. Like the REPORT-directory it can
safely be deleted, because it will always be recreated and populated
anew during the next test-run.) The HTML-log contains a detailed log of
the parsing process. This can be seen as a post-mortem debugger for
parsing that helps to find the cause of the failure of the test. The
most frequent causes for test-failures are 1) EBNF-coding-errors, i.e.
some part of the EBNF-encoded grammar does not capture or reject a piece
of the source text that it was expected to capture or reject, or 2) the
grammar does not yet encode certain constructs of the formal
target-language and needs to be extended. Here is an excerpt of the
test-log of a failed test from a converter for
Typescript-type-definitions which does not yet know the
"extends"-keyword and therefore fails a particular unit-test:

= == =================================== ======= ===========================================
L C  parser call sequence                success text matched or failed
= == =================================== ======= ===========================================
1 1  type_alias->\`export\`              DROP    export type Exact<T extends { [key: stri...
1 8  type_alias->\`type\`                DROP    type Exact<T extends { [key: string]: un...
1 13 type_alias->identifier->!\`true\`   !FAIL   Exact<T extends { [key: string]: unk ...
1 13 type_alias->identifier->!\`false\`  !FAIL   Exact<T extends { [key: string]: unk ...
1 13 type_alias->identifier->_part       MATCH   Exact<T extends { [key: string]: unknown...
1 18 type_alias->identifier->\`.\`       FAIL    <T extends { [key: string]: unknown ...
1 13 type_alias->identifier              MATCH   Exact<T extends { [key: string]: unknown...
1 18 type_alias->type_parameters->\`<\`  DROP    <T extends { [key: string]: unknown }...
. .  ...                                 ...     ...
1 19 ... ->parameter_types               MATCH   T extends { [key: string]: unknown }> = ...
1 21 type_alias->type_parameters->\`,\`  FAIL    extends { [key: string]: unknown }> ...
1 21 type_alias->type_parameters->\`>\`  FAIL    extends { [key: string]: unknown }> ...
1 21 type_alias->type_parameters         ERROR   ERROR 1010, 50 extends { [key: string]: ...
= == =================================== ======= ===========================================

Typically, the parsing-log is a quite long and the error becomes
apparaent only at the very end. So it is advisable to scroll right to
the bottom of the page to see what has caused the test to fail by
looking at the error message (which for the sake of brevity has been
omitted from the above excerpt, though the error number 1010 for
mandatory continuation errors still indicates that another item than the
following "extends" was expected).

The parsing log log's the match or non-match of every leaf-parser (i.e.
parsers that do not call other parsers but try to match the next part of
the text directly) that is applied during the parsing process. The steps
leading up to the call a leaf-parser are not recorded individually but
can be seen from the call-stack which follows the line and column-number
of the place in the document where the parser tried to match.

The match or non-match of the leaf-parser is indicated by the
success-state. There are six different success-states:

======= ==================================================================
success meaning
======= ==================================================================
MATCH   the parser matched a part of the following text
DROP    the parser matched but the matched text was dropped from the CST
FAIL    the parser failed to match the following text
!MATCH  the parser matched but as part of a negative lookahead it's a fail
!FAIL   the parser failed but as part of a negativ lookahead it's a match
ERROR   a syntax error was detected during parsing
======= ==================================================================

Finally, the last part of each entry (i.e. line) in the log is an
excerpt from the document at the location where the parser stood. In the
HTML-log, colors indicate the which part of the excerpt was matched. (In
the pure text-output as shown above this can only be inferred from the
next line.)

With these information in mind you should be able to "read" the above
log-excerpt. It takes a while to get used to reading parsing-logs,
though. Reading logs can become confusing when lookahead or, in
particular, when look-behind parsers are involved. Also, keep in mind
that DHParser uses memoizing to avoid parsing the same part of a
document over and over again with the same parser. Thus, if you
encounter a line in the log where the call stack appears to be clipped,
this is usually due to memoizing an the same parser having been called
at the same location earlier in the parsing process. (You might find the
first instance by looking for the same line and column in the earlier
part of the log.) Still, looking at the parsing-log helps to find and
understand the causes of unexpected parser-behavior, quickly.

.. TIP:: Parsing-logs are by default only generated for failed test.
    In case you'd like to see the parsing-log for a successful test,
    a simple trick is to flip the type of the test from "match" to
    "fail" in the .ini-file or vice versa.

    The test with the flipped type will then be reported as a failure,
    but the parsing-log is just the same as if it was a success. Once,
    you have seen the log, you can flip the type back again to get
    a correct test-report.


Development-Workflows
---------------------

The development workflows for writing parsers for domain specific
languages (DSLs) or parsing (semi-)structured text-data are very similar.
Only that in the latter case there already exists plenty of sample
material while in the former case one would usually start to draw
up some examples.

In both cases, however, it requires going through many iterations
of adjustments and refinements before the grammar stands. In the
case of a DSL, the even DSL itself might be adjusted in the course of the
development, requiring further changes of the grammar all alike.

This is where test-driven-grammar development comes into play. Before
even writing a grammar and running it on complete documents, you
start with a small subset that you gradually extend. There are basically
to stratgies for grammar-development:

   1. Top-Down-Grammar development, where one starts with the macro-
      structure and uses summary parsers to gloss over the
      microstructure, which will be replaced later.

   2. Bottom-Up-Grammar development, where yoou start with parsers
      for the parts of the documents and later connect them with
      higher level parsers.

Of course, it is also possible to work from both ends and to follow
both strategies at the same time, until the top-down and
bottom-up-development meets in the middle.

We will look at both strategies with the example of our outline-parser
in the following. In case you want to reenact the following steps, you
should start by creating a new project with the dhparser-command::

    $ dhparser Markdown
    $ cd Markdown

Top-Down-Grammar-Development
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Suppose, you'd like to write a Markdown-parser, then with a
top-down-strategy you'd start with the outer-elements which in this case
is the outline of the document, i.e. the structure of headings and
sub-headings. In the true spirit of test-driven-development we start
by writing some tests, before even coding the first draft of our
grammar. So we add a document ``tests\01_test_outline.ini`` to a
freshly created project with the following content::

    [match:document]
    M1: """# Main Heading
        ## Section 1
        ### SubSection 1.1
        ### SubSection 1.2
        ## Section 2"""

    [fail:document]
    F1: """# Main Heading
        ## Section 1
        #### BADLY NESTED SubSubSection 1.1.1
        ### SubSection 1.2
        ## Section 2"""

The meaning of these two test-cases should be obvious: The first is a
document that only contains an outline, but not yet any content -
because will start writing our grammar top-down with the definition
of the outline-elements leaving out the content-elements for now. The
match-test test will check that our grammar matches a properly formed
document-outline.

The second is a fail test, which checks that the parser for our grammar
does not accidently match a badly structured document. Now, we will
start writing a grammar that is suitable to cpature the snippet from
our match-test. As you'll see in the following, this already requires
quite a few definitions. Here is our first attempt (which still
contains a mistake!)::

    # First attempt of any outline-grammar. Can you spot the error?

    #  EBNF-Directives
    @ whitespace  = /[ \t]*/  # only horizontal whitespace, no linefeeds
    @ reduction   = merge     # simplify tree as much as possible
    @ disposable  = WS, EOF, LINE, LFF
    @ drop        = WS, EOF, backticked, whitespace

    #:  Outline
    document = [WS] main [WS] §EOF

    main  = `#` ~ heading { WS section }
    section  = `##` ~ heading { WS subsection }
    subsection  = `###` ~ heading { WS subsubsection }
    subsubsection  = `####` ~ heading { WS s5section }
    s5section  = `#####` ~ heading { WS s6section }
    s6section  = `######` ~ heading

    heading = LINE

    #:  Regular Expressions
    LINE = /[^\n]+/         # everything up to the next linefeed
    LFF  = /(?:[ \t]*\n)+/  # any ws at line-end and all following empty lines
    WS   = LFF              # same as LFF, but will be dropped
    EOF  =  !/./  # no more characters ahead, end of file reached

When running the grammar-tests, we notice that while the match-test
passes as expected, the fail-test fails, that is, it captures the badly
structured outline, although it shouldn't. The output of the
tst-grammar-script on the console looks like this::

    GRAMMAR TEST UNIT: 01_test_outline
      Match-Tests for parser "document"
        match-test "M1" ... OK
      Fail-Tests for parser "document"
        fail-test  "F1" ... FAIL

Can you guess why the fail-test did not pass? If
not it helps to cast a look at the parsing log of the failed test
that has been stored in file
"tests/LOGS/fail_document_F2_parser.log.html".
There you find the suspicious lines:

= == ================================================= ===== ===========================================
3	1	document->main->section->subsection-> `###`	       DROP	 #### BADLY NESTED SubSubSection 1.1.1 ##...
3	4	document->main->section->subsection->heading->LINE MATCH # BADLY NESTED SubSubSection 1.1.1 ### S...
= == ================================================= ===== ===========================================

Obviously, the parser "subsection" found its marker consiting
of three ``#``-signs, but then it did not stop short at the next
``#``-sign, but left this to be captured by its "heading"-parser
which simply reads the rest of the line, no matter what it looks like.

The remedy is simple: We add a negative lookahead to check that
after each heading-marker that no further ``#``-sign follows.
Otherwise, the respective section, subsection, etc. -parser
simply won't match. So, in the "Outline"-section of our grammar,
we change the following definitions, accordingly::

    main  = [WS] `#` !`#` ~ heading { WS section }
    section  = `##` !`#` ~ heading { WS subsection }
    subsection  = `###` !`#` ~ heading { WS subsubsection }
    subsubsection  = `####` !`#` ~ heading { WS s5section }
    s5section  = `#####` !`#` ~ heading { WS s6section }
    s6section  = `######` !`#` ~ heading

This time the grammar-tests yield the desired result::

    GRAMMAR TEST UNIT: 01_test_outline
      Match-Tests for parser "document"
        match-test "M1" ... OK
      Fail-Tests for parser "document"
        fail-test  "F1" ... OK

.. NOTE:: While not important for the topic of testing as such, a few
    design-decisions of the EBNF-grammar of the outline-example might
    be of interest for beginners:

    1. Since the structure of the outline is preseverd by the structre
       of the abstract syntax tree (i.e. the names and the nesting of
       its nodes) all tokens (#, ##, ...) and delimiters (WS) are dropped
       during parsing (see the ``@drop``-directive at the beginning
       of the grammar). Dropping Tokens, Delimiters and insiginificant
       whitespace is common practice - either when generating the AST
       or - as done here - during parsing already.

    2. No normalization is being done at the parsing stage. For example,
       headings as definied here may still contain trailing whitespace.
       Unless it organically results from the grammar definition,
       normalization is better done in the CST-AST-transformation stage
       to keep the grammar simple.

    3. It is a good practice to give the symbols that are considered
       disposable (i.e. they do not appear as node names in the syntax tree,
       alghouth their content is preserved) or which will be dropped (i.e.
       neither their name nor the captured content makes it into the syntax-tree)
       special, well recognizable names, like for example, names starting with
       a single underscore for disposable symbols and a double leading underscore
       for symbols to be dropped. However, in this simple example we do not
       follow this practice for the sake of readability.

Before going further down with our top-down-design of the grammar, we draw 
up a test-case that contains more structural details. For this purpose we
add under the heading ``[match:document]`` another test-case with a little
more structure::

    M2: """# Main Heading

        Some introductory Text

        ## Section 1
        One paragraph of text 

        Another paragraph of text. This
        time stretching over several lines.

        ## Section 2
        ### Section 2.1
        ### Section 2.2

        The previous section is (still) empty.
        This one is not.
        """

If we run the test now, it will expectedly fail with an error message like
"3:1: Error (1010): 'EOF' expected by parser 'document',
but »Some intro...« found instead!". Before the test succeeds, we need to
extend out grammar so as to capture the content inside of sections
as well. In true top-down fashion, first, we provide for the new content
elements which we will call "blacks" in the definiens of the section-elements::

    main  = [WS] `#` !`#` ~ heading [WS blocks] { WS section }
    section  = `##` !`#` ~ heading [WS blocks] { WS subsection }
    subsection  = `###` !`#` ~ heading [WS blocks] { WS subsubsection }
    subsubsection  = `####` !`#` ~ heading [WS blocks] { WS s5section }
    s5section  = `#####` !`#` ~ heading [WS blocks] { WS s6section }
    s6section  = `######` !`#` ~ heading [WS blocks]

Then, we define the the "blocks"-element::

    blocks  = !is_heading LINE { LFF !is_heading LINE }
    is_heading = /##?#?#?#?#?(?!#)/

.. NOTE:: Note that in the definition of "blocks" we use "LFF" instead of "WS" although
    they are synonyms for the same whitespace-parser, because other than in
    the definition of the section-structure the whitespace (including empty lines)
    does not serve as a delimiter but is part of the content, for example in a
    block consisting of multiple paragraphs.

This time, the grammar passes the recently added test. However, the new
element "blocks" is sill a *placeholder* that does not capture the individual
paragraphs, let alone other elements like lists or enumerations, as can easily
be seen by looking at the generated abstract-syntax-tree (AST) in the
test-report::

    (document
      (main
        (heading "Main Heading")
        (blocks "Some introductory Text")
        (section
          (heading "Section 1")
          (blocks
            "One paragraph of text "
            ""
            "Another paragraph of text. This"
            "time stretching over several lines."))
        (section
          (heading "Section 2")
          (subsection
            (heading "Section 2.1"))
          (subsection
            (heading "Section 2.2")
            (blocks
              "The previous section is (still) empty."
              "This one is not.")))))

This use of "placeholder"-parsers which sweepingly capture larger
chunks of text without dissecting their detailed structur is typical
for the top-down approach. We could continue by replacing (or amending)
the "blocks"-parser stepwise with more detailed parsers that
capture individual paragraphs, highlighted passages etc., possibly
making use of AST-tests (see below) in the process.

However, we will now turn the tables and start with the detail-
or "fine"-structure of our outlined text in order to see how the
bottom-up-approach works.

Bottom-Up-Grammar-Development
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For the bottom-up-approach one must first consider what are the
smallest elements that need to be semantically captured. Surely,
it would be exagerated to capture individual letters. One might
think of words and lines, but then individual words do not really
matter in Markdown-texts and lines have the disadvantage that
highlighted elements might stretch over several lines.

A possible choice are pieces of text
consisting of letters, punctuation and whitespace the may but
do not need to stretch over more than one single line, that is,
they may also contain line-feeds, but they should not encompass
empty lines. So, basically text is no-whitespace elements
interspersed by whitespace and single-linefeeds. Let's
first write a few tests and then cast this into a formal definition,
which in my humble opion is even clearer than the verbal expression.
Here are the tests::

    [match:TEXT]
    M1: "A bit of text."
    M2: """A bit of text
        over two lines!"""

    [fail:TEXT]
    F1: "  No leading whitespace"
    F2: """Empty lines

         separate paragraphs!"""


And here is the definition of a piece of text (which, as is typical
for the most atomic parsers, consist mostly of regular expressions
enclosed by slashes)::

    TEXT      = /[^\s]+/ { (LF | L) /[^\s]+/ }
    L         = /[ \t]+/           # significant whitespace
    LF        = ~/\n(?![ \t]*\n)/  # a single linefeed

TO BE CONTINUED...



- Test Driven Grammar-Development
- Particularly useful for the re-structuring of human written
  semi-formal noations with formal grammars!


Monitoring AST-creation
-----------------------

- ASTs can and should be tested, too
- No structural validation supported as of now. (Use XML-serialization
  and Relax NG for this)


Testing the processing-pipeline
-------------------------------

- Also, later stages of the processing pipeline can be tested with
  the same apparatus as long as their results are serializable


Conventional Unit-Testing
-------------------------

- Sometimes it becomes necessary to fallback to conventional
  unit-testing.

- How this is done

