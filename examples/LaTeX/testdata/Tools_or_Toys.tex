%\documentclass[12pt, a4paper]{article}
\documentclass[onecollarge]{STJour}
\usepackage[USenglish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{ucs} % unicode
\usepackage[T1]{fontenc}
\usepackage{t1enc}
\usepackage{type1cm}
\usepackage{times} 
\usepackage{setspace}
\smartqed  % flush right qed marks, e.g. at end of proof
%\usepackage[square,sort&compress,comma,numbers]{natbib}	% fuer BibTeX
\usepackage[sort&compress]{natbib}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{mathptmx}      % use Times fonts similar to Windows Office
%\usepackage{german}		% Apply if you wish to write in German
\usepackage{exscale}
\usepackage{psfrag}
\usepackage{layout}
\usepackage[dvips]{color}
\usepackage{eurosym}  
\usepackage{graphicx}
\usepackage{rotating}

\numberwithin{equation}{section}
\sloppy
 
\begin{document} 
%\onehalfspacing

\unitlength1cm 
%
\title{Tools or Toys?}
%\footnote{Part of the research for this paper has been funded
%by the German Research Foundation (DFG) within the Cluster of
%Excellence in Simulation Technology (EXC 310/1) at the University of
%Stuttgart}
\subtitle{On Specific Challenges for Modeling and the
Epistemology of Models and Computer Simulations in the Social
Sciences\\[0.2cm]
% - Paper for the Models \& Simulations 4 Conference, Toronto, May 2010
% -
}

%
% Define if Title is too long for running head
\titlerunning{Tools or Toys: Computer Simulations in the Social
Sciences}
%
\author{Eckhart Arnold}
%
\institute{% $^a$ ...
  Institute of Philosophy, University of Stuttgart,\\
  Seidenstraße 36, \\
  70174 Stuttgart, Germany \\[2mm]
  eckhart.arnold@philo.uni-stuttgart.de\\
%  www.eckhartarnold.de
  http://www.uni-stuttgart.de/philo/index.php?id=1043\\[2mm]
} 
%
% Define if Authorlist is too long for running head
\authorrunning{Eckhart Arnold}
%
% Convention: year  - issue in that year
\SimTechIssue{2010-36}
\date{July 12th 2010}
 
\maketitle

\begin{abstract}
\onehalfspacing
Mathematical models are a well established tool in most 
natural sciences. Although models have been neglected by the
philosophy of science for a long time, their epistemological status as a
link between theory and reality is now fairly well understood.
However, regarding the epistemological status of mathematical models
in the social sciences, there still exists a considerable unclarity.

In my paper I argue that this results from specific challenges that
mathematical models and especially computer simulations face in the
social sciences. The most important difference between the social
sciences and the natural sciences with respect to modeling is that in
the social sciences powerful and well confirmed background theories (like
Newtonian mechanics, quantum mechanics or the theory of relativity in
physics) do not exist. Therefore, an epistemology
of models that is formed on the role model of physics may not be
appropriate for the social sciences. I discuss the challenges that
modeling faces in the social sciences and point out their epistemological
consequences. The most important consequences are that greater emphasis
must be placed on empirical validation than on theoretical validation and
that the relevance of purely theoretical simulations is strongly limited.

\end{abstract}

\doublespacing

\tableofcontents 

\section{Introduction}

Models play a central role in many sciences. One can safely say that the
construction, analysis, discussion and validation of models is the daily
bread of most researchers in the natural sciences and engineering.
With the development of simulation technology the scope of
modeling has vastly increased and it has become even more obvious that
models have a ``life of their own'' independent from theories. 

However, there is quite a difference between the extent to which models
are used in the natural sciences and in the social social sciences.
While in the natural sciences and engineering models are a standard
tool and their use is undisputed, it is in the social sciences
only economics, where models are the standard form of articulating
hypotheses or causal assumptions. In some branches of the social
sciences, like history, models are usually not used at
all. And in other branches the use of mathematical models depends on
the particular school that one adheres to. In sociology and political
sciences, for example, it is the rational choice school that makes the
strongest use of models.

But even in those parts of the social sciences that use models there
exists a considerable unclairity about their epistemic role. It often
remains unclear what conclusions can be drawn from models and if and how
they can tell us something about the world. The kind of discussion about
the role of models in economics that was triggered by Robert Sugden's
credible world account \citep{sugden:2000} would appear surprising to a
natural scientist. And when Robert Sugden wonders that in economics
``authors typically say very little about how their models relate to the
real world'' \citep[p.\ 25]{sugden:2009} then he expresses an
embarassement that is quite uncommon in the the natural sciences. With the possible
exception of economics, it is not only the epistemic status of models
that is under discussion in the social sciences, but often it is disputed
whether models are of any use at all. The respective discussion blends
into the controversies about the usefulness or uselessness of mathematical
methods in the social sciences in general \citep{green-shapiro:1994,
shapiro:2005}.

Leaving the ideological question whether everything that happens in this
world can meaningfully be rendered in mathematical terms aside, these
facts about the social sciences raise the question why there is such an
unclarity about the possible role and function of models. In its
broadest outline the lesson that I believe can be learned from examining
this question, can be summarized in four theses:

\begin{enumerate}
  \item The possible role and function of models in the social sciences
  is often unclear, because models in the social sciences face specific
  challenges which indeed limit the scope of their useful
  employment.
  \item Because of this, the epistemic role of models in the social
  sciences cannot properly be understood in analogy to the role of
  models in the natural sciences.
  \item An epistemology of models that takes into account these
  challenges can help us to better understand the possible
  role and function of models in the social sciences. It might also
  help us to figure out when and how models and computer simulations
  can be applied usefully (where ``usefully'' means: ``in such a way
  that we can learn something about the world from them'').
  \item As a side effect this may also help us to understand
  difficulties that models face in the natural sciences in those
  situations where similar conditions hold as appear to be the standard case in
  the social sciences (e.g. insufficient background knowledge about the processes
  involved, strong measurement inaccuracies).
\end{enumerate}

In this article I shall mainly be concerned with the
epistemology of models and simulations as traditionally understood and
how it relates to the specific challenges that models face in the
social sciences. 
% The object of the epistemology of models is to clarify and answer the two
% questions: 1) What are models? 2) When and how do they prove? The
% first question asks for the ontological status of models as well as for
% their function in science. The second question concerncs the conditions
% and criteria of validity for models. It is primarily the second question
% that I am interested in. But, of course, both questions are interrelated,
% because the validation criteria depend on what function is
% assigned to a model in the chain of scientific reasoning.
I first state what I consider to be the standard
concept of models in the philosophy of science, namely, that
models are links between theory and empirical reality
\citep{morgan-morrison:1999, winsberg:2003}. In this context I also
discuss the more controversial question what status is to be ascribed to
computer simulations. My position is that 1) computer simulations are
just models that run on the computer 2) they, therefore, raise more or
less the same epistemological questions as models and 3) computer
simulations are not experiments, although they may under certain
conditions replace experiments. I illustrate this with an example of
a computer simulation from chemistry. The example shows that the
conception of models as links between theory and reality is by and large
appropriate. 

After that I go on to review some of the viewpoints on models in the
social sciences. These are highly diverse, which emphasizes that there is
a considerable unclarity about what role models play in the social
sciences. More importantly, the conception of models as a link between
theory and empirical phenomena captures at best a rare exceptional case
in the social sciences. This naturally raises the question of what
distinguishes the social sciences and why models, therefore, play a
different and generally much less prominent role in these sciences. I run
through a number of distinguishing features which are apt to explain this
difference and point out their epistemological consequences. I conclude
with some suggestions for adjusting the epistemology of models so as to
better capture the role of models in the social sciences.


\section{The role of models in science}

\subsection{The nature of models}

In the recent philosophy of science literature it has become popular to
conceive models as ``mediators'' between theory and empirical
reality \citep{morgan-morrison:1999} or, what amounts to more or less
the same, as a link between theory and reality \citep{winsberg:2001}. I
take this notion to imply three core aspects about models:

\begin{enumerate}
  \item {\em Theoretical foundation}: Models are at least partially
  derived from theories. They contain ``laws of nature'' that are taken from
  one or more background theories. 
  \item {\em Semi-autonomy}:\label{semi-autonomy} Models are not just
  logical consequences from theories and facts. The
  construction of models involves model building techniques 
  like simplification and approximation, the
  inclusion of auxiliary assumptions or other ``tricks of trade''.
  In extreme cases a model may even contain assumptions that contradict
  the theory \citep[2.14]{k_uuml_ppers2005}.
  \item {\em Relation to a target system}: Models are related to target
  systems in the real world. The relation between model and target system is at
  least one of similarity.
\end{enumerate}

I do not claim that this concept of a model covers all types of
models that occur in science, but only that it describes the most commen
kinds of models. In fact, by dropping any of these requirements
meaningful boundary cases can be derived: If a model does not have a
theoretical foundation, it is either a purely phenomenological model or a
model of data. If a model is not partially independent from its
background theories, i.e. if it can logically be derived from the theory
and known facts, then it is not a model any more but simply the
application of the theory to a particular instance of the laws of the
theory. Finally, if the model is not related to a particular target
system then it is a purely theoretical model or a ``speculative model''
or a ``toy model''. 

Saying that a model provides a link between theory and reality is 
not meant to imply that models are the only possible way how theory can be
linked to reality. A theory can also be linked to reality via
intermediate theories or by simple ``application of the theory''
\label{simple-application} in the just mentioned sense. It is true that
very often theories cannot be applied in a simple straight-forward manner
but still sometimes some theories can.


\subsection{Where models get their credentials from}

If models are mediators between theory and reality in the just described
sense, then the next question would be what gives models their
credibility or how they can be validated. Obviously, if models are
partially independent from theory, we cannot rely on the credibility of
the background theory alone. Instead, models draw their credibility
from three different sources: 

\begin{enumerate}

\item {\em Credible background theory and background knowledge}: In so
far as it makes use of background theories, the models' credibility
depends on the credibility of the background theory. And the validity of
the model depends on how faithful it is to the background theories
(where it makes use of them). The same holds for any factual
background knowledge that is incorporated into a model.

\item {\em Well approved modeling techniques}:\label{techniques} A model
furthermore draws credibility from the credibility of modeling techniques
that have been employed in its construction \citep[]{winsberg:2006}.
These techniques in turn are credible either because they can be analyzed
or tested with regards to their reliability or simply because they have
been successfully employed in the past.\footnote{In the latter case,
however, their success must at least at some point in the past have been
assessed by more direct means.} In order to validate this aspect of the
model one would have to inquire into the reliability of the modeling
techniques used and also check whether they have been employed
correctly.

\item {\em Successful empirical tests}: Finally, a model derives
credibility from being in accordance with the target system as assessed
by empirical tests. The direct validation through empirical testing may
not always be possible, though. In fact, one of the most
important uses of computer simulations is as substitutes for
experiments in cases where experiments are costly or impossible.

\end{enumerate} 

If these are the sources of credibility for models, then the question
arises if and how they are be related to each other.  The following two
conjectures about the mutual relation of these sources seem reasonable:

\begin{enumerate}
  
\item {\em Precedence of empirical validation}:\label{precedence} If
reliable empirical tests of a model are available, 
then empirical validation takes precedence over the other validation
paths. This means: If a model does not seem to be valid in terms its
theoretical assumptions or the employed modeling techniques 
but withstands empirical testing nonetheless
then the model is still acceptable if only as a phenomenological model.
The precedence of empirical testing as a validation criterion reflects 
the epistemic primacy of empirical facts in science. 

\item {\em Synergy of credibility sources}:\label{synergy} The less one
can rely on a particular one of the three above mentioned sources of
credibility, the more strain is put on the remaining sources. E.g. if
empirical testing of the model is not possible then the more important it
becomes to be able to rely on a well confirmed background theory or on
well-proven and reliable modeling techniques.

\end{enumerate}

It seems reasonable to distinguish the models that derive their
credibility primarily from the reliance on background theories,
background knowledge and modeling techniques from those that are
validated by direct empirical testing. The former could be termed ``{\em
input-controlled}'' models and the latter ``{\em output-controlled}''
models. The distinction is of course one of ``more or less''. Some models
may be both input and output controlled. This distinction is meaningful,
because with these two ideal types of models are associated quite
different modes of validation. With this terminological convention no
general assumption is made about the relatively greater or smaller
reliability of the one or the other. But it stands to reason that
different levels of credibility or reliability might be associated with
input or output-controlled models in specific contexts.

There is not much more that can be said about the credibility of models
on this very general level. Further below a case study will be discussed
in order to show how these three sources of credibility come into play in
a simulation model. But before, a few things need to be said to justify
why the terms ``model'' and ``computer simulations'' are used more or less
interchangeably in this paper.

\section{Why computer simulations are merely models and not experiments} 

With regards to the nature and epistemic role of computer simulations,
a definite consensus has not yet been reached by philosophers of
science. I do not intend to enter into all the philosophical disputes
about computer simulations. But I'd merely like to defend two positions
that are important in the context of my article:

\begin{enumerate}
  \item Computer simulations are models. Therefore, computer simulations
  do not raise any other epistemological questions than
  models. In particular, the burden of validation is exactly the
  same for models and simulations.

  \item Computer simulations are not experiments. There is a sharp
  distinction between computer simulations and models on the one hand
  side and experiments on the other hand side. Just like models and
  theories, computer simulations belong to the theoretical side of
  science as opposed to the empirical side which encompasses experiments,
  observations and experiences.\footnote{To avoid a possible source of
  misunderstanding: A theory or model is per se considered to be a
  theoretical entity notwithstanding its higher or lower degree of
  empirical accuracy and confirmation. So even a well-tested theory about
  empirical objects is still something theoretical in this sense. It is
  only the objects themselves (i.e. the objects a theory describes) as
  well as the sort of actions that scientists perform in order to study
  empirical objects (i.e. experiments, observations, measurements)
  which I count to the ``empirical side''.}

%   Computer
%   simulations do only yield knowledge that is already implied in the
%   theories and prior assumptions that enter in the
%   simulation. But computer simulations do not yield any ``new''
%   empirical knowledge. An important consequence is that even though
%   computer simulations may serve as a surrogate for some types of
%   experiments, a simulation can never replace an {\em experimentum
%   crucis} by which the truth or falsehood of a scientific theory is decided.
\end{enumerate}

\subsection{Computer simulations are just elaborate models}

Quite a few authors have claimed that computer simulations, being a
revolutionary new tool of science, call for a new kind of philosophy or
require an epistemology of their own that pays due credit to their
distinctive character \citep{humphreys:2009, humphreys:2004,
winsberg:2001}. The acclaimed novelty of computer simulations has been
examined in detail by Roman Frigg and Julian Reiss
\citep{frigg-reiss:2009}, who come to the conclusion that computer
simulations do not raise any new or substantially different philosophical
question from those that are discussed in the philosophy of science
already. Here I am mostly concerned with the issue of validation of
models. Frigg's and Reiss' most important point regarding the issue of
validation comes up in connection with Winsberg's notion that the
specific epistemological features of simulations are that they are
constructed ``downward'' (i.e. starting from a theory), ``autonomous''
(from empirical data which may not or only sparsely be available for the
simulated process) and ``motley'' (i.e. partially independent from
theory, freely mixing ad-hoc assumptions and assumptions from the
theoretical backgrounds) \citep[p.\ 447/448]{winsberg:2001}. With respect
to these features, Frigg and Reiss contend:

\begin{quote}

\ldots it is hard to see, at least
without further qualifications, how justification could derive from
construction in this way. There does not seem to be a reason to believe
that the result of a simulation is credible just because it has been
obtained using a downward, autonomous and motley process. In fact, there
are models that satisfy these criteria and whose results are nevertheless
not trustworthy. \citep[p.\ 600]{frigg-reiss:2009}

\end{quote}

An important pragmatic point made by Frigg and Reiss is that putting too
much emphasis on the question of novelty of simulations may divert the
attention of philosophers of science from more important and relevant
questions:

\begin{quote}

Blinkered by the emphasis on novelty and the constant urge to show that
simulations are unlike anything we have seen before, we cannot see how
the problems raised by simulations relate to exiting problems and we so
forgo the possibility to have the discussions about simulation make
contributions to the advancement of these debates. [\ldots]

For instance, if \ldots we recognise that the epistemological problems
presented to us by simulations have much in common with the ones that
arise in connection with models, we can take the insights we gain in both
fields together and try to make progress in constructing the sought-after
new epistemology. \citep[p.\ 611]{frigg-reiss:2009}

\end{quote}

Frigg's and Reiss' view that computer simulations do not introduce new
issues to the philosophy of science has been criticised by Humphreys
\citeyearpar{humphreys:2009}. Rather than repeating the arguments by
Frigg and Reiss, with which I largely emphasize, I am going to discuss the main
counter arguments by Humphreys as far as they may have a bearing on the
question of validation. What remains to be done is to show that
simulations are models and not experiments, for this is an issue with
respect to which Frigg and Reiss remain neutral.
 
Those of Humphreys' arguments for the novelty of computer simulations
that are potentially relevant for the validation issue concern 1) the
epistemic opacity of simulations, 2) the different semantics of
simulations, 3) the temporal dynamics of simulations and 4) the crucial
difference between ``in principle'' and ``in practice'' that according
to Humphreys deserves special attention once simulations enter the scene.

1) According to Humphreys, computer simulations are epistemically opaque
because we cannot monitor every single step (``epistemically relevant
elements of the process'' \citep[p.\ 618]{humphreys:2009}) of a
simulation that may run through many millions and billions of
``epistemically relevant'' steps before it produces its result. Does this
have any bearing on the validation of simulations? If it does then it can
only mean that simulations are a comparatively more dangerous tool than
models, because many simulations are opaque in Humphrey's sense. However,
since we do know and understand the algorithms programmed into a
simulation, and since we can probably at least monitor a few samples of
the ``epistemically relevant elements'' of the simulation process, this
kind of opacity may not pose too much of a problem for the justification
of the simulation.

2) Humphreys believes that neither the syntactic nor the semantic view of
theories are fully adequate to capture just how simulations relate to
target systems. According to him this relation differs from how
traditional models are applied: ``It is in replacing the explicitly
deductive relation between the axioms and the prediction by a discrete
computational process that is carried out in a real computational device
that the difference lies.'' \citep[p.\ 620]{humphreys:2009}. The aspect
that Humphreys hints at is the ``semi-autonomy'' of models from theory
(see point \ref{semi-autonomy} on page \pageref{semi-autonomy}). So, this
aspect is already taken care of. 

I am not sure what exactly falls under the category of ``traditional
models'', but I doubt that it is ultimately just computer simulations
that depart from the scheme of an ``explicitly deductive relation between
the axioms and the prediction''. If this is true then it appears to be
better to draw the line between simple application of a theory on the one
hand side and models and simulations on the other hand side, as I have
done before (see page \pageref{simple-application}), rather than between
traditional models and computer simulations.

3) That the temporal dynamics of simulations matter is most obvious for
real time simulations as they are used for example in control
engineering. Real-time requirements tighten the range of applicable
modeling techniques. Often the construction of a real-time simulation
involves the development of highly optimized problem-specific algorithms.
While this makes the validation via well-approved modeling techniques
more difficult in particular cases, it does not change the situation with
respect to validation fundamentally.

4) According to Humphreys, another ``novel feature of computational
science is that it forces us to make a distinction between what is
applicable in practice and what is applicable only in principle''
\citep[p.\ 623]{humphreys:2009}. It is not exactly clear, why this
distinction that can become important in many situations should become
unavoidable only when computational science is considered. As far as
the validation of models is concerned it is, of course, decisive what
is ``applicable in practice''. But again, there is no difference
between models and simulations in this respect. 


\subsection{Computer simulations are not experiments}

Having thus established that the alleged novelty of computer simulations
does not distinguish them with respect to validation from models, the
question remains how computer simulations are related to experiments.
The philosopher's opinions could not be more diverse on this question.
There are those who believe that computer simulations are experiments
and can just like experiments be used to test hypotheses. And there are
others that believe that simulations are more like miniature theories
and therefore -- quite the contrary -- in need of empirical testing
themselves.

\subsubsection{The simulations-experiments dispute}

As philosophers that take the ``simulations are experiments''-side I
would just like to quote Margarete Morrison and Uskali Mäkki. Morrsion
writes in an article titled ``Models, measurement and computer
simulation: the changing face of experimentation'':

\begin{quote}
\ldots hence we have no justifiable reason to assume that, in these types
of cases, experiment and simulation are methodologically or epistemically
different. As we shall see, the causal connections between measurement
and the object/property being measured that are typically invoked to
validate experimental knowledge are also present in simulations.
Consequently the ability to detect and rule our error is comparable in
each case. \citep[p.\ 43/44]{morrison:2009} [\ldots]

The conclusion, that simulation can attain an epistemic status comparable
to laboratory experimentation, involved showing its connections with
particular types of modelling strategies and highlighting the ways in
which those strategies are also an essential feature of experimentation.
\citep[p.\ 55/56]{morrison:2009}
\end{quote}
 
Interestingly, Morrison concedes in a footnote that the account of
computer simulations she has argued for may be more appropriate for the
natural than the social sciences \citep[p.\ 56, fn 33]{morrison:2009}. 

In a similar vein, Uskali Mäkki compares experiments with models and
reaches the conclusion that ``Models are Experiments, Experiments are
Models'' \citep{maeki:2005}. And the same holds -- as we may add without
distorting his idea -- a fortiori for computer simulations:

\begin{quote}

Consider material experimentation as based on causally isolating
fragments of the world from the rest of it so as to examine the
properties of those fragments free from complications arising from the
involvement of the rest of the world. The analogy with theoretical
modelling is obvious: while material experimentation employs causally
effected controls, theoretical modelling uses assumptions to effect the
required controls. \citep[p.\ 308]{maeki:2005}

\end{quote}

Maeki, however, does not claim that models and experiments can always
be equated in this way. And he sees a difference between models and
experiments in the fact that in the case of experiments the isolation
of causal factors requires material manipulation. A difference that
Morrison, in contrast, considers to be rather inessential 
\citep[p.\ 54]{morrison:2009}.

That the view that simulations are experiments is not merely an eccentric
philosophical point of view is further illustrated by the fact that
practitioners often use the term ``simulation experiments'' as a facon
de parler for referring to computer simulations. For example, Hegselmann and
Flache use the term ``experiment'' when referring to results obtained
with cellular automata that run on the computer
\citep[3.11]{hegselmann-flache:1998}. And in a recent simulation
of customer experience in retail stores, Siebers, Aickelin, Celia and
Clegg use the terminology of hypothesis and experiments when referring
to pure simulation studies \citep[p.\ 16ff.]{siebers-et-al:2010}. They are
aware, however, that these kinds of ``experiments'' are not empirical --
in contrast to what they call the ``validation experiments'' of their
simulation.

The opposite position with regards to the simulation-experiments
question is, among many others, taken by Kleindorfer and Ganeshan, who
place simulations firmly on the theoretical side of science by
declaring them with reference to Naylor and Finger
\citep[]{naylor-finger:1967} as ``miniature scientific theories'':

\begin{quote}
To simulate means to build a likeness and the question as to the
accuracy of the likeness, one version of the validation problem (some
might argue the only version), is never far behind. The validation
problem is an explicit recognition that simulation models are like
miniature scientific theories. \citep[p.\ 50]{kleindorfer-ganeshan:1993}
\end{quote}

If simulations are understood in analogy to theories rather than
experiments it appears only natural to also consider the validation
requirements of simulations in analogy to that of theories. This is what
Troitzsch does when he maintains that ``Validation of simulation models
is thus the same (or at least analogous) to validation of theories.''
\citep{troitzsch:2004}\footnote{See also \citet{k_uuml_ppers2005}, where
I took these references from, in this context.} And Naomi Oreskes,
Kristin Shrader-Frechette and Kenneth Belitz even warn that ``Any
scientist who is asked to use a model to verify or validate a
predetermined result should be suspicious.'' \citep[p.\
644]{oreskes-et-al:1994} Given that one denies that simulations are
experiments this warning is important, because then, rather than being
able to ``verify or validate'' given assumptions, models and simulations
are in the need of verification and validation themselves.

% The opposite position with regards to the simulation-experiments
% question is, among many others, taken by Oreskes, Shrader-Frechette and
% Kenneth Belitz who consider simulations to be 
% 
% (CITATION Oreskes)
% 
% 
% And Günter Küppers and Johannes Leonard also place simulations on the
% theoretical side of science by declaring them as ``mini theories'':
% 
% (CITATION Küppers, Leonard)


\subsubsection{Resolving the simulations-experiments dispute}

Which of the two positions on the relation between models and experiments
is the right one? Or how can these views be reconciled? In my opinion the
dispute can be decided very clearly in favor of those who deny that
simulations are experiments. The reason is straight forward: Simulations
cannot generate any results that are not already implied in the theories
and assumptions that enter into the simulation setup. Typically, these
implications are not known to us and the simulation results may
therefore be surprising just like the results of an experiment. 
Still, computer simulations cannot deliver anything that was not built
into them. There is no causal influence from nature or, more precisely,
from the investigated target system itself on the results of the
simulation. And therefore there is consequently also no transfer of
information from nature to the simulation results.

That there is a fundamental and irreconcilable categorial difference
between simulations and experiments becomes most apparent when we think
of the special kind of experiment which is called ``experimentum
crucis'', i.e. the kind of experiment which we use to test our most
fundamental theories. For example, there is obviously no way how Young's
double-slit experiment \citep{wikipedia:double_slit}, which was conducted
in order to decide between the corpuscular and the wave theory of light,
could be replaced by a computer simulation. Other than in the
experiment, the outcome of a computer simulation would simply depend on
which of the alternative theories is preferred by the programmer of the
simulation.

But if there is such a fundamental and obvious difference between
simulations and experiments, why do so many people then believe that
simulations are experiments? A possible reason is that many experiments
are indeed just simulations. If, for example, scale models are used to
study the properties of some physical system, then one can reasonably
maintain that the scale model is a simulation of the ``real'' system.
This is nicely illustrated by a Quotation from John von Neumann, who
explained the purpose that experiments in wind tunnels had at his time:

\begin{quote}
The purpose of the experiment is not to verify a proposed theory but to
replace a computation from an unquestioned theory by direct
measurement. \ldots Thus wind tunnels are used \ldots as computing
devices to integrate the nonlinear partial differential equations of
fluid dynamics.'' (quote taken from \citep[p.\ 114]{winsberg:2003})
\end{quote}

One could say that in such cases the experimental setup is in effect an
analog computer to perform certain calculations. It is no surprise that
experimental setups that function as analog computers can safely be
replaced by digital computer simulations. The quotation of von Neumann
also nicely highlights an important prerequisite for simulations to
replace experiments: There must already be an ``unquestioned
theory'' the laws of which can safely be assumed to govern the
phenomena that are simulated. As we will see later, this is one of the
main problems of ``simulation experiments'' in the social sciences.
% CITE Cartwright 2009 here?

Admittedly, as there exist similarities as well as dissimilarities
between simulations and models, the answer to the question whether
simulations are experiments or not becomes somewhat relative because it
depends on whether greater importance is attributed to the similarities
or to the dissimilarities between the two. A very important reason for
emphasizing the dissimilarities, however, is that if the differences
between simulations and experiments become blurred, scientists (or the
broader public for that matter) might much easier be inclined to
overestimate the cognitive value of simulations and prefer to stick to
pure simulation studies instead of carrying out the often much harder
work of empirical or experimental testing of their hypotheses. A remark
by Peter Hammerstein about the uselessness of the sort of simulation
studies of the ``evolution of cooperation'' that became fashionable in
the aftermath of Robert Axelrod's pioneering work
\citep{axelrod:1984} vividly illustrates this problem:

\begin{quote}
Why is there such
a discrepancy between theory and facts? A look at the best known examples
of reciprocity shows that simple models of repeated games do not properly
reflect the natural circumstances under which evolution takes place. Most
repeated animal interactions do not even correspond to repeated
games. \citep[p.\ 83]{hammerstein:2003} [\ldots]

Most certainly, if we invested the same amount of energy in
the resolution of all problems raised in this discourse, as we do in
publishing of toy models with limited applicability, we would be further
along in our understanding of cooperation. 
\citep[p.\ 92]{hammerstein:2003}
\end{quote}

It is indeed well-nigh impossible to apply any of the results of the
simulations of this particular simulation-tradition empirically, if by
empirical application more than just drawing vague and superficial
analogies is meant \citep[p.\ 145ff.]{arnold:2008}. What is even more
worrisome is that in some instances simulation scientists even appear to
be rather insensitive to the importance of empirical validation, as
another example illustrates, where a journalist summarizes his discussion
with a scientist who has simulated opinion dynamics:

\begin{quote}

None of the models has so far been confirmed in psychological
experiments. Should one really be completely indifferent about that?
Rainer Hegselmann becomes almost a bit embarrassed by the question.
``You know: In the back of my head is the idea that a certain sort of
laboratory experiments does not help us along at all.'' 
\citep[p.\ 2]{groetker:2005} \footnote{The German original of this
passage reads: Keines der Modelle wurde bisher in psychologischen Experimenten
bestätigt. Sollte einem das wirklich völlig egal sein? Rainer Hegselmann
macht diese Frage fast ein wenig verlegen. "Wissen Sie: In meinem
Hinterkopf ist die Idee, dass eine bestimmte Sorte von Laborexperimenten
uns gar nicht weiterhilft."}

\end{quote}

This attitude is even more surprising because the way the simulation
model is constructed it does not appear principally impossible to submitt
it to some form of empirical testing
\citep{hegselmann-krause:2002}.\footnote{It should be considered less
embarrasing for a scientist to test a model and fail the test than not to
test one's own models at all. One notices a difference of attitude and
research design between Hegselmann's and Krause's simulation study
\citep{hegselmann-krause:2002} and the earlier quoted study by Siebers,
Aickelin, Celia and Clegg \citep{siebers-et-al:2010}. To be sure,
Hegselmann and Krause did not have the same man-power at hand as the
other team, but Hegselmann should at least be aware that empirical
testing is essential in science.} Normally one would expect a scientist
to have a natural interest in the question whether his or her model is
true or not. It stands to reason that the anti-empirical attitude of some
simulation researchers is fostered by confounding the categories of
simulation and experiment.

The pragmatic aspect of directing research in the wrong or right
direction has not been paid much attention to in the philosophical
discussion about the relation between simulations and experiments. If
this aspect is taken into account then another important reason for
distinguishing these two categories is the danger of drawing
premature conclusions about the world from empirically unconfirmed
simulation studies.

Summing it up: While certain types of experiments can indeed be replaced
by simulations, there remains a fundamental difference between
simulations and experiments in so far as in experiments we can put
nature to the test, which we cannot do with simulations. Therefore,
instead of thinking of simulations as experiments or experiment-like it
would be more adequate to think of simulations as tools for analysing
the consequences of theories. 

But before we examine what consequences these results about the
simulations have for the employment of simulations in the social
sciences, I am going to illustrate what has been said about the
epistemology of simulations so far with an example from the natural
sciences.

\section{The epistemology of simulations at work: How simulations are
used to study chemical reactions in the ribosome}

The example from simulations in the natural sciences that I am going to
discuss comes from the field of biochemistry. It concerns ongoing
research about how peptid bonds between amino acids are formed in the
ribosome molecule.\footnote{I am greatly indepted to Professor Johannes
Kästner from the Institute of Theoretical Chemistry at the University of
Stuttgart for explaining this fascinating area of research to me.
Needless to say that what I write here is my own summary of this research
for which I take the full responsibility.} The {\em ribosome} is a macro
molecule in the cells of living organisms which assembles amino acids to
proteins according to the information on the {\em messenger RNA} (mRNA),
which in turn is a copy of the genetic information stored in the cell's
DNA. The process proceeds roughly as follows: The ribosome receives a new
{\em transfer RNA} (tRNA) molecule with an attached amino acid at a
specific location called the ribosome's {\em amino site}. At the amino
site the tRNA molecule is bound to the chunk of mRNA that is currently
``read'' by the ribosome. (Which kind of tRNA molecule and therefore which
amino acid can enter the amino site depends on the chunk of mRNA that
occupies the amino site during this step of the whole process of protein
formation.) The amino site is spatially close to the {\em peptide site}
of the ribosome, where another tRNA molecule is located, the amino acid
of which is already attached to the evolving protein. In a process called
peptide bond formation the amino acid of the ``new'' tRNA at the amino
site is connected to the amino acid of the ``old'' tRNA at the peptide
site. Finally, the tRNA at the peptide site is released (having given
away its amino acid) and the ribosome moves forward along the mRNA chain
so that the tRNA that was received at the amino site now occupies the
peptide site. This whole process is catalysed by the
ribosome. Just how the peptide-bond
formation is mediated is a question
that researchers currently investigate. With the means available today it
is extremely difficult if not impossible to investigate this question
experimentally. Experimental data is only available on certain features
of the reaction, most notably on the reaction barriers (i.e. the
difference in energy levels that must be surpassed so that the reaction
takes place). Therefore, molecular dynamics simulations are used to study
how the peptide bond formation is catalysed by the ribosome.

In the following I am going to look at one such simulation study
\citep[]{kaestner-sherwood:2010}. The questions that concern me here is
under what kind of ``epistemic situation'' these simulation studies take
place and whether the previously established epistemological categories
can roughly capture this situation. In order to answer these questions,
we shall work our way backwards from the results that were optained in this
study to how these results were optained.

The results that were found in the study are:

\begin{enumerate}

\item The ribosome performs its catalytic function of reducing the energy
barrier of the peptidyl bounding reaction ``by the electrostatic
influence of the environment rather than just a favorable positioning of
the reactants. The high concentration of mobile ions \ldots in the
ribosome \ldots was found to be the key to the catalytic activity of the
ribosome'' \citep[p.\ 304]{kaestner-sherwood:2010}. The conclusion was
reached by comparing the simulations of the reaction in the ribosome with
simulations of the reaction in the gas-phase.\footnote{Drawing a rough
analogy one could say that the reaction in the gas-phase amounts to what
in the social sciences what be termed the ``null hypothesis''. Regarding
in how far the comparison is warranted, the authors state: ``Of course,
here the comparison was done with respect to the gas phase. A fairer
comparison may be with the reaction in water. However, we doubt the
validity of common continuum solvation models for this system, as many of
the interactions are hydrogen bonds and interaction with ions (see below)
that cannot be covered by continuum solvation models. Taking solvation in
water or salt solutions into account explicitly is computationally rather
demanding and, therefore, outside the scope of this work.'' \citep[p.
8]{kaestner-sherwood:2010}} The average reaction barrier that was found
in the simulations was ``in good agreement with experimental
data'' \citep[p.\ 304]{kaestner-sherwood:2010}.

\item Both of the two different reaction
mechanisms (``direct proton transfer'' and ``proton shuttle'') that were
studied may indeed account for the proton transfer. ``Both were found to
have similar activation energies. They may compete in the real
system.'' \citep[p.\ 304]{kaestner-sherwood:2010} At least the
simulation results do not allow to exclude one of these results
definately for the time being.

\item The possible occurrence of a certain ``tetrahedral intermediate''
in the course of the reaction is ``irrelevant for the reaction
mechanism''. This conclusion could be drawn from the simulations, because
``no minimum corresponding to a tetrahedral intermediate was found on the
free-energy surface'' as it should have been the case if it played a
vital role in the reaction. The diagnosis of irrelevancy is furthermore
strengthened by results in the literature. \citep[p.\ 300]{kaestner-sherwood:2010}

\item For one scenario a discrepancy between simulation results and
experimental data occurred: `` The free-energy simulations for the
direct proton-transfer mechanism resulted in a significantly
higher free energy of activation than the potential
energy barrier.'' \citep[p.\ 304]{kaestner-sherwood:2010} 

\end{enumerate}

How were these results arrived at and how do the above mentioned
``sources of credibility'' come into play here? In order investigate the
process of peptid bond formation the researchers conducted series of
computer simulations of the ribosome of the {\em Thermus Thermophilus}
bacteria. The fundamental scientific theory upon which these simulations
rest is {\em quantum mechanics}. Needless to say that quantum mechanics
is a both quantitatively and qualitatively extremely well confirmed
scientific theory with no competitors in the applicable areas of physics
and chemistry. Researchers believe this theory to realistically describe
on the most basic level just how things happen in physics.
Unfortunately, quantum mechanics is computationally much too expensive
to simulate a whole ribosome molecule. (The ribosome of Thermus contains
roughly 2.6 millions of atoms.) A feasible approach to keep
computational costs in check is, therefore, to use combined quantum
mechanics and molecular mechanics simulations ({\em QM/MM-simulations})
where only the crucial parts of the reaction are rendered with quantum
mechanics. This was also done here. Molecular mechanics is not a
fundamental theory but can be considered as something of a simplified
theoretical approximation that works good enough for some purposes. Just
as quantum mechanics it has demonstrated its suitability in many
application cases. Thus, as far as the credibility of the background
theories goes, we have here the ideal case of extremely powerful and at
the same time very well-confirmed background theories that cover the
phenomenon under study. This situation seems to be typical for some
areas of the natural sciences though not for all of them (e.g. climate
simulations).

Apart from the background {\em theories}, there is quite a bit of factual
background {\em knowledge} that enters into the simulations. The basic
function of the ribosome has been understood since the midst of the 20th
century and its structure is known since the 1970s. Thus, if scientists
simulate the ribosome today they can draw on a wealth of more or less
reliable background knowledge that has already been collected. Just how
reliable some parts of this knowledge are, is almost impossible to judge
for a non-expert. A non-expert can at best rely on the general
trustability standards of the science concerned. In order to do so, some
general knowledge of the science concerned is still necessary. The only
other alternative for assessing the reliability of scientific knowledge
as a complete non-expert would be to wait for technical applications of this
knowledge, the success or failure of which is obvious even for the most
ignorant and uneducated person. It suffices, however, if at least experts
-- if in doubt -- are able to trace back the assumed background knowledge
to its sources.\footnote{Still, the problem of assessing reliability
should not be taken lightly. In the social sciences there exist whole
simulation-traditions which at no point seem to have a secure foundation
in reality (see \citet{arnold:2008}).} The simulation study discussed
here, could not have been done if a model of the ribosome did not already
exist. Also, the background knowledge was important for deciding which
alternative mechanisms of peptide bond formation (``direct transfer'',
``proton shuttle'') to examine in the first place. And it was used as a
source of credibility by notifying agreement with results in the
literature.

The application of a theory to a particular problem is by no means a
trivial task and often requires no less inventiveness than the
development of a new theory. In the example case discussed here, numerous
different problems had to be solved and quite a range of different
technologies had to be applied. This is where what Eric Winsberg calls
the ``tricks of trade'' \citep[p.\ 444]{winsberg:2001} come into play and
where the simulation relies on what I have termed the credibility of
simulation techniques before (see point \ref{techniques} on page
\pageref{techniques}). I am going to point out just a few of these:
\begin{enumerate}

 \item In order to build a ``hybrid'' QM/MM-simulation it must be decided
 which parts of the reaction are to be included in the quantum mechanics
 part and which are calculated with molecular dynamics and how these
 parts are to be linked \citep[p.\ 295]{kaestner-sherwood:2010}. This
 choice is still considered by the scientists as rather straight forward,
 though.

 \item While the ribosome model used already existed, the simulation
 system needed to undergo a complicated procedure of preparation and
 equilibration \citep[p.\ 295]{kaestner-sherwood:2010}.

 \item The simulations made use of the so-called density functional
 theory (DFT), an approximation to quantum mechanics. For some parts of
 the simulation the respective calculations would have required far too
 much time. For these parts, the simplified ``semi-empirical'' SCC-DFTB
 method had to be used. Although it has been compared to DFT and found to
 deliver similar results, the use of this less exact method is considered
 as one possible explanation for the discrepancy with empirical data
 which was detected at one point \citep[p.\ 304]{kaestner-sherwood:2010}.

 \item Finally, the simulation is realized within the ``Chemshell''
 simulation framework \citep{chemshell}, which of course also falls under
 the heading of ``simulation techniques''. Summing it up, the simulation
 makes use of well reputed techniques, and where in doubt (as in the case
 of SCC-DFTB), further testing is done.

\end{enumerate}
 
The simulation study discussed here does not exclusively rely on
background theory, background knowledge and simulation techniques. Where
possible and in so far as it is possible its results are compared to
experimental data. (Experimental data does introduce questions of
reliability of its own, which would lead too far to go into here. But
even if it is not totally reliable, the comparison with empirical data is
meaningful, because the experimental results are generated independently
and if they match the simulation results then this does at least add
some mutual ``holistic'' credibility to both of them.) The fact that the
experimentally determined reaction barrier matches the barrier found in
the simulations (within the error bar), strengthens the credibility of
the first two above mentioned results concerning the role of mobile ions and
the relative importance of the two alternative mechanisms of proton
transfer in the peptid bond formation. The third result,
concerning a ``thetrahydral intermediate'', seems to be more or less a
purely theoretical result. The fourth result in turn is obviously due to
empirical testing. Just as if done by the book (and as it would please
philosophers of science such as Karl Popper or Imre Lakatos), the
contradictory empirical evidence is taken as a discrepancy (though for
good reasons not already a total disconfirmation) that demands
explanation and gives rise to new research questions.

Where does this all leave us? First of all, the exmple (hopefully) shows
that the above stated categories (``sources of credibility'') allow by
and large for an analysis of the epistemic situation of a typical
computer simulation. All three sources of credibility come into play
here, and at the same time nothing important seems to have been left out.
The example, furthermore, seems to support the contention that even when
empirical data is too sprase to conclude what mechanisms are at work in
the target system from empirical data alone, it may still be good enough
for the validation of computer simulations that serve as a tool to
identify these mechanisms. If this can be granted then the example
provides evidence for the ``synergy of sources of credibility'' as stated
above (point \ref{synergy} on page \pageref{synergy}).

This is not to say that merely on the basis of such an analysis an
evaluation of the credibility of the simulation would be possible. (This
would require expert knowledge of the field under study and of the
technologies employed in order to evaluate each of the sources of
credibility in this particular case.) It is merely meant that these
categories help us to understand the general research logic underlying
simulation studies such as this one. Although the simulation study itself
is quite complicated, its research logic seems to be very straight
forward: The simulation is meant to simulate more or less
``realistically'' how the process of peptid bond formation takes place.
It is built upon powerful and empirically confirmed background theories
as well as on background knowledge. It employs well-reputed or otherwise
tested simulation techniques. Where possible and so far as possible,
the results are compared to empirical data. Discrepancies to the empirical
data are properly taken care of. In fact, the basic research logic is so
clear that there does not even need to be much debate about it. In the
following we will see that quite the opposite is true for the research
logic of many models and simulations in the social sciences.

%  In this paper I discuss the epistemology
% of models and computer simulations in the social sciences. In
% particular, I am going to ask the question what specific challenges the
% employment of computer simulations faces in the social sciences. And I
% make certain suggestions how an epistemology of computer simulations
% ought to deal with these challenges. 
% 
% In order to do so, I start with stating what is the role of models and
% simulations according to the contemporary philosophy of science. Without
% entering into the pertaining controversies about the status of models in
% science, I follow the view that models provide a link between theory and
% reality and that there is no fundamental difference between computer
% simulations and models but that computer simulations are programmed
% models. I illustrate this view with an example from the natural sciences.
% After that I state some of the quite diverse view that authors hold one
% the role of models and simulations in the social sciences. The diversity
% of these views shows that the role of models and simulations in the
% social sciences is not well understood. Part of the reason for this may
% be that most philosophers of sciences have a natural science background
% and tacitely assume that models and simulations can be understood with
% the same categories as in the natural sciences. I run through a number of
% specific features of the social sciences that suggest that the actual and
% potential role that models and simulations play in the social sciences is
% typically different than in the natural sciences. The challenges that
% models ans simulations face in the social science are likewise challenges
% for their epistemology. I conclude with some suggestion of how the
% epistemology of models in the social sciences ought to meet those
% challenges.

\section{How do models explain in the social sciences?}

It is much harder to see how models or simulations contribute to the
understanding of phenomena in the social sciences than in the natural
sciences, because in many cases social science models are highly
stylized. Often the degree of idealization is so strong that the models
are obviously unrealistic and it becomes hard to see how they represent
their target system at all (see \citet{hammerstein:2003} for an example
of this problem). The textbook literature on economics, which is the
social science in which the use of models is the most pervasive, defends
the use of strongly simplified models with the standard argument that
because reality is much too complex to be described directly, we would
not be able to gain any understanding at all without strongly simplified
models \cite[p.\ 22ff.]{mankiw:2004}. As it is at the same time obvious
that not any arbitrary simplifications are permissible, the question
inevitably arises what kind of simplifications are permissible and what kind of
simplifications are not. Unfortunately, the economic textbook literature
offers no satisfactory answer to this question. To rely on the predictive
success of otherwise unrealistic simulations offers no solution, because
 the predicitive success of economic models is
notoriously weak \citep[]{betz:2006}. And if this is true for economics
then the situation for models in other social sciences like sociology or
political sciences must be expected to be even worse.

Not surprisingly, therefore, the ongoing debate about the proper use and
the epistemic value of models in the social sciences is characterized by
a wide diversity of different and often contradictory viewpoints. To an
outsider it could almost appear as if mathematical models and computer
simulations are used in the social sciences without a common
understanding of what they are good for. Among the views taken in this
debate are the following:

\begin{enumerate}

  \item {\em Models as predictive devices}: As just mentioned, Friedman
  defends models as tools for generating empirical predictions
  \citep[]{friedman:1953}. Unfortunately, due to the usually poor
  predictive success only very few social science models can seriously be
  defended on this ground. This is especially the case if the predictive
  success of elaborated models is no better than that of simple naive
  prediction methods \citep[ch. 2/3]{betz:2006}.
  
  The basic rationale of this concept of models can be described as
  this: {\em Models help us to understand the world by generating successful
   predictions. Other than that they do not need to be particularly
   realistic.}
  
% What distinguishes the ``predictive devices'' view from most other
% accounts of models is that little importance needs to be attributed to
% the accurate resemblence of the target system by the model or
% simulation.

  \item {\em Models as experiments}: Some authors strongly emphasize the
  analogies between models and experiments \citep{morrison:2009,
  maeki:2005}. In their view most models and most experiments share more
  or less the same features (e.g. more or less close resemblance to a
  target system, controlled environment, potentially unpredictable and
  surprising results) and it is therefore often more a matter of
  convenience and feasibility which of the two is to be preferred under
  which circumstances. But, as has been argued earlier, the analogy
  quickly breaks down if we consider experiments that test the empirical
  truth or falsehood of fundamental theories ({\em experimentum
  crucis}). Most of the authors advocating this view are, of course,
  aware that it does not work for all types of models and experiments.
  But even if this is taken into account, there is the constant danger
  of forgetting about the epistemic primacy of the empirical side of
  science. (After all it is the empirical world that our theories must
  be adjusted to and not the other other way round.)
 
  The bottom-line of the ``models as experiments'' conception is: {\em
  Models and Simulations help us to understand the world much like
  experiments do, e.g. by representing a real-world target system in a
  controlled environment that allows us to test assumptions.}
% The ``experiments are models'' view works well in economics, where
% typical laboratory experiments are conducted to learn something about
% some ``target system'' within the real economy. For example, trust
% games are played between participants in a laboratory experiment in
% order to draw conclusions about the expected trustworthiness of
% traders in internet auctions (CITE OCKENFELS). At the same time there
% seem to be few {\em experimenta crucis} in economics. For, if a
% hypothesis has been refuted in a laboratory experiment this does prove
% comparatively little about its truth or falsood in the real economy.
% Still, it would be dangerous to confound knowledge that has been
% gained by computer simulations with experimental or empirical
% knowledge.
   
  \item {\em Models as isolating devices}: The view that models are
  ``isolating tools'' has some likeness to the ``models are
  experiments'' view, because when conducting experiments one often
  tries to isolate the causal relation under study as good as possible
  from all disturbing influences. To the adherents of this view it does
  not matter, how the isolated system is arrived at; if by shielding
  from other influences (experiments) or by including just as much as is
  needed to model the causal relation in the first place (models)
  \citep[]{maeki:2009}. The value of this analogy is disputed by others,
  however \citep[p.\ 127]{kourikoski-lethinen:2009}. And with regards to
  the epistemic situation the same caveats as for the analogy between
  models and experiments hold.
  
  If models are understood as isolating tools then the important
  question is, if and how we can learn something from isolating models
  about the real world. As Nancy Cartwright argues, this depends on the
  scientific field. According to her, in comparison with physics very
  little can be learned from isolating models in economics. And one can
  probably safely generalize this finding to all social sciences. The
  possible explanation for this limitation that compared to the natural
  laws in physics economics has only few general principles
  \citep[]{cartwright:2009} links well with our previous contention that
  strong background theories provide a good ground for successful
  modeling.
  
  {\em Models help us to understand the world, because they allow us to
  study the functioning of causal mechanisms in isolation which in the
  real world are usually mixed up with other mechanisms.}
  
  \item {\em Models as credible counterfactual worlds}: Robert Sugden's
  ``credible world'' account of models \citep[]{sugden:2000,
  sugden:2009} is motivated by the observation that models in the social
  sciences often do not represent any particular target systems. Rather
  surprisingly, ``authors typically say very little about how their
  models relate to the real world'' \citep[p.\ 25]{sugden:2009}.
  Determined to find a rationale behind this modeling practice
  nonetheless, Sugden develops his ``credible worlds'' account.
  According to Sugden models are neither isolations or abstractions nor
  do they merely serve the purpose of ``conceptual exploration''. But
  they constitute ``counterfactual credible worlds''. Because the models
  (in economics) are by their very nature ``counterfactual'' it would be
  misplaced to demand that they be realistic. Yet, they need to be
  ``credible'' in order to allow us to draw inductive conclusions from
  them.
  
  This account of models raises more questions than it answers: In what
  sense can a world that is ``counterfactual'' still be credible? And
  what are the criteria by which the credibility of a ``counterfactual''
  model must be judged?
  
  Regardless of how these questions might be answered, the role of
  models according to this view can be characterized as follows: {\em
  Models do not represent particular target systems but they constitute
  paralell counterfactual worlds. By being ``credible'' they allow us to
  draw inductive conclusions about the real world.}

  \item {\em Models as incredible counterfactual worlds}: In contrast to
  Sugden, \citet[]{kourikoski-lethinen:2009} do not assume that the
  counterfactual worlds of models need to be ``credible'' to be of good
  service to our understanding of the real world. Quite the contrary, a
  model may very well contain counterfactual or even incredible
  assumptions. Varying counterfactual assumptions plays a crucial role in
  what Kourikoski and Lethinen call ``derivational robustness analysis''.
  This is a procedure by which the robustness of a model's
  ``substantive'' assumptions can be tested by varying its ``auxiliary''
  assumptions. If the the substantive assumptions still produce the same
  result no matter what varying counterfactual auxiliary assumptions are
  made in the model, they can be considered robust and we are entitled to
  draw the inductive conclusion that even if the counterfactual auxiliary
  assumptions would be replaced by realistic assumptions, the same
  results can be expected.\footnote{This procedure is somewhat similar
  to the ``de-idealization'' procedure proposed by Ernan McMullin 25
  years earlier \citep{mcmullin:1985, alexandrova:2008}. Only that when
  ``de-idealizing'', the auxiliary assumptions must gradually be replaced
  by more realistic assumptions. One can conjecture that when both
  alternatives are available ``de-idealization'' is the safer method.
  Derivational robustness analysis could then be understood as a kind of
  second-best alternative to ``de-idealization'' when the latter is not
  available.}

  Though it does raise questions, this is an interesting robustness
  concept that certainly deserves further exploration. If it proves to be
  sound than it offers a strategy how a model can be hardened -- up to a
  certain limit, if the substantive assumptions are not to be 
  tautologies -- by a pure model to model comparison.
  
  The bottom line is: {\em If a model is counterfactual or even
  incredible it can still help us to understand the world.}

  \item {\em Models as partial explanations}: Drawing on the conceptual
  framework of C.G.Hempel, Aydinonat describes models as partial
  explanations \citep[]{aydinonat:2007}. This means that models capture
  one possible cause of a phenomenon that can have several causes which
  may differ from instance to instance. Aydinonant gives this account in
  the course of a case study on Schelling's neighborhood segregation
  model, a model that explains the macroeffect of segregated
  neighborhoods with the micromotive of individuals having a weak
  preference against living in a neighborhood that is dominated by an
  ethnic group other than their own. Saying that this model provides a
  partial explanation means that particular instances of neighborhood
  segregation may have been caused by the factor that the model
  describes but could also have been caused by other factors. For,
  neighborhood segregation can also result from housing prices in
  connection with a difference in average income levels of different
  ethnic groups. An empirical assessment is needed to decide which causes were
  effective in a particular instance of the phenomenon.
  
  Although it is probably not appropriate for all types of models, 
  Aydinonat's account is a very convincing one: It provides a clear and
  convincing idea of how and why models may contribute to explanations
  in the social sciences. And it can almost immediately be transformed
  into a research design. 
  
  Summarized, this account of models says: {\em Models are partial
  explanations that describe possible causes of phenomena of a specific
  type. For providing a full explanation of a particular phenomenon, the
  model must be sufficiently robust and it must be chacked empirically
  against other possible causes of the phenomenon.}

  \item {\em Models as ``open formulae''} An even more defensive reading
  of the role of models in social sciences that has been suggested by
  \citet{alexandrova:2008} who treats them as open formulae. By this it
  is meant that models are merely templates or schema to generate causal
  hypotheses about the world. If models are only templates for causal
  claims then they do not carry any direct burden of epistemological
  justification any more, but the burden lies on the hypotheses that are
  produced from the template-models.
  
  Just as the previous one this account has the merit of suggesting a
  research design where the use of models interacts with that of
  experiments. The kind of iterative research that results is described
  by Alexandrova with respect to the example of auction design
  \citep[p.\ 384ff.]{alexandrova:2008}.
  
  Summary: {\em Models serve as templates for the generation of
  hypotheses.}

  \item {\em Models as tools for conceptual exploration}: Finally, and at
  the other end of the spectrum models and simulations can be regarded as
  a purely theoretical device that serves the purpose of conceptual
  exploration. There is no doubt that models and simulations can be used
  to explore the implications of our theories and concepts. The question
  is whether they are good for anything else. What is assumed here is
  that, unless they are empirically applied to particular target
  systems, models and simulations do not serve any other purpose than that of
  conceptual exploration.
  
  Bottom line: {\em Models serve primarily theoretical functions like
  that of exploring the implications of concepts and theories.}

\end{enumerate}

The diversitiy of views on simulations, exemplified by the above list,
is not simply a consequence of the fact that models and simulations are
used for different purposes in the social sciences. For, some of the
contradictory views like the ``credible worlds'' \citep[]{sugden:2009},
``incredible worlds'' \citep[]{kourikoski-lethinen:2009}, ``isolation''
\citep[]{cartwright:2009, maeki:2009} and ``partial explanation'' account
\citep[]{aydinonat:2007} have been proposed by their authors in the same
discussion and under consideration of exactly the same examples. 

The situation is somewhat embarrassing because as Sugden has observed
``authors typically say very little about how their models relate to the
real world'' \citep[p.\ 25]{sugden:2009}. And it is not because the
answer is so obvious that they remain silent. Otherwise, why would there be such
a debate? One obvious approach to resolving the debate would be to go
through the accounts that have been advanced one by one and either find
an account that is the most adequate or to arrive at some kind of
synthesis. But, apart from the fact that this procedure would be rather
tedious, it is also not guaranteed that it leads to the desired result.
For, it is unclear which criteria a good account of models ought
to fulfill. Therefore, before entering into any discussion about
epistemological accounts of models it might be advisable to ask for the
reasons why the research logic of models and simulations in the social
sciences is not at all obvious.

So, how is it possible that scientists use models, yet nobody seems
really able to tell ``how their models relate to the real world''
\cite[p.\ 25]{sugden:2009}?

\section{Common obstacles for modeling in the social sciences}

A possible explanation why it is so difficult to grasp the research logic
behind social simulations is that the models and simulations do
themselves face much stronger obstacles in the social sciences than in
most of the natural sciences. If it proves hard to justify models and
simulations in the social sciences then the simple reason may be that
very often there is no justification for using these models. In the
following a number of common obstacles for modeling in the social
sciences will be examined and the possible epistemological
consequences for modeling will be discussed. 

It is not claimed that these obstacles are
exclusive for the social sciences. Some of them may to a lesser or
greater degree also plague some simulations in the natural sciences as
well. In this case presumably also the epistemological consequences will
be the same. If one is aware of these obstacles it becomes easier to
understand the specific epistemological conditions of models and
simulations in the social sciences. 

\subsection{Lack of universal background theories}

In the social sciences there exist hardly any empirically well confirmed
background theories that fully cover the phenomena in their domain. There
simply is nothing in the social sciences that compares to Newtonion
mechanics or quantum theory in physics. If a physicist wants to explain
some mechanical phenomenon it is no question that she must apply the
theory of mechanics. In the case of the simulation of the ribosome
discussed earlier, there was no question that quantum mechanics is the
right theory for the problem. The challenge consisted in how to apply
this theory to the problem at hand.

In contrast to that, the first challenge in the social science is to pick
the right theory or the right set of theories. One usually has a whole
range of theories to chose from. In their book on the Cuban misile
crisis, \citet[]{allison-zelikow:1999} present three different
paradigms, each of which encompasses a host of different theories and
scientific approaches, partly overlapping, partly contradicting and
partly complementing each other. The way, how
social scientists deal with this situation is to pragmatically select
from the theoretical supply whatever deems them appropriate, then to look
at the question at hand from different angles suggested by different
theories and, finally, to assemble this patchwork to a reasonably
comprehensive picture.

The best candidate for a universal theory in the social sciences would
probably be utility theory in economics. But even if we take this prime
example of an axiomatized and highly universal theory, we will not have a
theory that could rival the importance and success of Newtonian mechanics
in physics. The difference is obvious: Newtonian mechanics can be
confirmed empirically in many different constellations and it has at
least for a certain well defined range of phenomena (i.e. macroscopic
phenomena where the velocities involved are much smaller than the speed
of light) never been disconfirmed. Therefore, we can safely draw the
inductive conclusion that Newtonian mechanics remains true even in those
constellations that have not or cannot be tested directly. Utility theory
on the other hand can at best roughly be confirmed in some select
scenarios and its general truth or at least its empirical applicability
in other cases remains doubtful. One important reason for this state of
affairs is that reliable measurement procedures for (cardinal) utility do
not exist. It is hard to confirm a theory without being able to measure
its central magnitudes. A further reason is the scarcity of principles
(i.e. the analogues of natural laws in economics) that come with utility
theory, which means that a great part of the explanatory work of models
based upon this theory is in effect be done by auxiliary assumptions and
situation-specific rules. (See also \citet[p.\ 48/49]{cartwright:2009}
and \citet{cartwright:1999}.)

What are the epistemological consequences then? The most important
consequence is that reliance on theoretical validation (i.e. proven or tested
compliance with a well-confirmed background theory) remains insufficient,
because there are no sufficiently credible background theories to rely
on. The more important, therefore, becomes the direct empirical
validation of models and simulations in the social sciences.


\subsection{Pluralism of Paradigms} 

Social sciences are typically
characterized by a pluralism of paradigms and a multitude of competing
theories about the same domain. This is a fact that it is often lamented
about, but it can also be seen as a chance, because the limits of one
paradigm often become apparent only in the light of other paradigms.
This can also nicely be illustrated by the previously quoted study on the
Cuban missile crises \citep[]{allison-zelikow:1999}, because each of the
three therein discussed paradigms (rational actor, organizational
behaviour, governmental politics) lends itself to a comprehensive story
about the Cuban missile crisis. And it is only by considering the
other paradigms that one really becomes aware that there is more to it.

What consequences does the pluralism of paradigms in the social sciences
have for modeling and simulating? First of all, there is a danger of
exclusively paying attention to only those paradigms that allow for
mathematical modeling. Now, as there is no reason a priori why these
paradigms should be any better than other paradigms, choosing a paradigm
merely on the basis of the relatively irrelevant criterion of technical
implementability bears the danger of getting a seriously distorted image
of reality. Many of the associated problems have become
apparent in the heated debate about rational choice explanations in
political science that started in the midst nineties. They are most
clearly pointed out in Shapiro's ``The Flight from Reality in the Human
Sciences'' \citeyearpar{shapiro:2005}.

Thus, before a model is accepted as the proper description or
explanation, other alternatives, and this includes also non-mathematical
depictions of the object under study, should be considered, too. In this
respect, theoretical models (i.e. models that mostly rely on some
background theory or paradigm or, worst of all, on ``plausible
assumptions'') are probably much more dangerous than models of phenomena
or data. For, if one is directly working with the empirical subject
matter, one becomes easier aware of the insufficiencies of theoretical
assumptions.


\subsection{Why parsimony is a vice and not a virtue} 

Many important phenomena in the social sciences are characterized by the
fact that they may be caused in many different ways. While this can
happen in physics, too, it seems to be a standard case in the social
sciences. Take for example, the outbreak of war. There are many different
reasons why a war can break out. In each single instance of an outbreak
of war there is usually a bundle of different causes involved. And in
different instances of an outbreak of war probably different bundles of
causes lead to the same effect, namely, the outbreak of war. Finally, it
is in most instances difficult to determine which of a number of possible
causes were decisive. How
can historians deal with these problems?

The best way to deal with this situation is to consider all reasonable
assumptions about what the causes in a particular instance of the
outbreak of war are. And where the evidence remains insufficient as to
whether a particular possible cause was indeed relevant, it is better to
at least mention this cause as a possibility than to leave it out
completely.

Because multicausality in the just described sense as well as the
evidential underdeterminacy of particular possible causes are typical
features of historical explanations, the discipline of history has
developed a scientific culture that in some respects runs contrary the
scientific culture of the natural and technical sciences. Most
importantly, historians do not consider an explanation as better just
because it is simpler. Quite the contrary, an explanation in history is
the better the more ``differentiated'' it is. The reason for this
attitude is that it is always easy to cook up a simple story. (The most
simple explanations are those that rely on ideologies, e.g. ``history is
the history of class struggles.'') But it is usually much more
challenging to get all the details right. It is therefore no surprise
that among social scientists the charge of ``monocausality'' is almost a
kind of an insult. In history as well as many other social sciences,
explanatory {\em parsimony is a vice and not a virtue}.

This conclusions can be generalized to all cases where multicausality is
involved and where it is practically impossible check the relevance of
all potential causes. In this situation, it does not make any sense to
demand that explanations should be as simple as possible. For, there is
no way of determining when an explanation has become too simple.

What consequences does this have for the employment of models and
simulations in generating explanations. One consequence is that simple
models that demonstrate merely logical or -- as the practitioners
sometimes prefer to say -- ``theoretical'' possibilities are at best
a small piece in the puzzle. They are ``partial explanations'' in the
sense of \citet{aydinonat:2007}. And before they can be
considered a part of a full explanation it must be checked whether the
so demonstrated theoretical possibility is a real possibility in the
given situation and how it compares to other possible explanations.

Often, unfortunately, the surplus in explanatory power to be gained by
simulation models that merely demonstrate logical possibilities is almost
negligible. This can be seen, for example, when comparing Robert
Axelrod's account of the informal truces between soldiers of opposing
forces on large parts of the western front in World War I in terms of his
simulations of the repeated prisoner's dilemma with the original
historical study by Tony Ashworth on which Axelrod based his account
\citep[p.\ 180-189]{arnold:2008}.

Linking to the discussion whether in the social sciences KISS (``keep
it simple stupid'') models are better than KIDS (``Keep it
descriptive stupid'') models, one might now conclude that this is a
problem of KISS models in particular (see \citet{pyka-werker:2009}
with further references).
%(CITE http://www.springerlink.com/content/9yclq08wfxq4f3a7/)
Without entering into the full discussion here: From their 
approach and their own aspiration KIDS models do indeed avoid to be
overly parsimonious. Yet, they are plagued by many problems of
their own like being more difficult to understand, often lacking
robustness or, despite being more complex, still not coming close enough
to empirical reality to be of explanatory value.

\subsection{``Wholistic'' nature of many phenomena in the social
sciences}

There is good reason to assume that many phenomena in the realm of
social sciences are of a wholistic nature. By ``wholistic nature'' it
is meant that the effect which a particular ``entity'' or ``force''
produces changes from one occasion to another and depends on the particular
circumstances of each occasion. It is an empirical question whether
this is true of many or most phenomena in the social sciences. But if
it is true then it explains why explaining phenomena by breaking down
their cause into single causes and then determining their joint effect
by some law of combination does hardly ever work in the social
sciences (see \citet[p.\ 390/391]{alexandrova:2008} and
\citet[p.\ 48ff.]{cartwright:2009}). 

In physics one can break down the forces acting upon a body into
different components and then combine them with the rules of vector
calculus. This works quite well in practice. The same thing does not work
in the social sciences. The question can be left open whether it does not
work because the ``wholistic nature'' of social phenomena poses an
epistemic barrier, which merely makes it extremely difficult for us to
find the right ``capacities'' (Cartwright) and rules of combination, or
whether there is more to it and the ``wholistic nature'' of social
phenomena raises an ontological barrier to the very existence of
processes that could reasonably be broken down into single components.
The epistemological consequence remains the same: One has to be very
careful with drawing general conclusions from models about ``capacities''
or regularities.

As a sidenote it can be mentioned that this feature, too, is reflected
in the scientific culture of some social sciences. Historians typically
have a strong sensitivity for the individuality of events and
historical processes. The idea was taken to its extreme
by the school of historism which denied that there are ``laws'' in
history.



\subsection{Difficulties of measurement}

The empirical data in the social sciences does often not have the form
of measurable quantities and where it does, it is often difficult
to measure it precisely. 

An example for non-quantitative data would be historical sources like
international treaties. Examples for quantitative data that are
well defined and can be measured precisely are money or the number
of inhabitants of a country at a given time. Quantitative magnitudes
that are less well defined and hard to measure would be the power a
state has in relation to other states or the utility a consumer derives
from the consummation of a certain good. To make this point a little
clearer the latter examples shall be discussed in slightly more
detail. 

While power seems to be a magnitude that has an order of greater or
smaller, any comparison remains almost inevitably vague. This is
especially true when different forms of power like economic power and
military power are to be compared. Thus, despite of what Bertrand Russell
had hoped some time ago \citep[p.\ 10]{russell:1938}, it is impossible to
form a concept of power that works similar to that of energy in physics,
where different forms of energy, say potential energy and heat energy,
can be measured and compared precisely.

As regards utility: In order to apply utility theory in the same way as,
say, the concept of force in physics, one would have to determine
peoples' preferences and measure the cardinal utility values they attach to them
with a reasonable degree of accuracy. Even in purely economic
contexts this is often well-neigh impossible. Money, to be sure, can be
measured, but then what prompts peoples' actions is not money but the
utility they derive from money or other means.

Now all this is of course well known, but what is easily overlooked are
the restrictive consequences these facts have for the range of reasonable
modeling in the social sciences. Assume, for example, a scientist wants
to explain why the victorious powers of the Second World War were willing
to agree to the unification of Germany in 1990 and she wants to do so by
using a game theoretical model of the negotiation process. Now, the raw
data available consists of protocols (if available) of the two plus four
negotiations, communiques and news releases of the involved parties,
treaty drafts, the final treaty and the like. Before any game theoretical
model can be fed with these data they would need to be transformed into
quantitative parameters through a careful process of interpretation.
Given that there is always a certain range of reasonable interpretation
the interpreted data must be considered quite noisy. The need of
interpretation also occurs on the way back when interpreting the results
of a formal model so that they make sense in terms of the phenomena that
the model is about.

One might of course deny that this is a suitable problem for the
application of a game theoretical model. But if this is denied then
already one important lessen is learned: Due to the nature of the data
occurring in the social sciences, formal modeling is sometimes not a
reasonable option. And if it is not denied
% that this example provides a reasonable application case for game
% theoretical modeling
then it does at least highlight some of the specific challenges that the
application of mathematical models faces in the social sciences due to
difficulties of measuring data quantitatively.

The epistemological consequences can be summarized as follows: 

\begin{enumerate}
\item Establishing a link between a model and empirical reality can be
difficult as it may require careful interpretation of empirical facts. Other than
in the natural sciences the last step in the chain of models leading from
theory to empirical reality may not simply be a model of data or
phenomena but a hermeneutical interpretation of data. (By ``hemerneutical''
I mean ``involving the interpretation and understanding of a product
of human cognition by a human agent''.)

% Attention must
% be paid that nothing important (i.e. causally effective for the outcome
% in question) gets lost when interpreting the data so that it
% can be fed into the model.

\item Because of the difficulties of quantitative measurement, great
strain is placed on the robustness of models. In order to draw valid
conclusions, a model must be robust with respect to variations of the
values of its input parameters within the range of measurement
inaccuracies. The larger the measurement inaccuracies or -- in cases
where hermeneutical interpretation takes the place of measurement -- the
range of acceptable interpretations, the more robust, therefore, the
model must be.

% 3. The restrictions regarding the type and inaccuracy of obtainable
% data are best already considered at the design stage of a model
\end{enumerate}

\subsection{Pluralism of scientific styles} 

Social sciences in general are characterized by a multitude of different
styles of presentation like rich narratives or ``thick descriptions'',
stylized verbal descriptions, mathematical descriptions. Often
explanations in the social sciences work completely without formal
models. If an example is seriously needed\footnote{It seems that it is,
because \citet{epstein:2008} denies that social science without modeling
is possible. Epstein's argument that one has only the choice between
either implicit or explicit models, wherefore it is better to make
explicit models, fails for several resons: 1) An implicit model can be
better than an explicit model, if one fails to render one's implicit
model in explicit terms properly. Just as the formalization of a verbal
theory is a highly non trivial task so is the rendering of implicit
assumptions in explicit terms. 2) Implicit assumptions about human
behaviour and human nature often work quite well. We use them every day
in our life with considerable success. 3) The question that is at stake
when discussing social simulations is not whether a model is implicit or
explicit but whether it must be mathematical or not. My claim is that for
many connections that we can perfectly well describe verbally we do not
(yet) have acceptable mathematical equivalents. No formalization is
better than poor adhoc-formalizations. Epstein, however, is right in so
far as explicetness is desirable.} then Orlando Figes\label{figes} ``The
Whisperers'' could be cited \citep{figes:2008}. In this book Figes
describes how the persecution in Stalinist Russia formed the habits of
its citizens in everyday's and family life and he explains why it did so.
The book is a fine example of ``oral history'' that rests mainly on
interviews with contemporary witnesses. Even though he explains things in
his book, Figes does, of course, not have any use for mathematical models
or computer simulations whatsoever.

But even in cases where mathematical models might help us to understand
social phenomena, they typically operate in a field that is also covered
by theories and descriptions of a very different scientific style. To put
the content and the results of mathematical models or simulations into
relation to theories and descriptions that are rendered in a completely
different scientific style requires a considerable interpretative effort.
% The associated difficulties typically become manifest in
% interdisciplinary research \citep[]{ostrom:????}.
Getting around these difficulties by confining oneself to a modeling
approach and ignoring descriptions and theories that do not fit a
mathematical style of research is not recommendable, because it can lead
to omitting relevant information. In non-economical contexts, where the
description of the empirical subject matter to which the models are
related usually has a narrative form it is not an option at all, anyway.

A good example for this situation is the interpretation of mathematical
results in social choice theory. The most famous of these results is
Arrow's theorem which shows that a mapping from individual preferences to
a collective preference relation is impossible if mild and seemingly
self-evident restrictions are placed on the mapping function (such as
that it should be ``non-dictatorial'', guarantee ``pairwise
independence'' and allow any kind of well-formed individual preference
relations in its domain) \citep[p.\ 583ff.]{mueller:2003}. But what does
this abstract mathematical result mean in terms of voting and decision
making in a democracy? Does it mean that democratic decision making
procedures are unavoidably precarious, as some authors believe
\citep{riker:1982}? In order to answer these questions the mathematical
results need to be related to empirical descriptions of democratic
elections and democratic decision making as well es philosophical
concepts of democracy, liberalism, political participation and the like.
Many things can go wrong if the task of interpreting the mathematical
results in terms of empirical and philosophical concepts is not done
carefully (see \citet{mackie:2003} for a comprehensive portrayel and an
acute criticism of misinterpretions of Arrow's theorem). Again, this
interpretative task is not the same as the respective task in physics of
interpreting the results of a calculation with respect to the physical
situation, if only because the hermeneutical gap between the language of
the models and the language of the empirical descriptions is much larger
in the social sciences than in physics.

The epistemological consequences that the plurality of scientific styles
in the social sciences has for modeling can be summarized as follows: 1.
Specific attention must be paid to the task of integrating mathematical
models with the results obtained by other methods. 2. In some cases
mathematical models might not be a reasonable option at all. This should
best be evaluated before embarking on the task of constructing models.


\section{Conclusions}

There are two kinds of conclusions that can be drawn from
the previous considerations: Conclusions that directly concern modelers
in the social sciences and conclusions that concern philosophers of
science that seek to understand and reconstruct the modeling practice
of the social scientists.

\subsection{Consequences for modellers in the social sciences}

As far as the consequences for the practitioners are concerned, one can
find two quite different research strategies in the social sciences:
Research that is method centered and research that is problem
orientated.\footnote{This distinction is motivated by Green's and
Shapiro's criticism of method centered research
\citep{green-shapiro:1994, shapiro:2005}. Rather than dismissing method
centered research in general, as Green and Shapiro do, I consider method
centered research as a different kind of research strategy and try to
catch the defects of the method centered strategy by placing specific
requirements on the research design of method centered research.} The
consequences to be drawn for these types of research strategies are
somewhat different, but follow the same general principle as a guideline,
which I call the ``principle of appropriate method'': 

\begin{quote}
{\em Principle of appropriate method:} The use of a
certain method for investigating a specific research question is
justified if no superior method for investigating the same question
exists and if the results that it yields are more reliable than mere
guessing.
\end{quote}

The rationale behind this principle is that ultimately the goal of
science is to find something out about reality, i.e. to describe, to
understand and to explain pieces of reality. In order to do so all kinds
of methods are employed. Now, as a result of the division of labour in
science, some scientists specialize on the development of theories, of
models or of methods while other scientists specialize on the empirical
research. There is good reason for specializing in this way, especially
if the mastering of particular methods, like computer simulations or
mathematical models requires specific skills that it takes years to
learn. But the study of models and the development of methods is not an
end in itself. Because the ultimate goal of science is to generate
knowledge about reality, all of its activities must either directly or
indirectly be related to this goal. And a certain scientific activity is
justified to the degree in which it is appropriate to serve this goal.
The ``principle of appropriate method'' substantiates this idea with
respect to the employment of specific methods. Even under the conditions
of scientific division of labour a scientist has a certain responsibility
for making sure that the scientific activity she is engaged in serves the
ultimate goal of science.

\subsubsection{Consequences for problem orientated research}

Under ``problem orientated research'' I understand the kind of research
where scientists try to answer a particular empirical research question.
If a scientist follows a problem orientated research strategy then the
problem is fixed and the methods should be chosen or disposed of as
appropriate. From what has been said before about the specific conditions
for modeling in the social sciences, two obvious conclusions follow:

\begin{enumerate}

  \item {\em Keep your options open}:  Other options than the use of
  models or simulations should be evaluated as well. It might even be a
  good idea to use several different methods to get the most
  comprehensive view on the problem.
 
  A possible exception to this rule is the science of economics, where
  there seem to be few methodological alternatives to modeling. It
  almost seems as if in economics, any answer of an economical question
  needs to be rendered in the form of a model in order to be acceptable to
  the community. (I am not in a position to judge whether what appears to be
  the common understanding of economists about their science is misguided
  or not. Therefore, I am just mentioning this fact.)
  
  \item {\em Choose wisely}: If models or simulations do not work for a
  particular problem, then models or simulations need not and, in fact,
  should not be used. For some types of research questions (as in the
  aforementioned example of Orlando Figes' oral history of Stalinist
  Russia (see page \pageref{figes})), formal models are simply
  inappropriate.
  
  But even in those cases where the kind of research question does not
  preclude a mathematical approach prima facie, modeling is not always
  worth the effort. If a model can neither be validated empirically nor
  vindicated theoretically then there is no point in modeling.
% Without sufficient credentials the epistemic strength of a mathematical
% model is not greater than that of a verbal metapher of a just-so story.
% Unfortunately, due to the difficulties of measurement empirical
% validation is often impossible in the social sciences, and due to the
% lack of well-confirmed background theories theoretical vindication
% often remains precarious.
\end{enumerate}


\subsubsection{Conseqeunces for method centered research}

The method centered approach can be understood as a research strategy
where a certain methodology is developed and investigated with regards
to its ability to answer different research questions. If a scientist
follows a method centered research approach then the method is fixed
and the problems are chosen or disposed of according to their
suitability for applying the method. The conclusions that can be drawn
for method centered research are symmetric to those for problem
orientated research:

\begin{enumerate}
  
  \item {\em Chose the right problems for your method, make sure that
  relevant scientific problems for the method exist}: The
  ``right problems'' are problems where the success of the 
  models can be tested. A common danger of method-centered research is
  the irrelevancy of its results \citep{shapiro:2005}. This happens, if
  problems are chosen only because they fit the method and not because
  they are relevant problems in any other sense.

  \item {\em Keep in mind that the model needs to be validated}: Models
  and simulations should be designed so that they can be validated. This
  implies that free parameters should be avoided and measurement
  inaccuracies should be taken into account. The burden of attuning
  models to measurement restrictions clearly rests on the shoulders of
  the modelers and not of the empirical researchers that develop
  measurement techniques, because the possibilities for
  developing measurement are limited by the empirical world.
  
  \item {\em Validate your model, take failures seriously}: Models need
  validation. It is insufficient to base a model -- as is often done (see
  \citet{hegselmann-krause:2002} for an example) -- merely on ``plausible
  assumptions'' without either systematically testing the validity of
  these assumptions nor empirically validating the results. A model that
  has not been validated does at best have the epistemological strength
  of a metaphor or a just-so story. Admittedly, this may be
  sufficient in certain contexts.
  
  Failures of validation ought to be taken serious: A model that fails
  validation is a false model. A model that cannot even be validated
  should be considered as not yet a scientific model in the same sense as
  an unfalsiafiable theory is considered as unscientific.

\end{enumerate}



\subsection{Consequences for philosophers of science}

The object of the philosophy of models and simulations is to reconstruct
how models and simulations contribute to the generation of scientific
knowledge. In order to do so the philosophy of models asks what models
are and, more importantly, how models prove. The latter question is more
important, because it helps us to draw the line between proper use
and improper use of models. In this respect the philosophy of models takes up
a similar task as the philosophy of science in general does with the
demarcation problem. To draw a line between proper use and improper use
of models is a particularly important task for the philosophy of
simulations in the social sciences, because, as a matter of fact, the
scientific value of many social simulations appears to be rather
doubtful \citep{arnold:2008, hammerstein:2003}.

>From what has been said previously, it should be clear that the epistemic
situation for models and simulations in the social sciences is somewhat
different from that in the natural sciences and engineering, although the
transition is of course smooth and interlocking. Paying proper
attention to this difference leads, as I believe, to the following
conclusions for the the philosophy of models and simulations:

\begin{enumerate}
  \item {\em Models ``mediate'' differently in the social sciences.}
  While in the natural sciences ``models as mediators'' are linked to exact and
  well-confirmed background theories on the theory-side and to precisely
  measurable data on the empirical side, the standard case in the social
  science appears to be quite different.
 
  On the theory-side there are no precise, well-confirmed and
  content-rich background theories. Often modelers help themselves with
  plausible assumptions to feed their models
  \citep{hegselmann-krause:2002}, which unfortunately adds quite a bit
  of arbitrariness to the models right at the beginning. 
  In other cases the assumptions are result of a careful
  empirical assessment of the target system \citep{siebers-et-al:2010}.

  On the empirical side, the data that the models are related to are
  only sometimes precisely measurable quantities. Often the ``data''
  consists of or is embedded in narrative descriptions of situations
  which need to be strongly stylized before they can be fed into models.
  
  Summing it up: The ``mediation'' concept of models is of
  comparatively more limited applicability in the social sciences,
  because i) there are no theories on the one end of the ``mediators''
  and ii) there is more involved in the process of mediation than merely
  models or a cascade of models. 

  \item {\em The analogy between simulations and experiments is harder
  to justify.} In an experiment we learn something about nature from nature.
  A simulation in contrast ``generates new knowledge on the basis of
  existing knowledge''\footnote{This is the very clear expression used by
  Jen Schellinck and Richard Webster in their talk at the Models and
  Simulations 4 Conference in Toronto, May 2010. To avoid any kind of
  misunderstanding one might add that ``simulations generate knowledge
  {\em exclusively} on the basis of existing knowledge''.}. This is not
  to say that we cannot learn something about nature from simulations. We
  can do so if the existing knowledge already is fairly comprehensive and
  well-assessed. In the natural sciences, part of the existing knowledge
  consists of powerful and empirically well-confirmed theories. Under
  this condition we can learn something about nature from ``computer
  experiments'' that apply these theories to particular research
  questions (as in the ribosome example) or that put more specialized
  theories to the test.
 
  Because of the lack of well confirmed and powerful (i.e.
  structure-rich) background theories, simulations in the social sciences
  usually do not have this quasi-experimental status. The may attain such
  a status if the assumptions that are built into the simulation are --
  even without a background theory -- at least empirically well-assessed
  for the simulated scenario. But frequently this is not the case and if
  it is not the case then the ``experimentation''-terminology used in
  connection with mere computer simulations (as in
  \citep[3.11]{hegselmann-flache:1998} for example) can be misleading.
  For, what these simulations show are only the consequences of more or
  less arbitrary assumptions, but not the behaviour of the simulated
  entities in nature.
  
  Philosophers of science should be aware that there is a categorial
  distinction between experiments and simulations. The analogy between
  experiments and simulations works only under certain favorable
  conditions such as the existence of comprehensive and reliable
  background knowledge. These conditions are usually not met in the
  social sciences.

  \item {\em Validation and research designs for models and simulations
  differ in the social sciences.} As far as validation ist concerned, the
  lack of empirically well confirmed background theories means that the
  assumptions entering into the model need to be assessed individually
  for the scenarios to which the model is to be applied. (The not
  uncommon practice to rely on merely ``plausible assumptions'' is rather
  unsatisfactory and should not be sanctioned by a critical philosophy of
  science.) Also, because the model input (e.g. assumptions, measured
  parameter values, tried and trusted modeling practices) is typically
  less reliable in the natural sciences, direct empirical validation of
  the simulation results becomes more important.
  
  It stands to reason that as a consequence of these differences the
  kinds of research design that are most successful in the social
  sciences are different form those in the natural sciences. If we follow
  Alexandrova's examination of the use of auction models
  \citep{alexandrova:2008} then a successful research design is one where
  models function as ``open formulae'' for generating causal hypotheses
  in a trial and error approach that includes models as well experiments
  as complementary elements of the research process. At the same time
  other accounts of modeling which are somewhat more in line with the
  research logic in the natural sciences fail to adequately capture the
  showcase of the auction design \citep[p.\ 387-393]{alexandrova:2008}.
  
  While similar trial and error research can also occur in the natural
  sciences, it might turn out that it is the standard case of a
  successful simulation-research design in the social sciences. This
  suffices to give the activity of modeling or simulating a distinct
  flavor in the social sciences.
  
  \item {\em Philosophers of science should refrain from rationalising
  bad practices.} Philosophy of science is not so much a descriptive but a
  critical enterprise. Its aim is to reconstruct how science generates
  reliable knowledge about the world. But the philosophy of science
  should also criticise scientific practise when it is flawed. In this
  respect the aim of the philosophy is not only to understand how science
  generates knowledge but also to critically examine whether it
  actually does.
  
  If, as in the case of Robert Sugden, one has reason to wonder that
  ``authors typically say very little about how their models relate to
  the real world'' \citep[p.\ 25]{sugden:2009} then the most salient
  explanation is that these models are simply not fit to teach us
  anything about the world \citep[p.\ 48ff.]{cartwright:2009}.
  Philosophers should allow for this possibility and refrain from
  rationalising bad methodological practice.
  
  This is the more important, because some of the common research designs
  for simulations in the social sciences appear to be heavily flawed. For
  example, the (implicit) research design that lies at the basis of many
  simulations of ``the evolution of cooperation'' in the tradition
  initiated by Robert Axelrod \citep{axelrod:1984} which works by
  constructing purely theoretical simulations and then drawing
  generalizing conclusions from the results is flawed, because the
  generalizing conclusions are not sufficiently warranted 
  \citep[p.\ 313-319]{binmore:1998}. And a more modest variant of this
  research design, which consists in constructing purely theoretical simulations
  and then never drawing any empirical conclusions at all, is also
  not convincing, because it raises the question why we should be
  interested in models from which we cannot learn anything about the world.
  
  As in science and philosophy rational argument ought to decide about
  the truth and falsehood of opinions and not the number of supporters,
  philosophers of science need not to be impressed by how widespread
  certain faulty research designs are.

\end{enumerate}

But why is it important to be aware of these epistemological differences
of simulations in the social sciences and simulations in the natural
sciences? The answer is that our explicit or implicit epistemological
ideas have a regulatory function when designing research programs. Wrong
epistemological ideas can entail the long-term failure of research
program. If we believe that the epistemological conditions in the social
sciences are just the same as in the natural sciences then we will expect
simulation studies that are designed on the role model of the natural
sciences to sooner or later yield good results. Any failure to do so will
in the first place be considered as a failure of the particular
simulation study and not of the research program. If we are aware of the
differences between social sciences and natural sciences, then we might
still consider it worth while to learn and apply techniques for social
simulations that have been sucessful in the natural sciences. But in
cases where these fail we will much sooner consider the possibility that
the research design was inappropriate and that the research program needs
to be readjusted.

% \section{Appendix: Hall of Shame. A synopsis of bad excuses for bad
% modeling.}
% 
% 
% 
% Anyone who has to deal with people who do social simulations and dares
% to ask the question what social simulations are good for if they cannot
% be validated empirically, will sooner or later come across certain
% standard arguments that are meant to justify the development and use of
% unvalidated simulations. One does not find these arguments in
% scientific papers so often. But they do nonetheless belong to the
% scientific culture surrounding social simulations. And one can hear
% them often enough in converstations at conferences or sometimes being
% uttered on the more informal or solemn occasions of the scientific
% business like keynote adresses \citep{epstein:2008}.
% 
% My own opinion on these arguments is that most of them are just bad
% excuses for bad scientific practice. If social simulations are to be a
% serious branch of science that produces tenable results then it must
% learn from its mistakes. But it is impossible learn from mistakes, if one
% has excuses for committing the same mistakes again and again. And it is
% impossible to work on the problems, if one is not willing to accept that
% there these problems exist in the first place. Therefore, I will go
% through some of the most prominent excuses and explain why they are
% just that: bad excuses.
% 
% \subsection{Excuse No 1: We cannot know much about society, anyway}
% 
% {\em The excuse}: Our ability to know is very limited in certain fields
% like the social sciences, anyway. Therefore, if we cannot produce
% explanatory simulations,  we need to make do with the kind of computer
% simulations that we can conduct, even if they are not good enough to base
% explanations of empirical phenomena on them.
% 
% {\em The answers}: 1. If the computer simulations of a certain
% phenomenon, say evolution of cooperation, turn out not to be of any
% explanatory value \citep[p.\ 189ff.]{arnold:2008}, then cannot already
% be interpreted as a failure of science in general but only as the
% failure of a certain scientific paradigm. Ohter, non simulation-based
% paradigms might still prove to be successful in explaining the same
% phenomena. 2. If we really cannot explain anything with the simulations
% then why the effort. One can equally confine oneself to philosophical
% speculation, which costs less effort.
% 
% \subsection{Excuse No 2: Even if simulations fail, one can still learn
% from them.}
% 
% {\em The excuse}: Take again the simulations of the ``Evolution
% of Cooperation''\cite{axelrod:1984} as an example. Even if they have
% failed, we can still learn a lot from this failure, e.g. that patterns
% of the evolution of cooperation are so simple as imagined, that there
% may be all kinds of boundary conditions and disturbing factors etc. All
% things we did not know before.
% 
% {\em The answer}: If the main thing that we learn about a certain
% method when using it is that it fails, then we have hardly learned
% anything at all: If one tries to build a house and it breaks down on
% the first day of the move then we have indeed learned how not to build
% a house, but we still do not have any idea of how to build a house.
% 

\newpage
\bibliographystyle{apsr}
\bibliography{bibliography}



\end{document}
