
\documentclass[12pt, a4paper]{article}
%\documentclass[12pt, onecollarge]{STJour}
\usepackage[USenglish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{ucs} % unicode
\usepackage[T1]{fontenc}
\usepackage{t1enc}
\usepackage{type1cm}
\usepackage{times} 
\usepackage{setspace}
%\smartqed  % flush right qed marks, e.g.\ at end of proof
%\usepackage[square,sort&compress,comma,numbers]{natbib}	% fuer BibTeX
\usepackage[sort&compress]{natbib}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{mathptmx}      % use Times fonts similar to Windows Office
%\usepackage{german}		% Apply if you wish to write in German
\usepackage{exscale}
\usepackage{psfrag}
\usepackage{layout}
\usepackage[dvips]{color}
\usepackage{eurosym}  
\usepackage{graphicx}
\usepackage{rotating}

\numberwithin{equation}{section}
\sloppy
 
\begin{document} 
%\onehalfspacing

\unitlength1cm 
%

\title{Experiments and Simulations\\Do they Fuse?}


%
\author{Eckhart Arnold}
%
% \institute{% $^a$ ...
%   Institute of Philosophy, University of Stuttgart,\\
%   Seidenstra√üe 36, \\
%   70174 Stuttgart, Germany \\[2mm]
%   eckhart.arnold@philo.uni-stuttgart.de\\
%   www.eckhartarnold.de
%   http://www.uni-stuttgart.de/philo/index.php?id=1043\\[2mm]
% } 
%
% Define if Authorlist is too long for running head
%\authorrunning{Eckhart Arnold}
%
% Convention: year  - issue in that year
%\SimTechIssue{2012-??}
\date{May 2012}
 
\maketitle

\begin{abstract}
%\onehalfspacing

Recent advances in computer technology have led to a dramatic increase of the use of computers in science. Computer simulations are among other things used in collaboration with and as surrogate for experimental techniques. Does this mean that simulation techniques and experimental techniques fuse or become increasingly difficult to distinguish as some philosophers believe? In this chapter I argue that although there are similarities and some overlap between the two, both categories can clearly be separated with respect to their epistemic function. 

A particular problematic case in this respect is that of hybrid methods that involve both empirical measurement and computational post-processing of data. What, if anything, distinguishes an empirical measurement where the data is merely computationally refined from a computer simulation that makes use of empirical input data? Several possible answers will be discussed. 


{\bf Keywords}: {\em Epistemology of Models, Computer Simulations, Hybrid Methods}

\end{abstract}

\newpage

%\doublespacing

\tableofcontents 


\section{Introduction}

In today's science computers have become an indispensable tool. They are used for the evaluation of scientific data, for storing data, for the preparation of results and the communication among scientists. But computers are not only a tool that helps scientists to process and evaluate scientific data, they also produce scientific data when they are used for running computer simulations. This raises the question whether the data that computer simulations produce is the same as other kind of scientific data, in particular experimental data? What speaks for this assumption is that the data produced by simulations is usually previously unknown to the scientists, often cannot be derived mathematically and may yield the same or at least similar kinds of information about a simulated empirical system as an experiment. What speaks against this assumption is the fact that simulation data stems from a calculation performed with a computer and that it is not or not directly the result of an empirical measurement. This is also the stance that I am going to take in this chapter.

The reasons for taking this stance will be set out in detail in the following section, when the debate on the relation of simulations and experiments will be reviewed. In particular, it will be argued that neither are computer simulations material in any sense that would liken them to experiments (as maintained by \citet{parker:2009}) nor are experiments to such a degree infested with models that the function of models in experiments becomes indistinguishable from the function of models in simulations (as maintained by \citet{morrison:2009}).

But there is also a further possible line of reasoning against a strict separation of simulations and experiments that is not so easily dismissed. According to this line of reasoning simulations and experiments cannot strictly be separated, because at least in some of their instances the role that empirical data takes can appear indistinguishable between simulations and experiments. The question arises for those simulations that do in some way or other make use of empirical input data and for those experiments that in some way or other involve the computational post-processing of the measured data. In both cases empirical input data is processed by a computer to produce some kind of output data. And the question is precisely: What kind of output data? 

%It would be too rash an answer to say that in the case of the simulation the empirical input data was produced {\em before} the simulation  while in the case of the experiment the empirical input data was produced in the course of the experiment. For, assume that the computational post-processing step in the case of the experiment was carried through independently (say, because years later a new method of computational data evaluation had been invented), then this should not make a difference with regard to the empirical or non-empirical status of the output data. The post-processing step if considered in isolation always looks just like a computer simulation. Still, we would probably want to treat the output of an experiment as empirical, even if post-processed.

We can define those scientific procedures that involve both empirical input data and computational processing of this data collectively as {\em hybrid methods}. The problem of hybrid methods can then be formulated as the following question: 

\begin{quote}
{\em What, if anything, distinguishes a computer simulation that makes use of empirical input data from a measurement that involves the computational refinement of empirical data?}
\end{quote}

It is not entirely clear whether this question is the right way of formulating the problem. Different alternatives will therefore briefly be discussed in the third section of this chapter as well.
%Also, as with many philosophical questions, it is prima facie not clear whether the question is a serious question or whether it merely represents a phony problem (``Scheinproblem'') to which there is a simple and obvious answer. In this case it should be expected that the question dissolves as soon as one leaves the macro level and examines particular example cases. If, however, the question represents a real problem, then it is an important question, because it challenges the logic of scientific discovery which rests on the distinction between theoretical constructs (theories, hypotheses, beliefs, models) on the one hand side and empirical findings (observations, measurements, experiences
%\footnote{``Experiences'' is to be understood in a broad sense, encompassing also such diffuse categories of life experiences. In the philosophy of science, with its notorious slant towards the natural sciences it is often forgotten that in the social sciences life experiences are one very important source of empirical knowledge.}
%), where the former are to be tested, controlled and restricted by the latter. 
%It's true that similar challenges have arisen previously in the philosophy of science, for example in the context of the problem of theory-ladeness of measurement. The problem of theory-ladeness of measurement challenges the suitability of measurements to provide us with objective empirical data about nature on the grounds that any measurement is inevitably infested with theory, namely the theory that describes the working mechanisms of the measurement machinery. (This problem is similar, though not the same as the problem of theory-ladeness of observations with our natural sense organs, i.e.\ perceptions.) To this challenge there exists a response that has convincingly been put forth by Ian \citet{hacking:1983}: The theory-ladeness of measurement does not lead to a justificatory circle as long as the theory that we want to test with a particular piece of measurement data is not (presupposed by) the theory of our measurement apparatus. As long as we keep in mind that the reliability of our measurement machinery must be assessed independently, it is thus innocuous to speak of our measured data as ``empirical data''.
%But Hacking's solution to the problem of theory-ladeness of measurement cannot directly be transferred to the problem discussed here. For, although indirectly related to it, the question at hand is not primarily a question of reliability of a scientific procedure with respect to informing us about nature. Rather, it is in the first instance a question about the empirical or non-empirical status of scientific procedures and their outcome that are in part but fully dependent on some kind of empirical input.
The answer to the problem of hybrid methods that is advocated here treats it as a partly conventional matter whether the outcome of hybrids is considered as empirical data or as theoretical data (which includes simulation data). The convention proposed here is that {\em hybrids} should be considered as empirical methods, if 

\begin{enumerate}
\item the output data represents quantities that are either causally responsible for the values of the input data or that are mathematically connected to them.
\item and the output data characterizes factors that operate in close spatio-temporal proximity of the input data or, more precisely, the source data.
\end{enumerate}

In order to defend this convention, I am going to argue that it is in harmony with the self-ascription by the scientists using these methods, with the traditional understanding of measurements and with our intuition.

%Because the answer partly rests on conventions, it could be objected that the distinction between empirical and non-empirical becomes conventional, too, and is thus not objective any more, as it ought to be if the logic of scientific discovery is to be based on it. I do not have a full answer to this objection, but I point out that it is in harmony with the way we apply the term ``empirical'' when talk about traditional measurement devices. For example, we say we measure the temperature, when in fact we are measuring the extension of the volume of a liquid in a thermometer and infer the temperature with the help of a scale. Still, we consider the temperature value as empirical data and I believe we do so, because the kind of inference we make adheres to the two conditions stated above. If this is true, then at least no new conventions are introduced. I also believe that this convention matches by and large the way scientists use the terms ``empirical'' or ``non-empirical'' themselves when talking about hybrid methods. 

%One could of course also adopt a skeptical position and accept only raw data as empirical data. But then the word ``empirical'' would be of little use for characterizing different types of scientific methods and different kinds of data. And one can still think of the conventional position as a simplifying fiction that helps us to understand and communicate the nature of different scientific methods.


\section{The current state of the debate}

The philosophical debate on the epistemic status of computer simulations can be traced back at least until the early 1990-ies. One of the popular slogans that already appeared as early as this in the debate was that of simulations as a ``third way of doing science'' \citep{rohrlich:1990, axelrod:2003, kueppers-lenhard:2005}, indicating that computer simulations neither fully resemble material experiments nor conventional forms of theory or model building, but that they are something ``in between''. While this is a fair characterization of the activity of conducting computer simulations, which in many ways resembles experimentation but also requires specific practical skills and virtues that differ from those of experimenters, it is doubtful whether computer simulations can be characterized as a ``third way'' also in an epistemological sense. For scientists themselves it has most of the time been clear that computer simulations are not an empirical method of science, even though they resemble experiments, and that therefore computer simulations, just like theories and models, are in the need of empirical validation themselves rather than being able to confer empirical validation to theories \citep{gilbert-troitzsch:2005, heath-et-al:2009}. This view is also reflected in much of the philosophical literature on computer simulations at the turn of the century \citep{guala:2002, morgan:2003, humphreys:2004}.

However, in the latest installments of the philosophy of simulations this view has come under attack. In the context of a sometimes confused debate about the alleged materiality of simulations philosophers have denied that there is any fundamental or epistemologically relevant difference between simulations and experiments. Or, if there is, then at least ``any epistemically relevant differences between experiment and simulation [are] very difficult to articulate'' \citep[p.\ 48]{morrison:2009}. I am convinced that this is a mistake. First, therefore, I am going to set out some of the core arguments against the epistemic difference of simulations and experiments and I try to show why all of them are wrong, some of them quite obviously so. Then, I am going to put forward positive arguments for the difference between simulations and experiments. Finally, I explain why in spite of the clear conceptual distinction hybrids still provide a challenge for the epistemology of simulations.

\subsection{Arguments against the difference of simulations and experiments}

The philosophers that are the most critical of the attempts of drawing a clear distinguishing line between simulations and experiments are Wendy \citet{parker:2009}, Eric \citet{winsberg:2009, winsberg:2010} and Margaret \cite{morrison:2009}. Wendy Parker argues that simulations are in a sense are also ``material'' and that at any rate what matters is not materiality but ``relevant similarity'' \citep[p.\ 484]{parker:2009}, which can be quite independent from the material status of the experiment or simulation. Winsberg does not go quite as far as Parker, but he, too, argues that simulations and experiments cannot sharply be distinguished by their materiality or any similar criteria. The only distinction he concedes is that the way in which scientists justify their belief that the object under study (in a simulation or an experiment) can stand in for the target differs between simulations and experiments. As we shall see, he cannot advocate this view without contradiction, because the justifications cannot differ without referring to some other difference on which the different justifications are based. But then the different kind of justification is not the only difference any more.

Morrison, in contrast to Parker, does not diminish the difference between simulations and experiments by arguing that simulations are also somehow material and, thus, somehow like experiments. But, quite the contrary, she argues that experiments in advanced science are somehow like simulations, because ``the way models function as the primary source of knowledge ... is not significantly different'' \citep[p.\ 43]{morrison:2009}. As we shall see, she overlooks the simple fact that in simulations a model also functions as the source of data while in experiments the data is at least co-produced by nature.

The flaws of the central arguments by Parker, Winsberg and Morrison shall now be explained in more detail. Parker offers several arguments which are partly independent from each other. As mentioned, one argument is that simulations like experiments are also ``in a sense'' material. The sense in which simulations are material is this: 

\begin{quotation}
The experimental system in a computer experiment is the programmed digital computer
-- a physical system made of wire, plastic, etc.\ As described in the last section, a computer simulation study involves putting a computing system into an initial state, triggering its subsequent evolution (the simulation), and collecting information regarding various features of that evolution, as indicated by print-outs, screen displays, etc. It is those data regarding the behavior of the computing system that constitute the immediate results of the study. In a computer simulation study, then, scientists learn first and foremost about the behavior of the programmed computer. \citep[p.\ 488f.]{parker:2009}
\end{quotation}

But, obviously, the kind of materiality that computer simulations enjoy because they are run on a material system, i.e.\ the computer hardware, does not at all liken them to real material experiments. It is misleading to say that the data that is presented on the print-outs and screen-displays is ``data regarding the behaviour of the computing system''. For the data of a simulation usually does not convey any information about the computer on which it was produced, but only information about the simulated system. It would be equally awkward if someone makes a calculation with pen and paper to consider the resulting figure as data regarding the pen and the paper.  In particular, the person could potentially perform the same calculation with the same result in her head, which would imply that the result written on the paper must also be data regarding the brain of the person. Clearly, this is absurd. But then it is also wrong to say that the data that results from calculations performed on a computer is data regarding the computer. But if this is not true, then also Parker's basic contention that ``that any computer simulation study classified as an experiment is first and foremost a material
experiment'' loses its ground.

The same confusion of different levels of consideration, i.e.\ the symbolic or, if preferred, the "`semantic level"' \citep{barberousse-et-al:2009} on which a computer simulation operates and the material level of the hardware on which it is implemented, is carried over by Parker to her reading of intervention. In Parker's opinion intervention in a computer simulation study occurs when the user sets up the simulation and puts it into an initial state, for which purpose the user has to interact materially with the computer. What Parker appears to misunderstand at this point is that it is not the interaction between the experimenter and the experimental machinery which is at stake when one speaks of {\em material experiments} in contradistinction to {\em computer simulations} or {\em computer experiments} but that between the investigated experimental object and either the machinery or the experimenter or both. Now, in a computer simulation the experimental object is either a fictional symbolic object or a symbolic (or ``semantic'' for that matter) representation of a material object. In any case the intervention on the ``experimental'' object of a computer simulation always occurs on the symbolic level, e.g.\ by assigning certain values to certain control variables. Thus, if one classifies computer simulation studies as experiments on the grounds that they involve intervention, which is, admittedly, one of several typical (though not exclusive) characteristics of experiments, then one still must concede that there exists an important difference between simulations and experiments regarding the type and kind of this intervention: In computer simulations it remains purely symbolic and only in experiments it is material.

That is not to say that Parker is totally unaware of the representational nature of computer simulations. At one point Parker even contrasts the representational quality of computer simulations with the property of involving interventions that experiments have: 

\begin{quotation}
These characterizations imply at least the following fundamental difference between simulations and experiments: while a simulation is a type of representation ‚Äî one consisting of a time-ordered sequence of states ‚Äî an experiment is an investigative activity involving intervention. \citep[p.\ 487]{parker:2009}
\end{quotation}

But apart from the fact that the there is at least a counterpart to the representational quality of the simulation model, namely the representative quality of the experimental object, it is not at all clear why a simulation does not involve intervention. In both a simulation and an experiment intervention consists in changing or determining certain conditions of the experimental system in a controlled way. And for both simulations and experiments there exist examples where this kind of intervention is achieved by a) determining the boundary conditions through the setup before the experiment or simulation starts or by b) user interaction during the simulation or experiment. While this line of reasoning might appear to strengthen Parker's point about the comparability of simulations and experiments as scientific methods, it still does not alleviate the counterargument that experiments operate on a material object while simulations operate on a symbolic representation. 

When saying that the experimental object is a representative, then this means that it is a part or an instance of the target system of the experiment, i.e.\ the system in nature, the investigation of which was the purpose of the experiment. It is clear that the programmed model that represents the target system in nature in a computer simulation can never be a representative in this sense. On the other hand there exist experiments where the object is also not a representative but merely some kind of representation. An example would be a ripple tank that is used to study such phenomena as reflection and interference of waves. While the waves in the ripple tank are water waves the ripple tank could also be used to learn something about waves of another kind like sound waves or light waves. In this case the waves in the ripple tank are not an instance of the target system and therefore the experimental object would not be called a representative of the target system. One can, in this special case, speak of the experiment as an {\em analog simulation} and consider the experimental object as a representation of the target system just like in the case of a computer simulation. There still remains one obvious and one more subtle difference, however. The obvious difference is that the object of an analog simulation remains a material object, while the object of a computer simulation is always symbolic. Only, this difference does not have any epistemic relevance in the case of analog simulations. The more subtle, but potentially epistemically relevant difference is that in the case of the analog simulations there is still some kind of isomorphism involved between the object and the target, while in the case of computer simulations the relation remains purely representative.

\begin{figure}

\begin{small}
\begin{center}
\begin{tabular}{l|c|c|c|} 
\multicolumn{1}{c}{ } & \multicolumn{1}{c}{ } & \multicolumn{2}{c}{$\overbrace{\hspace{7cm}}^{Experiments}$} \\ \cline{2-4}
                      & {\bf computer simulation} & {\bf analog simulation} & {\bf plain experiment} \\ \hline
materiality of object
                      & semantic              & \multicolumn{2}{c|}{material} \\ \hline
relation to target  
			          & \multicolumn{2}{c|}{representation}       & representative \\ \hline
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{$\underbrace{\hspace{7cm}}_{Simulations}$} & \multicolumn{1}{c}{ } \\
\end{tabular}
\end{center}
\end{small}
\caption{Conceptual relation of simulations and experiments}\label{SimulationExperimentsScheme} 

\end{figure}

The different types of simulations and experiments that have just been described are summarized on figure \ref{SimulationExperimentsScheme}. Failure to distinguish properly between computer simulations and analog simulations is a constant source of error in both Parker's and Winsberg's treatment of simulations. For example, Parker complains that ``the proposed distinction implies that no study as a whole can be simultaneously both a simulation of some target system T and an experiment undertaken to learn about that same target system T , since the required relationships with T are mutually exclusive'' \cite[p.\ 486]{parker:2009}. Then, she continues by presenting an example of a study that according to her interpretation is simultaneously an experiment and a simulation. Not surprisingly, her example of the San Francisco Bay Model concerns an analog simulation. But this merely shows that the categories of simulations and experiments are not mutually exclusive in the first place. At the same time it does not imply that there is no epistemically relevant difference between (computer) simulations and experiments which are not analog simulations, which, however, is the conclusion that Parker suggests. In a similar vein \citet{winsberg:2009} complains that `` if we can never be sure if something is an experiment or a simulation'' it would not be worth knowing that -- as Mary S. \citet{morgan:2003} maintains -- ``experiments are more epistemically powerful than simulation'' \citep[p.\ 582]{winsberg:2009}. But doubts whether something is an experiment or a simulation can arise only in the case of analog simulations. And even here it is possible to distinguish analog simulations from plain experiments by the relation to the target system as depicted in figure \ref{SimulationExperimentsScheme}. 

Another point that Parker makes deserves more consideration, namely, that  ``what is ultimately of interest when it comes to justifying inferences about target systems is not materiality, but relevant similarity'' \cite[p.\ 493]{parker:2009}. This is quite true, because material similarity does not automatically transform into epistemic reliability. Also, numerical representations of nature in computer simulations can be quite accurate at times. Still, being of the same material stuff can be a good reason to assume relevant similarity (which Parker concedes). In some case it may be the sole reason. It must be expected that this is particularly true for those processes in nature about which we do not yet have acquired a comprehensive theoretical background knowledge in terms of either fundamental laws or at least well tested phenomenological laws. Parker seems to be faintly aware of the connection between the existence of background knowledge and the possibility to simulate: ``especially when scientists as yet know very little about a target system, their best strategy may well be to experiment on a system made of the `same stuff' '' \citep[p.\ 494]{parker:2009}. But she does not seem to be aware that in this case it is not just an option (``best strategy'') but a necessity to conduct real material experiments. As the frontier of science is being pushed forward, one can assume that greater and greater regions of nature fall into the realm of what can reliably be simulated on the basis of our scientific background knowledge. But there will always remain scientific questions where material experimentation is unavoidable.

Winsberg in his paper entitled ``A Tale of Two Methods'' \citep{winsberg:2009}, maintains that simulations and experiments can only be distinguished by how scientists argue for their validity. He does not notice that it would be impossible to argue in different ways for the validity of either simulations or experiments if there did not exist any other differences on which the different arguments could be founded.\footnote{Against this criticism of Winsberg an anonymous referee objects that ``two claims can be justified in different ways but have the same epistemic warrant.'' But since the epistemic justification of a scientific procedure usually consists in explaining or pointing out what its epistemic warrants are, it is hard to see how this is possible in this context. And, as the passages quoted in the following from Winsberg demonstrate, Winsberg himself is unable to uphold his position that simulationists and experimenters rely on the same epistemic warrents when they justify their method.} And indeed he implicitly admits this when he says of the experimentalist that ``She believes the inferences she will make are legitimate because she is prepared to argue that the two systems are, in relevant respects, the same kind of system, made out of the same material, and can be expected to exhibit relevantly similar behavior.'' \citep[p.\ 590]{winsberg:2009} But this means that the experimentalist relies on a relevant material similarity. And then relevant material similarity must be another difference between simulations and experiments besides the different justifications given for the respective methods. If it were not, it would not be understandable why the simulationist should not appeal to the same reason when justifying his procedure. And of the simulationst Winsberg claims that he ``will want to argue \ldots that the computational model of his computer is relevantly similar to a good model of the behavior of the gas jets that interest him'' \citep[p.\ 590]{winsberg:2009}. But this is an argument based on formal similarity, which means that formal similarity in contrast to material similarity must be an exclusive feature of simulations, if the justification based on formal similarity is to be exclusive to the simulationist. For otherwise Winsberg's thesis that simulations and experiments differ by the way they are justified would be empty. Thus, Winsberg is forced to admit the validity of Guala's (2002) distinction between material and formal similarity that he tries to deny in his paper.

This is not the only contradiction in Winsberg's paper. In order to explain his point Winsberg sets out with the thought experiments of two physicists, one using a tank of fluid, the other using a digital computer to study fluid interaction. In other words, one scientist is conducting a material experiment, the other a computer simulation. At one point he concretizes his story as follows: ``What if we were to find that both of our original physicists' primary area of interest is astrophysics? The systems that actually interest them are supersonic gas jets that are formed when gasses are drawn into the gravitational well of a black hole.'' \cite[p.\ 52]{winsberg:2010} With respect to this setting Winsberg remarks: ``Neither physicist, then, is actually manipulating his or her actual system of interest. Neither one is even manipulating a system of the same type, on any reasonable sense of the term.'' (p.\ 52). Thus, we are to assume that simulation and experiment cannot be distinguished by whether the actual system of interest is manipulated. But only a few lines later Winsberg maintains exactly the opposite: ``In some respects, the physicist's tank is an instance of the system of interest, since it is in fact an instance of a supersonic interaction of a pair of fluids.'' Now, how can a system that is not a ``system of the same type, on any reasonable sense of the term'', be at the same time an ``instance of the system of interest''? It seems that Winsberg's denial of a distinction between simulations and experiments that is any more fundamental than the different kinds of their respective justification rests in part on a self-contradictory analysis of the central thought experiment of his paper.

Another objection that Winsberg raises against the distinction is that ``on the Simon/Guala definitions of simulation and experiment, they are both success terms. An investigation will count as an experiment only if it is successful in the sense that the relevant material similarity between object and target actually obtain'' \cite[p.\ 581]{winsberg:2009}. He concludes from this that on this definition an experiment failed to establish a relevant material similarity then it would not be a failed experiment but it would simply fall into the other category, which seems wrong to him. And he worries that ``if experiment and simulation are success terms, then investigators may never be in a position to know if they are conducting a simulation or an experiment''. But Winsberg, following a suggestion from Parker, already offers the obvious counter argument against his objection himself, namely ``that simulation studies are characterized by the fact that the investigators aim for their objects to have relevant formal similarities to their targets and that ordinary experiments are characterized by the fact that the investigators aim for their objects to have relevant material similarities to their targets'' \cite[p.\ 584f.]{winsberg:2009}. Only, Winsberg never answers this counter-argument. Instead, he continues: ``I do not think this works. I think the whole idea of formal versus material similarity is confused, no matter how much it is tempered by `relevant,' `aimed for,' or whatever.'' That is, Winsberg reasserts his opinion but does not offer an argument.

Margret Morrison does not buy Parker's argument that computer simulations are also somehow material: ``Locating the materiality of computer experiments in the machine itself, however, carries with it no epistemological significance'' \cite[p.\ 48]{morrison:2009}. Nevertheless she reaches the similar conclusion that ``the modeling features of simulation are co-extensive with its experimental character making any epistemically relevant differences between experiment and simulation very difficult to articulate.'' More precisely, her claim is ``that the way models function as the primary source of knowledge ... is not significantly different'' \cite[p.\ 43]{morrison:2009}. But this is obviously false, because in a simulation it is a model that produces the data --  which is impermissible in a material experiment.\footnote{See also \cite[p.\ 15]{peschard:2011} who utters a very similar criticism of Morrison and nicely summarizes her complaints: ``Admittedly, we ‚Äòknow‚Äô of the features of the system that affect the instrument only in so far as we ‚Äòknow‚Äô of the relation between these features and the state of the instrument, that is, only in so far as we have and are justified in using a given model of the instrument. But to say that this mediating role of model makes causal interaction in experimentation epistemically irrelevant looks like saying that the role of language in expressing our sensory experience makes the sensory character of this experience epistemically irrelevant''.} In a similar vein Morrison maintains that ``Experimental measurement is a highly complex affair where appeals to materiality as a method of validation are outstripped by an intricate network of models and inference'' \cite[p.\ 53]{morrison:2009}. But one of her own examples, magnetic resonance imaging (MRI), suggests the opposite. For, in order validate that an MRI scanner works correctly, it is among other things tested with material objects. And when it is put to use in medicine, it is done so, because it allows to reveal material features of the body or body part under examination and thus allows to validate or refute assumptions about health or illness by an appeal to materiality.\footnote{According to an anonymous referee I have misunderstood the point that Morrison wanted to make with her example of MRI. I am aware that Morrison has several things to say about MRI. It is just this specific consequence about the relative epistemic weight of material factors and models that I intend to criticize. In the worst case my criticism only touches an unfortunate formulation by Morrison.} Because devices like an MRI scanner are diligently built to determine material properties of the objects under study, one could say that the ``intricate network of models and inference'' is tailored to the expression of the materiality of the object, rather than outstripping the appeal to materiality.

As we have mentioned earlier, with the scientific frontier moving onward, it is imaginable that increasing ranges of natural phenomena can be simulated, thereby potentially outstripping the need for experiments. But this is something completely different than maintaining that the appeal to materiality can be outstripped by models and inference in those cases where material experiments are still conducted. One might speculate that in future science there will be a growing dependence on observations that are made with intricate and highly technicized measurement devices and continuously less reliance on ordinary sense perception. But it is doubtful whether the point where sense perception becomes superfluous as a means of scientific investigation will ever be reached. Although one can say with \cite{humphreys:2004} that this increases the epistemic opacity or that a greater and greater part of the epistemic processes that lead to knowledge will take place hidden from our eyes. But even then humans will remain in the epistemic centre, because it is humans that build and design the epistemic machinery that they make use of. Only, the path -- or, more likely, some of the paths -- to the periphery where the epistemic machinery gets into contact with the world will continuously be extended.

Morrison may have been mislead into likening experiments to models by her own historical example which she presents at the beginning of her paper. For the purpose of commenting on the contemporary discussion about models and experiments this example does unfortunately not appear to be particularly well chosen. The example concerns Lord Kelvin's interpretation of electrodynamics. ``As I mentioned at the outset, Kelvin saw mechanical models as intimately connected to measurement and experiment. He considered numerical calculation measurement as long as it was performed in the context of model construction, testing and manipulation. All of these features enabled one to know an object ‚Äòdirectly‚Äô rather than simply becoming acquainted with a mere representation.'' \cite[p.\ 30]{morrison:2009}. This can be misleading if applied to the contemporary discussion, because it seems that Kelvin's notion of knowing an object ``directly'' rests entirely on an ontological commitment of Kelvin in favor of mechanical models and explanations. Other than that, his jelly bowl \citep[p.\ 37]{morrison:2009} is just another example of what we call analog simulations and as such just as remote from its target system as Maxwell's calculations. Therefore, the example of Kelvin is not a good example for showing, as Morrison seems to intend, that material experiments do not have a more direct relation to their target systems than simulations and that appeals to ``knowing an object directly'' through a certain kind of scientific method are badly founded. The appeal is merely badly founded in Kelvin's case. Incidentally, we see again how important the clear distinction between plain experiments and analog simulations is for the whole discussion.

Briefly summing it up: None of the arguments against the separation of simulations and experiments by Parker, Winsberg and Morrison appears to be pervasive.\footnote{According to an anonymous referee, this misrepresents Winsbergs, Parkers and Morrisons position, because none of them believes that simulations and experiments are one and the same thing, but only that in some cases they may have the same epistemic warrents. Now, my primary goal is not to criticize Winsberg, Parker and Morrison, but to refute those arguments that have been put forward against the difference between simulations and experiments. Some of the few concessions these authors make in the discussed papers in favor of the distinction between simulations and experiments I have pointed out above. In no way do the discussed papers support the conclusion that Winsberg, Parker and Morrison restrict themselves to {\em some} cases only. But even if restricted to some cases, most of their arguments remain false and seriously misleading. 
%In my opinion the the discussion has fallen back behind the level that has already reached earlier with \citet{guala:2002} and \citet{morgan-morrison:1999}. 
%The same referee also believes that no one denies the distinguishing features of simulations and experiments that I point out below. I am delighted to learn that everybody, including Winsberg, Parker, Morrison, that other referee who objected on the grounds that my position was an expression of outfashioned positivism and the rest of mankind fully agrees with me.
} But there is one point by Parker that ought be kept in mind, namely, that in any concrete case what ultimately matters is not the materiality of the procedure and neither primarily whether the relation to the target system is a material or a formal similarity, but whether a relevant similarity can be established.

\subsection{Arguments for the difference of simulations and experiments}
\label{Differences}

Having refuted the arguments against making a difference between simulations and experiments, the question remains what positive arguments there are for drawing a strict distinction between simulations and experiments. There appear to be at least three fundamental and important differences between simulations and experiments:

\paragraph{Only experiments can operate on a representative of the target system.}

Operating on a representative of the target system means that the object that is manipulated and studied in the experiment is either a part or an instance of the target system or the target system itself. In contrast, both analog and computer simulations only operate on a representation of the target system. In the case of analog simulations this is true in virtue of the definition of an anlog simulation as an experiment that operates on a representation rather than a representative of the target system. In the case of a computer simulations this is true by necessity as long as the target system is a target system in nature.\footnote{One can also conceive of a model as a target system of a computer simulation. But this is a special case which in an epistemic connection is not at all comparable to the case where the target system is a system in the real world.} Both, the relation of being representative of and that of being a representation of, raise the analogous question of whether the respective relation truly holds. But this does not mean that both questions are one and the same. For establishing either of these relations provides a different challenge. Generally speaking, in order to establish the relation of representation requires comprehensive background knowledge about the target system, while the relation of being a representative can be established (though, as always, with a probability of error) already on the basis of other indicators. E.g.\ if one wants to know whether some kind of wood burns at 250 degrees centigrade it suffices to take a piece of that wood to establish the relation of representative of (in this case in the sense of being part of). But before one could be sure that a certain computer model of a piece of wood is truly a representation of that kind of wood, one would either need a comprehensive knowledge of the chemical structure of the kind of wood in question and of the chemical laws guiding oxidation, or one would at least need to know sufficiently detailed phenomenological laws about the burning of wood as to allow us to draw conclusions about the temperature at which the particular kind of wood in question starts to burn. Thus, the difference between representation and representative is an epistemically highly relevant difference.

This difference in the relation to the target system can also be described as a difference between {\em material similarity} that holds between the experimental system and the target system in the case of an experiment and {\em formal similarity} that holds in the case of computer simulations \citep{guala:2002}. The case of analog simulations is ambiguous with respect to this terminology and requires clarification as to whether {\em material similarity} also covers a similarity of different materials that obey the same laws. If this clarification is made or if the case of analog simulations is excluded then Winsberg's criticism of this terminology \citep{winsberg:2009} can be circumvented. Another phrase that has been used to describe material similarity is the phrase ``same stuff''. This phrase is less ambiguous than the phrase ``material similarity'', because it clearly suggests that the material must be the same.



\paragraph{Only experiments can deliver us knowledge that goes beyond what is implied in our background knowledge.}

Because computers are merely calculating machines they cannot provide us with any knowledge about the world beyond what is implied in the premises of a computer simulation. As the premises must be rooted in our prior knowledge, the insights one can gain from computer simulations is limited to this prior knowledge and its implications.\footnote{It is important here to understand the difference between a) things that are not logically implied in our prior knowledge, b) things that are logically implied in our prior knowledge but not known to us and c) things that are logically implied in our prior knowledge and also known to us. For category a) simulations cannot help us but only experiments. For category b) simulations and experiments can help us. And for category c) neither is needed, because we know it already.} The same does not necessarily need to be true of analog simulations. In order to be meaningful an analog simulation only requires that the mapping relation (typically an isomorphism) between the object that serves as a stand-in for the target and the target system itself is known, but not that the laws of nature that govern the object are known as well. Therefore, the object could potentially reveal a behavior that is not merely a logical consequence of our prior knowledge. If we assume that the mapping relation is applicable nonetheless, then the novelty exposed by the object's behavior carries over to the target system as well. It may of course be disputed whether this assumption is true or whether it has much practical impact. But the case is at least imaginable.

Because of this limitation computer simulations can best be thought of as tools for evaluating the consequences of an existing stock of knowledge. But only experiments (potentially including analog simulations in the hypothetical case just described) can break through the epistemic barrier that is determined by our prior knowledge and to which computer simulations are inevitably confined.

One can speculate whether one day our background knowledge will be so complete that we can deduce any possible further knowledge about the world from it. But this is, of course, pure science fiction and it seems as good as impossible within the limitations of the {\em conditio humana} that it should ever become real.

\paragraph{Only experiments can be used to test fundamental theories}

Can simulations be used to test hypotheses? They can, but only against the background of an existing theory. It may be the case that this theory can in turn be tested via simulations against another more fundamental theory. But at some point we reach a most fundamental theory which cannot be tested by a simulation any more, because no theories or principles remain upon which such a simulation could be built. Thus, it is for principle reasons impossible to replace an {\em experimentum crucis} by a simulation. And this is true for both computer simulations and analog simulations, because an {\em experimentum crucis} requires that the investigated object is a representative of the target system, the particular nature of which is in question.

What counts as fundamental theory is, of course, historically relative. For example, Galileo's laws of motion and Kepler's laws of the movement of the planets were both fundamental theories at the time of their invention. But both can be derived from Newtonian Mechanics and, therefore, they lost the status of fundamental theories which was now taken by Newtonian Mechanics. Once Newtonian Mechanics are accepted, Kepler's Law's could also be tested by simulation. (Though this is strictly speaking unnecessary, because they can be derived mathematically already.) But then this simulation does not replace an {\em experimentum crucis} of a {\em fundamental} theory any more. Since at any past, present or future point in the history of science there will exist at least one theory that is the most fundamental theory, material experiments will still be needed to test at least this fundamental theory. Even if we assume the hypothetical scenario from above, where mankind has accumulated sufficient knowledge to derive everything else that is worth knowing from this knowledge, material experiments would still be needed to justify the fundamental theories that are part of this set of knowledge.

\paragraph{Further Differences and Conclusions}

One can easily think of further differences between simulations and experiments: For example, experiments are material in the sense that the object under investigation is a material object. Simulations in contrast are virtual in the sense that the object that is investigated is a semantic representation. The criterion of materiality should not be confused with the relation of material similarity. Materiality as such concerns only the object under investigation and not the relation between object and target (see figure \ref{SimulationExperimentsScheme}). With respect to the relation of material similarity materiality is a necessary but not a sufficient condition, because an analog simulation is also material but not of the ``same stuff'' as its target. Since it does not allow to distinguish analog simulations from other experiments, materiality alone is a comparatively less important criterion for the distinction than, say, material similarity.

Yet another difference is that experiments are an empirical method while computer simulations remain purely theoretical. Again, the case of analog simulations may be a cause of ambiguities, because in virtue of the materiality of their object analog simulations could be considered empirical just like ordinary experiments, but they do not deliver us empirical knowledge about the target system.

All in all, we find that there are sufficiently many and sufficiently important differences to warrant an epistemological distinction between simulation methods and experimental methods. This said, it cannot be denied that it is a fact that in modern science both methods, the experimental method and the simulation method, are frequently used in close connection with each other. Does this mean that they merge into complexes where simulations and experiments become yet indistinguishable? This is the question, we will now turn our attention to.


\section{The Challenge of Hybrid Methods}

In contemporary science experimental methods are often closely intertwined with simulations or with simulation-like computational procedures: Simulations can be used to determine the optimal experimental design before experiments are carried out \citep{kramer-radde:2010}. Computational methods can be used to select experimental data for further analysis while the experiment is run as it is done in particle accelerator experiments \citep{lhc-wlcg:2011}. And they can be employed to post-process the raw data from measurements as for example in computed tomography \citep{lee-carroll:2010}. In economics, experiments usually involve real human subjects that are placed in an artificial environment that differs substantially from the sort of real world environments with respect to which scientists try to draw conclusions from the experiments \citep{guala:2002, guala:2012}. Sometimes the artificial environment contains computer agents that interact with humans in the experiment. But also in the natural sciences we frequently encounter cases where empirical measurements and simulation methods jointly function as source of data. Multiscale models of electrocardiac physiology as described by Annamaria Carusi, Kevin Burrage and Blanca Rodriguez in another chapter of this book as model-simulation-experiment systems may serve as an example.

To give a name to these kinds of sophisticated procedures we can speak of them as \emph{hybrids of simulations and experiments}. Hybrid Methods constitute a challenge for the philosophy of science in several respects. They challenge the distinction between simulations and experiments that has been defended above.  
Doing so, hybrid methods also challenge the logic of scientific research in general. For the logic of scientific research as understood by most scientists and also by many philosophers of science rests on the testing of hypotheses against empirical data. This presupposes, one should assume, a clear distinction between the empirical and the theoretical. Or, to put it in another way, if we cannot uphold the distinction between the theoretical and the empirical, then we would have to reconstruct the whole logic of scientific research. 

The distinguishing features between simulations and experiments presented earlier do not really solve the problem of hybrids, because they only tell us what the difference between the categories of experiment and simulation are. But they do not allow us in all cases to decide whether a particular procedure belongs to the class of simulations or to that of experiments. If we follow the reasoning of the first part, then we know that only experiments can operate directly on the target system. But we may not be sure in a particular case whether some particular scientific procedure that makes scant use of some sort of empirical data and heavy use of computation falls into this category.

To solve the problem of hybrids several quite different approaches are imaginable. One can even say that so far neither the framing nor the exact formulation of the question is clear. I am not going to attempt to give a comprehensive list of approaches to the problem of hybrids that have been proposed so far or that appear imaginable, but I will confine myself to the discussion of three approaches. Two of these approaches have been suggested by other authors and I briefly present them here, because I consider these promising. After that I am going to present my own best guess at how the problem of hybrids could be solved. 

%\subsection{The skeptic's answer: Only input data as empirical data}
%
%It remains always possible when analyzing hybrid procedures to separate the empirical part from the computational part. At some point the computer receives its input in form of digital data and this is precisely the point at which the computational part begins. Now, a skeptic could suggest that it would be safest to treat only the input data as empirical data and consider any output data of hybrid methods as tainted by theory. That is not to say that this data is theoretical or speculative, but we cannot really say any more that it is still empirical and we should better withhold our judgment with respect to the empirical nature of the output data of hybrids.
%
%The problem with the skeptical solution is that it stands in strong contrast to the established use of language. For, in order to be consistent the skeptic would not only have to withhold the judgment concerning the empirical nature of the output data of a hybrid method but of any other kind of measurement instrument that involves some sort of transformation of data as well. This would break the common practice of classifying the output of amp√®remeters, thermometers, Geiger counters etc.\ as empirical data.
%
%It is also somewhat unsatisfactory to leave the question open how the procedure as a whole is to be classified. Is the image of a CT scan empirical data or is it not? {\em Prima facie} this sounds like a natural and reasonable question to ask.
%
%Thus, as so often, the skeptical solution, while logically conclusive, turns out to be not very helpful. But in one respect it is still important: Although it might be difficult (or, according to the skeptic's position: impossible) to tell whether the output data of a hybrid is empirical or not, we recognize that the input of the computational part of a hybrid is always a precisely and unambiguously defined magnitude, namely the digital data as it is entered into the computer (either by hand or by a digitizing device) before any calculations on this data have been carried out. Thus with respect to the computational procedure the concepts of ``input data'' and ``raw data'' are absolutely clear. 
%
%One might take this a step further and claim that at least to the input data the label ``empirical'' could safely be applied. However, this needs further qualification: The input data is already molded by the instruments or the analog part of a hybrid with which it is recorded. Only, if we can say that the data that the these instruments provide is empirical, we are entitled to label the input data of the computer as empirical data. If a skeptic is willing to grant this then the concept of ``empirical data'' and its distinction from non-empirical data is still meaningful, even if not all data can be categorized as either empirical or non-empirical.

%\subsubsection{Classification in terms of reliability}
%
%Building on the assumption that one of the main reasons why we consider the distinction between what is empirically observed and what is theoretically derived as important consists in the connotation that empirical observations give us a more direct and in this sense more reliable knowledge, one could also try to turn things around and classify hybrids according to their reliability. Like the skeptic's answer suggests, the classification as ``empirical'' or ``theoretical'' or as ``experiment'' or ``simulation'' would be given up at least for hybrids. In so far this answer is liable to the same counter arguments as the skeptic's solution. But in contrast to the skeptic's solution, the idea is to develop a categorization of types and degrees of reliability or, to pick up the phrase from \cite[p.\ 493]{parker:2009}, ``relevant similarity'' to fill in the gap. This might indeed be a solution. So far, however, no universally applicable categorization scheme based on degrees or types of reliability has been developed.

%With this answer it is implicitly admitted that the distinction between the empirical and theoretical is not really applicable in the context of today's science any more. Retaining the connotation of the words but dropping their literal meaning, the distinction simulation-experiment is then employed to indicate different degrees of reliability.
%
%But does this make sense? A possible objection would be that it does not blen d in well with those cases that are not hybrids. For, there are reliable as well as less reliable experiments or observations and there are reliable and less reliable simulations. But then it becomes confusing to base the distinction between simulations and experiments on the reliability of the procedure. Surely, there are bad experiments. But why should they be called simulations?\footnote {A similar objection was raised -- though erroneously -- by \citet[p.\ 581]{winsberg:2009} against Guala's distinction between simulations and experiments based on the formal or material between the object and the target.} And it does not seem to be a good solution to apply the reliability criteria only to hybrids but not to clear cut cases. Not least because there are no sharp borderlines it is better to have one set of classification criteria that can be applied across the board. It might still be reasonable to employ reliability as one subordinate criterion among other criteria, but as a main criterion for classifying hybrids as simulation-like or experiment-like it fails.
%
%The latter problem, however, might considered as a problem primarily of the labeling of a method as ``simulation'' or ``experiment''. Why not give this labeling up altogether and merely develop a categorization of different types and degrees of reliability. This is a viable alternative.  

\subsection{Hybrids as mixtures of empirical and virtual data sources (Zacharias/Lenel)}

\citet{guala:2002} considers economic experiments where real human agents act in an artificial laboratory situation as hybrid methods. Let's for the sake of simplicity imagine an experiment where human agents interact with computer agents. Generalizing from this case and adjusting it to the terminology developed in the first part of the paper, this leads to one possible definition of hybrids as procedures where the data source is partly empirical and partly virtual.

How does this relate to our earlier distinction between simulations and experiments in virtue of the material or formal similarity of object and target? Well, the example shows that both the object under study and the target can be complex entities that are made of different components. The material similarity that makes the method an experiment may hold only for some components of the object and target but not for others. 

As a consequence of this, the differences between simulations and experiments that have been described earlier apply only insofar as components of the object under investigation are concerned that do actually bear a material similarity to (parts of) the target system. One could classify hybrids (in the just defined sense) as experiments, if one were willing to weaken the formulations of the differences a bit, for example, by allowing that it suffices that at least one component of the object is a part or and instance of some part of the target system. But this would be a somewhat strained attempt to keep up a strict dichotomy between simulations and experiments.

A much better solution has been proposed by Moritz Lenel and Sebastian Zacharias (unpublished). They give up the strict dichotomy in favor of a cross classification of simulations and experiments (first dimension) and laboratory and field methods (second dimension). In order to do so they drop the idea of a monolithic target system. Instead they make a difference between the target object and the target situation. Experiments and simulations are then distinguished by whether the operate directly on the target object or on a representation thereof. And laboratory research is distinguished from field research by whether it takes place in the target situation or in an artificially crafted laboratory environment. This classification scheme works quite well for economic experiments and simulations and for the social sciences in general. Economic experiments would most of the time fall under the category of laboratory experiments, but there is also room for laboratory simulations, field simulations and field experiments.

It is an open question how well this or a similar scheme could work in the natural sciences. Also, the case where human agents act together with computer agents in one and the same situation of an economic experiment might strain the classification. Still it is so far one of the most convincing answers to the problem of hybrids.


\subsection{Classification in terms of the degree of materiality (Morgan)}

A quite natural approach would be to examine to what extend the employed method depends on materiality, i.e.\ material data sources, material interaction, material output, throughout the course of the simulation or experiment in question. This is the approach that Mary S. \citet{morgan:2003} has taken. Doing so, she reaches a fine-grained classification that ranges from lab experiments over ``virtually experiments'', ``virtual experiments'' (which are not the same as ``virtually experiments''!) to mathematical model experiments. Morgan does not only take into account the material status of input, intervention and output, but also the relation between object and target where, again, she carefully distinguishes between ``representative of'', ``representative for'' and ``representation of''. Morgan's ``Experiments without material intervention'' (2003) is also one of the few attempts to explicitly deal with hybrid methods. It cannot be attempted to do justice to her careful and well-reasoned examination here. But a few remarks are in order.

First of all, while it seems reasonable to consider the materiality or non-materiality of the intervention for distinguishing degrees of virtuality, it is not equally clear why the material or non-material status of the inputs or outputs should really matter. A simulation can start with empirical input data of some system and then calculate the future evolution of the system. But this would not make the simulation any more experimental. The most that can be said is that materiality of input data is a necessary but not sufficient requirement for a procedure to be an experiment or empirical measurement. As will be argued below, it is, if anything at all, the relation between the output and the input that makes a hybrid an experiment or a simulation. 

Mary Morgan's distinction between representative and representation is more convincing. But although it is very helpful for delineating experiments from simulations, it does not seem fit to solve the problem of hybrid methods, because -- as has been argued above -- the problem arises when both of these relations are present in the course of one and the same procedure. As example cases Morgan examines two different simulations of hipbones. Both differ in the way the model of the hipbone is obtained on which the simulation is carried out. In one case the model is obtained by cutting one particular hipbone into slices and determining the three dimensional structure of the hipbone from these slices. In the other case the scientists started with a stylized bone-model that is then refined with ``averages of measurements of internal strut widths (taken from a number of real cow bones) and are gently angled in relation to each other by use of a random-assignment process'' \citep[p.\ 222]{morgan:2003}. Only in the first case the input data is clearly of empirical origin. The other case could -- from the description given by Morgan -- alternatively be interpreted as an example of a theoretical model that is adjusted or corrected with empirical data. For Morgan the first simulation is therefore more like a material experiment than the second, and both lie somewhere between pure material experiments and pure mathematical modeling.

The stance adopted here leads to a different evaluation, though. According to the view advocated here, both examples are clearly simulations. The reason is that the empirical origin of the input data alone is not sufficient to classify a procedure as experimental even if only partially experimental. In either of the two cases described by Morgan it is at best the input data that is empirical. For, the object that is manipulated during the study is obviously a model. According to  Morgan's description, ``in both cases \ldots the experiment consists of the `application' of a conventionally accepted \ldots mathematical version of the laws of mechanics \ldots The computer experiment calculates the effects of the `force' on individual elements in the grid and assembles the individual effects into an overall measure of the strength due to structure.''

The last description seems to fit quite well one of our earlier characterizations of simulations in contrast to experiments, namely, that in a simulation it is a model and not a material object that produces the simulation data. This characterization is not as clear as it may seem at first sight, though, because it requires that we can always distinguish the case where a model that is merely set up with empirical parameter values produces simulation data from cases of mere refinement of empirical input data, like for example noise reduction. In the examples that Morgan presents, however, it seems clear enough that the data is produced by programmed models in a way that goes beyond the typical inferential patterns that can be found in measurements. And it does not contradict this finding that the models have been created from empirical data.


%\subsubsection{Classification according to the kind of transformation from input to output}
%
%Another approach would be to distinguish simulations, hybrids and experiments according to the kind of transformation that the empirical input data undergoes in the process.\footnote{Of course simulations that do not make use of empirical input data at all (e.g.\ toy simulaions, how-possible simulations or proof-of-concept simulations) are out of the question here.}  We can then ask the question what properties distinguish a refinement-transformation, i.e.\ a transformation that retains the empirical status of the processed data, from a simulation-transformation where we would not want to describe the output data as empirical data any more but as simulation data.
%
%Framing the question in this way, one could now examine different characteristics of the transformation function as possible candidates for the distinction. For example one could ask whether and to what extent the transformation adds or removes information to the input data. Or, to what extent changes of the input data are reflected in the output data. Or, whether the transformation involves interpolation or extrapolation of data.\footnote{This suggestion has been made to me by Ulrike Pompe.} If, for example, extrapolation is considered as epistemically more daring than interpolation then one might be inclined to classify a transformation that involves extrapolation as more simulation-like in comparison to a transformation that ``only'' involves interpolation. In a similar vein one could ask whether the transformation involves inductive leaps of any sort.
%
%Without pursuing any of these possible approaches any further, it can be observed that it is doubtful that the distinction between simulations and experiments can be tied to purely mathematical characteristics of the transformation function alone. For, in this case on would probably assume that simple bijective or linear transformations preserve the empirical character of the input data. But a simple counter example shows that this is not the case: Imagine a simulation that simulates the path that the earth will take around the sun next year, by simply mapping the path the earth takes this year onto itself and adding one year to the time stamp. In spite of the triviality of this transformation, the output data cannot by classified as empirical data any more, because it concerns a state of affairs that lies in the future and that cannot even possibly have been observed. On the other hand the much more complicated transformations that occur in electron microscopes, computed tomography etc.\ do not prevent scientists from considering the output of these machines as empirical data. As a consequence it seems that it is not so much the intrinsics of the transformation function that matters for the distinction between simulations and experiments, but how the input and the output of the simulation or experiment are related in terms of time, space, causal direction, information flow and the like. This is the approach that will be examined in the following.
%
%
%%(Imagine an apparatus that measures a certain signal but at the same time runs a simulation of the empirical phenomenon that produces the signal alongside with the measurements, and that mixes both data sources weighing them according to an estimate of the reliability of the empirical signal.) 
%
%

\subsection{Classification in terms of the relation between input and output}

In the following I am going to present my own best guess at how to answer the problem of hybrids. As stated earlier, the -- in my opinion -- best way of framing the question is to ask, how computer simulations that make use of empirical input data can be distinguished from empirical measurements that involve the computational refinement of raw data. The difference can, I believe, easily be made clear with the help of examples. 

Think for example of a climate simulation: A climate simulation calculates the future development of the climate. In order to do so it is fed with empirical data. Thus both components of a hybrid, empirical input data and the computational processing of this data, are present. Yet it is clear that a climate simulation is a simulation and not a measurement, because it is impossible to measure something that lies in the future. 

Now take as another example an MRI scan: Again both components of a hybrid are present: The object or the person in the scanner from which the empirical input data is recorded in form of electro-magnetic waves that are emitted in response to the prior incitation of its H-atoms; and the computational processing, which in this case produces a visual image of the internal structure of the object from the input data. While the classification may be not quite as indisputable as the example of the climate simulation, it still appears reasonably clear that this is an empirical measurement, because the object's structure is reconstructed from data that reflects this structure.

As clear as the example cases may be, it is much more difficult to find general criteria by which to decide whether a particular method or procedure belongs to the class of simulations or to that of  measurements (or experiments for that matter). In the following I am going to attempt an answer in two steps. The uniting idea for both steps is the assumption that the difference between simulation-like hybrids and measurement-like hybrids can best be spelled out in terms of the relation that subsists between magnitudes that the output data represents and the magnitudes that the input data measures.\footnote{The relation between input and output that is meant here is not to be confused with the transformation function that transforms the input data into output data. Rather it concerns the relation of the input and output values within the target system. Examining the nature of the transformation from input to output might provide yet another alternative to deal wit the problem of hybrids. But this alternative is not examined here.}


\subsubsection{A first cut: The same-system formula}

Following the idea that one distinguishing feature of experiments from simulations is that experiments can operate on the physical target system itself, one can formulate the following criterion:

\begin{quote}
{\em Same-system formula}: A hybrid is a measurement if its output data describes the same system in the same state as its input data.\footnote{It might be worth of notice that the input of the computational part of a hybrid is always a precisely and unambiguously defined magnitude, namely the digital data as it is entered into the computer (either by hand or by a digitizing device) before any calculations on this data have been carried out.}
\end{quote} 

One can easily check that this criterion works well with the two examples given above: The output of the MRI scan is obviously data about the very system that the input data is taken from and it is about the system in exactly that state in which the input data was recorded. While in the case of the climate simulation one could say that the input and output system is the same, namely the climate system, the output clearly concerns the system in a future state and therefore in another state than the input. The same-system formula therefore correctly places it in the class of simulations.

The same system formula works well enough in many cases, but unfortunately not in all cases. Imagine a similar case as Margret \citet{morgan:2003}
discusses: We determine empirically the structure of a particular hipbone. Then we run simulations where pressure is put on the hipbone in order to estimate the strength of this hipbone. The hipbone's strength is thus inferred by a calculation from its structure. Now, measurements often involve some kind of inference, but usually this is backward inference, where we measure by some overt phenomenon the deeper causes of this phenomenon, e.g.\ the temperature by the extension of the liquid in a thermometer. But in this case the inference goes in the other direction. It therefore appears very doubtful whether one could call this a measurement of the hipbone's strength.

 
\subsubsection{A second cut: The measuring the cause by its effects pattern}

Since the same system formula fails as a sufficient criterion for classifying hybrids, a more subtle criterion is needed. Spelling out the same idea that only experiments operate on the physical target system itself, I propose the following two criteria for classifying hybrids as measurements: 

\begin{enumerate}

\item {\em Spatio-Temporal concordance of source and output}: The output values have the same spatio-temporal location as the source values.

\item {\em Causal dependency of input on output}: The output values are either a necessary (!) cause for the input values or the output values are linked by definitions or mathematical laws to the input values.

\end{enumerate}


The first restriction makes sure that neither prognoses
%\footnote{As far as prognoses are concerned, these are also excluded by the second criteria.} 
nor retro-dictions (i.e.\ inferences about past events based on present observations) are accidentally classified as measurements. The second restriction reflects the well known pattern of measuring a magnitude by its causal effects, as, for example, if one measures the force through the expansion of a spring. The further qualification that also a link by definition or mathematical laws suffice is meant to capture such simple cases such as measuring the density by measuring and then dividing the weight and the volume of an object. If a hybrid procedure is found to be a measurement by these criteria then we can also speak of the input data as \emph{raw data} and the output data as \emph{refined data}, thereby indicating that in the case of a (computationally enhanced) measurement the input and the output data still concern one and the same thing. There exists an overlap between both criteria in so far as both the second and the first criterion exclude prognoses, but this overlap is harmless. And one can easily verify that neither criterion is superfluous in the sense of preempting the other criterion.

We speak here of ``values'' rather than ``data'', because data is strictly speaking an entity located in a computer and causally linked to the software that processes it. What matters here, however, are the magnitudes in nature that the data informs us about. We understand ``values'' as always having the time, location and causal connection of their occurrence in nature. Also, it should be noted that in the first criterion we do not refer to {\em input} values but to {\em source} values.\footnote{This distinction relates to Paul Humphreys' distinction between source data and accessible data. See figure 1 of Humphreys' article in this volume.} This accounts for the fact that the measuring device can be located more or less remotely from its object. For example, a person observing an explosion may hear a noise and see a flash of light, both of which occur at a different time to the observer. Because of this it would not be very useful to require the spatio-temporal concordance with the input values in the first criterion. Admittedly, introducing the concept of source values here raises questions regarding the relation between source values and input values. Since the source values cannot directly be observed it requires at least a further inferential step to reconstruct the source values from the input values. It would lead too far to go into this problem here. Therefore, it must be noted as an open question.

In order justify the proposed criteria for classifying hybrid methods, we will briefly go through a number of typical examples of hybrid methods and try to show that the classification according to these criteria is sound in the sense of matching the intuitions one might have about the particular examples.

The probably most well-known example of simulation science are climate simulations. Climate simulations are clearly an example of simulations based on empirical input data and not experimental measurements. The output of climate simulations concerns the future development of the earth's climate. It would seem awkward to consider climate simulations as a measurement of the possible future climate. As the output does not fall into the same spatio-temporal region as the source, climate simulations are also not measurements according to our criteria. Thus, the classification of climate simulations according to our criteria is in harmony with our intuition and the self-ascription by scientists.

Another famous example of the most advanced kind of ``technoscience'' is the Large Hadron Collider. An interesting peculiarity of the Large Hadron Collider is that from the enormous number of events occurring during one second in the collider only a number of events that is several magnitudes smaller is preselected\footnote{LHC terminology speaks of ``re-processing'' of data. But as the data is not changed but merely a subset of data filtered from a larger set of data, we use the term ``preselection'' here to avoid misunderstanding.} by automatic procedures for further examination \citep{lhc-wlcg:2011}. This nicely illustrates the idea of epistemic opacity which according to \citet{humphreys:2004} is one of the characteristic features of modern computer based science, because it is the computer that decides which data will be selected and it is in principle impossible for any human agent to doublecheck each single decision, even though the algorithms for that decision are of course developed by humans. 

According to our criteria, which remain neutral with respect to the selection and preselection of data, this still counts as experimental measurement. This is in accordance with the self-description of the LHC project which also speaks of experiments. And it is reasonable to do so, because the events selected by the computer for further analysis are still empirical events that occurred in the collider itself. 

It is more difficult to decide how computational post-processing of data affects its status as empirical data. In magnetic resonance imaging the raw data obtained from the electromagnetic signals emitted by the previously stimulated protons of the body is turned into an image by means of various highly sophisticated computations \citep{lee-carroll:2010}. According to our criteria magnetic resonance imaging falls still into the category of experimental measurement, because the output is an image of the structure of the body, but it is just that structure of the body that determines what the electromagnetic signals, i.e.\ the raw data, are like. In this sense the output values are causally responsible for the input values. At the same time both output values and source values lie in the same spatio-temporal region. But not only according to our criteria, also intuitively it makes sense to consider magnetic resonance imaging as a measurement. For it bears a strong similarity to photography. And it can be verified by dissection that the images it produces resemble the object under study and thus are not fabricated by a model.

Simulations are a very popular tool in astronomy. One reason for this is that it is impossible to carry out material experiments with stars and galaxies. However, the fact that it is impossible to study, say, the collision of galaxies experimentally, does not turn a simulation of the collision of galaxies into an experimental procedure other than in a purely metaphorical sense of the word ``experimental''. If we consider such examples, then these are not experimental measurements according to our criteria, because already the input data is not empirical but model data about hypothetical galaxies \citep{struck:1997}. In this case the simulation would not even be classified as a hybrid in the first place. 

There are of course other kinds of simulations in astronomy that make heavy use of empirical input data, like the bolshoi-simulation \citep{bolshoi:2011}. The bolshoi-simulation is a simulation by our criteria because the output of the simulation (evolution of the universe or, rather, regions of the universe) is not a cause of the initial state nor is it located at the same time and place. The classification of the bolshoi-simulation as a simulation and not as an experiment is in accordance with the self-ascription by its creators and it also is intuitively plausible that it is a simulation and not an experiment. 

%A further indicator for the fact that simulations in astronomy are not experiments is the fact that in order to justify the results of such simulations like the collision of galaxies or the bolshoi simulation epistemically, it is impossible to do so, as one could do in an experiment\footnote{Here we follow the idea of \citep[p.\ 63]{winsberg:2010} that what ``distinguishes simulations from experiments is the {\em character of the argument given} for the legitimacy of the inference from object to target''.}, by arguing that the results are true, because they have been observed. Rather, we believe that if such simulations can be justified epistemicaly at all then it is either because they can be validated empirically in case the simulated process is also observable in nature (e.g.\ simulations of the occurrence of solar or lunar eclipses) or because we know the laws of nature according to which the simulated process takes place and we are able to approximate these laws numerically so that we can program them in the computer (e.g.\ simulations of collisions of galaxies) or because of both (e.g.\ bolshoi simulation).

This brief survey of examples indicates that our criteria for distinguishing experimental measurements that involve the computational refinement of data from simulations based on empirical input data can account for many prominent examples of advanced science. This in turn suggests that the criteria articulate at least an implicit standing convention for distinguishing data-based simulations from empirical measurements. It still leaves open the philosophical question whether and how this practice can be justified epistemologically. But because this answer to the problem of hybrids builds on a structural feature that is already present in traditional measurement instruments and that has been described here as the measuring the cause by its effects pattern, there is reason to assume that the problem of justifying it is either exactly the same or very similar as that of justifying the common understanding of traditional measurement or observation methods that build in some way or other on inferential steps as well. For example, we say we measure the temperature, when in fact we are measuring the extension of the volume of a liquid in a thermometer and infer the temperature with the help of a scale. Still, we consider the temperature value as empirical data and I believe we do so, because the kind of inference we make adheres to the two conditions stated above.

%If the last assertion is true then this provides at least a good pragmatic justification for the proposed criteria. Being more restrictive and counting only raw data (i.e.\ input data) as truly empirical data would go against the grains of the established use of the word ``empirical'' in scientific practice, where the term is commonly applied to the output of measurement instruments. Applying less restrictive criteria one might at some point have to face such oddities as classifying the weather forecast for tomorrow as a measurement of tomorrow's weather.

%One the other hand, even if it should be true that the proposed criteria reflect a common practice of attributing empirical or non-empirical character to the data produced by hybrid methods, this 

%But this means that the basic distinction between simulations and experiments made earlier remains intact for hybrids of simulations and experiments.



%Before, however, it will be appropriate to briefly summarize in what relation our point of view stands to that of other authors that have commented on the relation of simulations and experiments.

%\subsection{Justifying the distinction in analogy to the life-world epistemology}

%The difficulty of justifying the here proposed solution to the problem of hybrids consists in the fact that it rests on a convention: Why do we consider backward inference to the causes as acceptable for classifying a procedure as empirical measurement but not forward inference? The only answer I can give is a pragmatic one: It is meaningful to do so, because it nicely blends in with our life-world epistemology and thus draws on a model that is already well-understood by everybody. Therefore, a distinction based on these conventions can provide a helpful and, at least for the non-expert, most of the time adequate understanding of hybrid methods. To spell this out, I am going to proceed in three steps: First, I will explain what I understand under our life-world epistemology. Then, I will argue that it contains implicit assumptions that are in important respects similar to the convention guiding the classification of hybrid methods. Finally, I will try to explain why this provides a good guidance for understanding hybrids.

%Under ``life-world epistemology'' I understand the implicit principles that guide our cognitions and the way we gain and evaluate knowledge in everyday's life. Now, our life-world epistemology seems to be characterized by a strong privileging of direct observations (i.e.\ perception with our very own sense organs). We are apt to trust what we have seen ``with our own eyes'' more than what we have only heard of, believe or were convinced of before have seen it ourselves etc. Usually, we allow our believes to be corrected by our perceptions -- at least as long as they refer to something that is easily and directly perceivable. Furthermore we make the experience that our perceptions are most of the time quite reliable -- in contrast to our assumptions, believes, plans, conceptions. To be sure, perceptions can fail and there are instances where our believes or our prior knowledge is more reliable than our perceptions. But they do not often fail.

%This skew towards perceptions carries over to the world of science, where, again, we consider empirical observations as decisive over our theories. And, again, this is not always the case. In particular in areas with good theoretical foundations observations may have been comparatively weakened in comparison with theoretical reasoning, but in many instances it is still true. And there is an additional sense in which observations assume an epistemic priority in science: It is the very project of science to gain knowledge and understanding of the empirical world that is given to us through our senses. Therefore, it is the theories that have to accommodate to our experiences and not the other way round. And therefore any scientific claim must in the last instance be backed by perceptual knowledge. Even where science challenges the reliability of perceptions, this challenge must in the last instance lead back to the perceptions that back the theories and technologies in the name of which our perceptual abilities are challenged. This is also true for scientific measurements, which -- if a comparison is possible -- often are much more reliable than our perceptions. But the theory and technology on which the measurements rest can be verified only by other measurements, these again by other measurements, until at some point the end of the line is reached where the scientist must refer to sense-perception.
%It is true, though, that this only holds for sense perceptions and not for observations made with measurement technology. Therefore, this constitutes a slight break in the analogy between the life-world epistemology and scientific epistemology. But in other respects the analog still holds.

%The analogy between perception and technologically assisted observation and measurement also holds with respect to the {\em observing the cause by its effect}-pattern. For, in a way our sense perception already follows this pattern: We say that we see a car, where in fact what we observe is only the image of a car that is caused by a car and transmitted to us by the light that falls into our eyes. We also say that we observe a star, although the star might be millions of light years away and not even exist any more by the time we can see it in the nightly heaven. Thus, if we employ the terminology of empirical observation in those scientific contexts where the observing the causes by its effect pattern applies, there is little danger that we will be mislead concerning the nature of the operation. For, we already know this pattern and have a good feel for it. By the same token it can be dangerous to apply the terminology of empirical observation to scientific procedures where the pattern does not apply (e.g.\ a simulation that makes use of empirical input data but where the relation of output to input does not adhere to this pattern), because we might be mislead as to the nature of this operation. And underlying metaphysical assumption is that in our world causal and temporal backward inference is by and large less risky than forward inference.

%Now, it is true that the reliability of advanced scientific methods can only be properly estimated by experts (in contrast to the reliability of our sense organs which we learn to know in our childhood and adjust throughout our life), but then it is the job of the experts to clearly specify the conditions under which the data generating machinery (measurement devices, simulation software packges etc.) delivers reliable results and to train the specialists to use it accordingly. If these conditions are met, however, the empirical terminology can safely be applied to those procedure that follow the {\em observing the cause by its effect}-pattern. And it is helpful to do so, because it allows scientists convey a by and large adequate idea of how their data is obtained without the need to refer to the details of a reliability assessment that only experts understand. And it should be remembered that the users of such data might be scientists from other fields that with respect to the methods by which the data is gained are non-experts, too. Thus, the use of a simplifying but generally understandable terminology to describe scientific methods is important non only for communicating science to laymen but also for the communication between scientists.

%Summing it up: While the here proposed answer to the problem of hybrids inevitably rests on a conventional element, this may not be harmful if the chosen convention is meaningful and non-misleading (in sense of reducing the chance of drawing wrong conclusions). Admittedly, there remain open questions. But if the proposed answer or any other of the here discussed non-skeptical approaches turns out to be successful then this means that the basic distinction between simulations on the one hand side and experiments and empirical measurements on the other hand side remains intact for hybrids of simulations and experiments.


%It is these two restrictions that introduce the conventional element in our classification of hybrid methods into simulations and experimental measurements. One could also be more restrictive and only count raw data as truly empirical data. But, as explained earlier, this would go against the grains of the established use of the word empirical in scientific practice where the term empirical is commonly applied to the output of measurement instruments. Or one could try to be less restrictive but then one would probably have to face such oddities as classifying the weather forecast for tomorrow as a measurement of tomorrow's weather.

%(Imagine an apparatus that measures a certain signal but at the same time runs a simulation of the empirical phenomenon that produces the signal alongside with the measurements, and that mixes both data sources weighing them according to an estimate of the reliability of the empirical signal.) 

%What further complicates the distinction is the fact that the evaluation of experimental data may be decoupled from the material process that takes place in the course of the experiment. One can imagine the case where the same experimental raw data is (re-)evaluated years later, say, after new computational methods have been developed. The evaluation step could then appear as a computer simulation, because no material object is present any more. Yet, we would still want to describe the output data in this case as empirical or experimental data, albeit {\em refined} empirical data.

\section{Summary and open questions}

In this chapter I have argued that experiments and simulations and, by the same token, empirical measurements and theoretical calculations are clearly separate and well-distinguished categories. I have defended this distinction against what appears to me to be a strong tendency towards the contrary in the newer philosophy of simulation literature. However, the problem of hybrid methods, i.e.\ methods that combine empirical measurement of data with the computational processing of this data, raises conceptional problems which are not so easily solved. There are different possible approaches to solve these problems. In my opinion the best way to frame these problems is by asking the question what distinguishes a computer simulation based on empirical input data from an empirical measurement that involves the computational refinement of data. My answer consisted in transferring a typical of pattern of traditional measurement methods to the case of hybrids.

Several questions remain open, however. First of all, as the approach proposed by me is not the only possible or promising approach, it can still turn out that other approaches work better. Or it could turn out that no universal answer can be given, but only different answers for different subject areas. For the area of economic simulations in particular, the approach proposed by Sebastian Zacharias and Moritz Lenel appears to be the most well-suited and promising.

But there are also other open questions. The definition of hybrids that I have used more or less silently assumes that the output data really is computed from the input data and not ignored or dropped or the influence of the empirical component changing over time. But plausible cases where this does not hold can at least be imagined: Imagine, for example, a control device that regulates a machine based on data it receives from sensors. Let's assume that since the sensors tend to be unreliable from time to time, the regulatory device runs a simulation of the machine alongside the sensors. Whenever some kind of plausibility test shows that the sensors have delivered unreliable data the machine switches to the simulation. Otherwise it uses the sensor data as input and updates the simulation with the measured state of the machine. While it is not really possible to tell whether the data produced by the device is empirical or not, this case turns out to be rather unproblematic upon closer inspection. For lack of another word we could describe the data produced by this device as {\em potentially empirical data}. Now regarding the epistemic potential of this data, it is absolutely clear that this data can only be used in those contexts where in principle also simulation data would suffice (provided it is accurate enough), but not in those contexts, like empirical theory testing or model validation, where real empirical data is indispensable. 

Similarly unproblematic is the case where not a switch between empirical and simulation sources of input data occurs but where empirical and simulation sources are merged. This case is already covered by the theory of hybrids proposed here: As long the empirical data source has any significant influence on the output the procedure can be classified as empirical data. In principle it is suitable for all purposes for which real empirical data is needed. Of course, the details still matter. If a theory is to be tested then the validity of any model that is required for producing (or better: revealing) the empirical data against which it is to be tested must be assessable independently from the theory. This must of course already be considered in the case of conventional measurements. It does not constitute a novel or singular problem of computationally enhanced measurement techniques.


\newpage
\bibliographystyle{apsr}
\bibliography{bibliography}



\end{document}
